import streamlit as st
import re
import unicodedata
from docx import Document
import fitz  # PyMuPDF
import json
from datetime import datetime

# ==================== 0. æ–‡å­—æ­£è¦åŒ–å·¥å…· ====================

def normalize_text(text):
    """æ­£è¦åŒ–æ–‡å­—ï¼šå…¨å½¢è½‰åŠå½¢ã€æ¸…ç†å„ç¨®ç©ºç™½èˆ‡æ§åˆ¶ç¬¦"""
    if not text:
        return ""
    # 1ï¸âƒ£ å…¨å½¢å­—å…ƒè½‰åŠå½¢ (åŒ…å«æ‹¬è™Ÿã€æ¨™é»ã€ç©ºæ ¼)
    text = unicodedata.normalize('NFKC', text)
    # 2ï¸âƒ£ å°‡å¸¸è¦‹éš±è—ç©ºç™½ï¼ˆNBSPã€å…¨å½¢ç©ºç™½ç­‰ï¼‰çµ±ä¸€ç‚ºä¸€èˆ¬ç©ºç™½
    text = re.sub(r'[\u3000\xa0\u200b\u200c\u200d]+', ' ', text)
    # 3ï¸âƒ£ å»é™¤å¤šé‡ç©ºç™½ã€æ›è¡Œã€tab
    text = re.sub(r'\s+', ' ', text)
    # 4ï¸âƒ£ å»é ­å°¾ç©ºç™½
    return text.strip()

def normalize_citation_for_matching(citation):
    """å°ˆé–€ç”¨æ–¼å¼•ç”¨æ¯”å°çš„æ­£è¦åŒ–"""
    text = normalize_text(citation)
    text = re.sub(r'\s', '', text)
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    text = text.replace('ã€', '[').replace('ã€‘', ']')
    return text.lower()

# ========== [NEW] test1201 æ–°å¢çš„è¼”åŠ©å‡½å¼ ==========
def has_chinese(text):
    """[NEW] åˆ¤æ–·å­—ä¸²æ˜¯å¦åŒ…å«ä¸­æ–‡å­—å…ƒ"""
    return bool(re.search(r'[\u4e00-\u9fff]', text))

def extract_doi(text):
    """[NEW] å¾æ–‡å­—ä¸­æå– DOI (é€šç”¨)"""
    # [Updated from test1204-6] æ”¯æ´æ›´å¤šè®Šé«”
    doi_match = re.search(r'(?:doi:|DOI:|https?://doi\.org/)\s*(10\.\d{4,}/[^\sã€‚]+)', text)
    if doi_match:
        return doi_match.group(1).rstrip('ã€‚.,')
    return None
# =================================================

# ==================== 1. æ–‡ä»¶è®€å–æ¨¡çµ„ ====================

def extract_paragraphs_from_docx(file):
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

def extract_paragraphs_from_pdf(file):
    text = ""
    with fitz.open(stream=file.read(), filetype="pdf") as doc:
        for page in doc:
            text += page.get_text("text") + "\n"
    return [p.strip() for p in text.split("\n") if p.strip()]

# ==================== 2. åƒè€ƒæ–‡ç»å€æ®µè­˜åˆ¥ ====================

def is_appendix_heading(text):
    """åˆ¤æ–·æ˜¯å¦ç‚ºé™„éŒ„æ¨™é¡Œ"""
    text = text.strip()
    pattern = r'^([ã€ã€”ï¼ˆ(]?\s*)?((\d+|[IVXLCDM]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾]+)[ã€ï¼. ]?)?\s*(é™„éŒ„|APPENDIX)(\s*[ã€‘ã€•ï¼‰)]?)?$'
    return bool(re.match(pattern, text, re.IGNORECASE))

def is_reference_format(text):
    """åˆ¤æ–·æ®µè½æ˜¯å¦çœ‹èµ·ä¾†åƒåƒè€ƒæ–‡ç»"""
    text = text.strip()
    if len(text) < 10:
        return False
    if re.search(r'\(\d{4}[a-c]?\)', text):
        return True
    if re.match(r'^\[\d+\]', text):
        return True
    if re.search(r'[A-Z][a-z]+,\s*[A-Z]\.', text):
        return True
    return False

def extract_reference_section_improved(paragraphs):
    """æ”¹é€²çš„åƒè€ƒæ–‡ç»å€æ®µè­˜åˆ¥ (ç¶­æŒåŸæœ¬è¼ƒå¼·å¤§çš„é‚è¼¯ï¼ŒåŒ…å«é™„éŒ„æ’é™¤)"""
    reference_keywords = [
        "åƒè€ƒæ–‡ç»", "åƒè€ƒè³‡æ–™", "references", "reference",
        "bibliography", "works cited", "literature cited",
        "references and citations"
    ]
    
    def clip_until_stop(paragraphs_after):
        """æˆªå–è‡³é™„éŒ„ç‚ºæ­¢"""
        result = []
        for para in paragraphs_after:
            if is_appendix_heading(para):
                break
            result.append(para)
        return result
    
    for i in reversed(range(len(paragraphs))):
        para = paragraphs[i].strip()
        para_lower = para.lower()
        para_normalized = normalize_text(para_lower)
        
        if len(para) > 50:
            continue
        
        for keyword in reference_keywords:
            if normalize_text(keyword) == para_normalized:
                return clip_until_stop(paragraphs[i + 1:]), para, "ç´”æ¨™é¡Œè­˜åˆ¥"
        
        pattern = r'^((ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬]+ç« [ã€ï¼.ï¸‘,ï¼Œ]?)|(\d+|[IVXLCDM]+)?[ã€ï¼.ï¸‘,ï¼Œ ]?)?\s*(åƒè€ƒæ–‡ç»|åƒè€ƒè³‡æ–™|references?|bibliography)\s*$'
        if re.match(pattern, para_lower):
            return clip_until_stop(paragraphs[i + 1:]), para, "ç« ç¯€æ¨™é¡Œè­˜åˆ¥"
        
        fuzzy_keywords = ["reference", "åƒè€ƒ", "bibliography"]
        if any(para_lower.strip() == k for k in fuzzy_keywords):
            if i + 1 < len(paragraphs):
                next_paras = paragraphs[i+1:min(i+6, len(paragraphs))]
                if sum(1 for p in next_paras if is_reference_format(p)) >= 1:
                    return clip_until_stop(paragraphs[i + 1:]), para.strip(), "å…§å®¹ç‰¹å¾µè­˜åˆ¥"
    
    return [], None, "æœªæ‰¾åˆ°åƒè€ƒæ–‡ç»å€æ®µ"


def extract_reference_section(paragraphs):
    """åŸæœ¬çš„ç°¡å–®ç‰ˆæœ¬ï¼ˆfallbackï¼‰"""
    reference_keywords = [
        "åƒè€ƒæ–‡ç»", "åƒè€ƒè³‡æ–™", "references", "reference",
        "bibliography", "works cited", "literature cited"
    ]
    
    for i in reversed(range(len(paragraphs))):
        para = paragraphs[i].strip()
        para_lower = para.lower()
        para_normalized = normalize_text(para_lower)
        
        if len(para) > 50:
            continue
        
        for keyword in reference_keywords:
            if normalize_text(keyword) == para_normalized:
                ref_paragraphs = []
                for p in paragraphs[i + 1:]:
                    if is_appendix_heading(p):
                        break
                    ref_paragraphs.append(p)
                return ref_paragraphs, para, i
        
        pattern = r'^((ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬]+ç« [ã€ï¼.ï¸‘,ï¼Œ]?)|(\d+|[IVXLCDM]+)?[ã€ï¼.ï¸‘,ï¼Œ ]?)?\s*(åƒè€ƒæ–‡ç»|åƒè€ƒè³‡æ–™|references?|bibliography)\s*$'
        if re.match(pattern, para_lower):
            ref_paragraphs = []
            for p in paragraphs[i + 1:]:
                if is_appendix_heading(p):
                    break
                ref_paragraphs.append(p)
            return ref_paragraphs, para, i
    
    return [], None, None

def classify_document_sections(paragraphs):
    """å°‡æ–‡ä»¶åˆ†ç‚ºå…§æ–‡æ®µè½å’Œåƒè€ƒæ–‡ç»æ®µè½"""
    ref_paragraphs, ref_keyword, method = extract_reference_section_improved(paragraphs)
    
    if not ref_paragraphs:
        ref_paragraphs, ref_keyword, _ = extract_reference_section(paragraphs)
        if not ref_paragraphs:
            return paragraphs, [], None, None
    
    ref_start_index = None
    for i, para in enumerate(paragraphs):
        if para.strip() == ref_keyword:
            ref_start_index = i
            break
    
    if ref_start_index is None:
        # Fallback if keyword index not found exactly
        ref_start_index = len(paragraphs) - len(ref_paragraphs)
    
    content_paragraphs = paragraphs[:ref_start_index]
    return content_paragraphs, ref_paragraphs, ref_start_index, ref_keyword


# ==================== 3. å…§æ–‡å¼•ç”¨æ“·å– ====================

def is_valid_year(year_str):
    try:
        year = int(year_str)
        return 1000 <= year <= 2050
    except:
        return False

def extract_in_text_citations(content_paragraphs):
    """
    ä¿®æ­£ï¼šAPA å…§æ–‡å¼•ç”¨ä¸æŠ“æ•¸å­—ä½œè€…ï¼Œæ’é™¤å¦‚ 14(2022)
    """
    full_text = " ".join(content_paragraphs)
    citations = []
    citation_ids = set()
    # APA: (ä½œè€…, å¹´ä»½) and ä½œè€…(å¹´ä»½) å‹å¼
    pattern_apa1 = re.compile(
        r'(?<![0-9])[ï¼ˆ(]\s*([\w\s\u4e00-\u9fff-]+?)\s*(?:(?:&|and|èˆ‡|ã€)\s*([\w\s\u4e00-\u9fff-]+?))?\s*'
        r'(?:,?\s*et\s*al\.?)?\s*[,ï¼Œ]\s*(\d{4}[a-z]?)\s*[ï¼‰)]',
        re.UNICODE | re.IGNORECASE
    )
    for match in pattern_apa1.finditer(full_text):
        author1 = match.group(1).strip()
        author2 = match.group(2).strip() if match.group(2) else None
        year = match.group(3)[:4]
        # æ’é™¤ä½œè€…å…¨ç‚ºæ•¸å­—ï¼ˆå¦‚ 14ï¼‰
        if author1.isdigit():
            continue
        if is_valid_year(year):
            citation_id = f"{match.start()}-{match.end()}"
            if citation_id not in citation_ids:
                normalized = normalize_citation_for_matching(match.group(0))
                citations.append({
                    'author': author1,
                    'co_author': author2,
                    'year': year,
                    'original': match.group(0),
                    'normalized': normalized,
                    'position': match.start(),
                    'type': 'APA-parenthetical',
                    'format': 'APA'
                })
                citation_ids.add(citation_id)
    pattern_apa2 = re.compile(
        r'(?<![0-9])([\w\u4e00-\u9fff]+(?:\s+(?:et\s*al\.?|ç­‰))?)\s*[ï¼ˆ(]\s*(\d{4}[a-z]?)\s*[ï¼‰)]',
        re.UNICODE | re.IGNORECASE
    )
    for match in pattern_apa2.finditer(full_text):
        author = match.group(1).strip()
        year = match.group(2)[:4]
        # æ’é™¤ä½œè€…å…¨ç‚ºæ•¸å­—
        if author.isdigit():
            continue
        if is_valid_year(year):
            citation_id = f"{match.start()}-{match.end()}"
            if citation_id not in citation_ids:
                normalized = normalize_citation_for_matching(match.group(0))
                citations.append({
                    'author': author,
                    'co_author': None,
                    'year': year,
                    'original': match.group(0),
                    'normalized': normalized,
                    'position': match.start(),
                    'type': 'APA-narrative',
                    'format': 'APA'
                })
                citation_ids.add(citation_id)
    pattern_ieee = re.compile(r'[ã€\[]\s*(\d+)\s*[ã€‘\]]', re.UNICODE)
    for match in pattern_ieee.finditer(full_text):
        ref_number = match.group(1)
        citation_id = f"{match.start()}-{match.end()}"
        if citation_id not in citation_ids:
            normalized = normalize_citation_for_matching(match.group(0))
            citations.append({
                'author': None,
                'co_author': None,
                'year': None,
                'ref_number': ref_number,
                'original': match.group(0),
                'normalized': normalized,
                'position': match.start(),
                'type': 'IEEE-numeric',
                'format': 'IEEE'
            })
            citation_ids.add(citation_id)
    return citations

def check_references(in_text_citations, reference_list):
    """
    å®Œæ•´ä¿®æ­£ç‰ˆæ¯”å°å‡½å¼ï¼š
    1. è§£æ±º "et al." ç­‰ç¶´è©å°è‡´æ¯”å°å¤±æ•—çš„å•é¡Œ (åªæ¯”å°å§“æ°æ ¸å¿ƒ)ã€‚
    2. ç¹éè§£æå™¨å¯èƒ½éŒ¯èª¤çš„ 'year'/'author' æ¬„ä½ï¼Œç›´æ¥æ¯”å° 'original' åŸå§‹æ–‡å­—ã€‚
    3. åŠ å…¥ã€Œç–‘ä¼¼å¹´ä»½éŒ¯èª¤ã€åµæ¸¬ï¼Œç•¶ä½œè€…å°ä½†å¹´ä»½ä¸å°æ™‚ï¼Œæ¨™è¨˜æç¤ºã€‚
    """
    
    # --- è¼”åŠ©å‡½å¼ï¼šæå–ä½œè€…æ ¸å¿ƒå§“æ° (å»é™¤ et al., and, & ç­‰é›œè¨Š) ---
    def get_core_author_name(name):
        if not name: return ""
        # 1. è½‰å°å¯«
        name = str(name).lower()
        # 2. ç§»é™¤å¸¸è¦‹ç¶´è©ï¼Œé¿å…å¹²æ“¾
        for junk in ['et al.', 'et al', 'and', '&', ',']:
            name = name.replace(junk, ' ')
        # 3. åªå–ç¬¬ä¸€å€‹å–®å­— (é€šå¸¸å°±æ˜¯å§“æ°)
        parts = name.split()
        if parts:
            # åªä¿ç•™è‹±æ•¸å­—ï¼Œç§»é™¤é»è™Ÿç­‰
            return "".join(filter(str.isalnum, parts[0]))
        return ""

    # --- è¼”åŠ©å‡½å¼ï¼šæ¸…ç†åƒè€ƒæ–‡ç»æ–‡å­— ---
    def clean_ref_text(text):
        # è½‰å°å¯«ï¼Œåªä¿ç•™è‹±æ•¸å­—ï¼Œç”¨æ–¼é«˜å®¹éŒ¯æ¯”å°
        return "".join(filter(str.isalnum, str(text).lower()))

    matched_indices = set()
    missing_in_refs = []

    # --- 1. å»ºç«‹åƒè€ƒæ–‡ç»çš„ç·¨è™ŸæŸ¥æ‰¾è¡¨ (é‡å° IEEE æ ¼å¼åŠ é€ŸæŸ¥æ‰¾) ---
    ref_map_by_id = {}
    for i, ref in enumerate(reference_list):
        if ref.get('ref_number'):
            ref_num = str(ref['ref_number']).strip()
            ref_map_by_id[ref_num] = i

    # --- 2. éæ­·å…§æ–‡å¼•ç”¨ ---
    for cit in in_text_citations:
        is_found = False
        potential_year_error_hint = None # ç”¨ä¾†è¨˜éŒ„ç–‘ä¼¼æ­£ç¢ºçš„å¹´ä»½
        
        # è·¯å¾‘ A: IEEE æ ¼å¼å¼•ç”¨ (å„ªå…ˆç”¨ç·¨è™ŸæŸ¥ï¼Œæœ€æº–)
        if cit.get('ref_number'):
            cit_num = str(cit['ref_number']).strip()
            if cit_num in ref_map_by_id:
                is_found = True
                matched_indices.add(ref_map_by_id[cit_num])
        
        # è·¯å¾‘ B: APA æ ¼å¼å¼•ç”¨ (ç”¨ "æ ¸å¿ƒå§“æ° + å¹´ä»½" æƒæåŸå§‹æ–‡å­—)
        if not is_found and cit.get('author') and cit.get('year'):
            # æº–å‚™ç‰¹å¾µå€¼
            cit_year = ''.join(filter(str.isdigit, str(cit['year']))) # ä¾‹å¦‚ "2022"
            cit_auth_core = get_core_author_name(cit['author'])       # ä¾‹å¦‚ "yuanjiang"
            
            # åªæœ‰ç•¶æå–å‡ºæœ‰æ•ˆçš„ä½œè€…å’Œå¹´ä»½æ™‚æ‰é€²è¡Œæ¯”å°
            if cit_year and cit_auth_core:
                for i, ref in enumerate(reference_list):
                    # ç²å–åƒè€ƒæ–‡ç»çš„åŸå§‹æ–‡å­— (åŒ…å«æ‰€æœ‰è³‡è¨Š)
                    ref_original = str(ref.get('original', '')).lower()
                    ref_original_clean = clean_ref_text(ref_original)
                    
                    # æ­¥é©Ÿ 1: æª¢æŸ¥ã€Œæ ¸å¿ƒå§“æ°ã€æ˜¯å¦å‡ºç¾åœ¨åƒè€ƒæ–‡ç»ä¸­
                    if cit_auth_core in ref_original_clean:
                        # æ­¥é©Ÿ 2: å¦‚æœä½œè€…å°äº†ï¼Œå†æª¢æŸ¥ã€Œå¹´ä»½ã€æ˜¯å¦ä¹Ÿå­˜åœ¨
                        if cit_year in ref_original:
                            is_found = True
                            matched_indices.add(i)
                            break # å®Œç¾åŒ¹é…ï¼Œè·³å‡ºè¿´åœˆ
                        else:
                            # ä½œè€…å°äº†ä½†å¹´ä»½ä¸å° -> å¯èƒ½æ˜¯å¹´ä»½å¼•ç”¨éŒ¯èª¤
                            # å˜—è©¦å¾è©²æ¢åƒè€ƒæ–‡ç»ä¸­æŠ“ä¸€å€‹ 4 ç¢¼å¹´ä»½ä½œç‚ºæç¤º
                            # é€™è£¡ç°¡å–®æŠ“å– 19xx æˆ– 20xx çš„æ•¸å­—
                            years_in_ref = re.findall(r'(19\d{2}|20\d{2})', ref_original)
                            # å¦‚æœæœ‰æŠ“åˆ°å¹´ä»½ï¼Œä¸”é‚„æ²’è¨˜éŒ„éæç¤ºï¼Œå°±è¨˜éŒ„ä¸‹ä¾†
                            if years_in_ref and not potential_year_error_hint:
                                potential_year_error_hint = years_in_ref[0]
        
        if not is_found:
            # æ¨™è¨˜éŒ¯èª¤é¡å‹ï¼Œæ–¹ä¾¿å‰ç«¯ UI é¡¯ç¤ºä¸åŒæç¤º
            if potential_year_error_hint:
                cit['error_type'] = 'year_mismatch'
                cit['year_hint'] = potential_year_error_hint
            else:
                cit['error_type'] = 'missing'
            
            missing_in_refs.append(cit)

    # --- 3. æ‰¾å‡ºæœªä½¿ç”¨çš„åƒè€ƒæ–‡ç» ---
    unused_refs = []
    for i, ref in enumerate(reference_list):
        if i not in matched_indices:
            unused_refs.append(ref)
            
    return missing_in_refs, unused_refs

# ==================== 4. åƒè€ƒæ–‡ç»è§£æ (èˆŠç‰ˆ 1204 é‚è¼¯ - å·²è¨»è§£ä¿ç•™) ====================

## [èˆŠç¨‹å¼ç¢¼ 1204 - æ–·è¡Œåˆä½µèˆ‡ç‰¹å¾µåµæ¸¬]
## é€™äº›åŠŸèƒ½å·²è¢« test1201 çš„ merge_references_unified (é€²éšç‰ˆ) å–ä»£
## def find_apa(ref_text):
##     """
##     æ”¹é€²ï¼šæ”¯æ´ (2007, October 11) / (2011, Jan. 14) / (n.d.)
##     """
##     ref_text = normalize_text(ref_text)
##
##     apa_match = re.search(
##         r'[ï¼ˆ(]\s*(\d{4}(?:[a-c])?|n\.d\.)\s*(?:,\s*(?:[A-Za-z]+\.?\s*\d{0,2}))?\s*[)ï¼‰]',
##         ref_text, re.IGNORECASE
##     )
##
##     if not apa_match:
##         return False
##
##     year_str = apa_match.group(1)[:4]
##     year_pos = apa_match.start(1)
##     pre_context = ref_text[max(0, year_pos - 5):year_pos]
##     if re.search(r'\d', pre_context):  # é¿å… 887(2020) èª¤åˆ¤
##         return False
##
##     return year_str.isdigit() or apa_match.group(1).lower() == "n.d."
##
## def find_apalike(ref_text):
##     valid_years = []
##     for match in re.finditer(r'[,ï¼Œ.ã€‚]\s*(\d{4}[a-c]?)[.ã€‚ï¼Œ]', ref_text):
##         year_str = match.group(1)
##         year_pos = match.start(1)
##         year_core = year_str[:4]
##         if not is_valid_year(year_core): continue
##         pre_context = ref_text[max(0, year_pos - 5):year_pos]
##         if re.search(r'\d', pre_context): continue
##         after_context = ref_text[match.end(1):match.end(1) + 5]
##         if re.match(r'\.(\d{1,2}|[a-z0-9]{2,})', after_context, re.IGNORECASE): continue
##         arxiv_pattern = re.compile(
##             r'arxiv:\d{4}\.\d{5}[^a-zA-Z0-9]{0,3}\s*[,ï¼Œ]?\s*' + re.escape(year_str), re.IGNORECASE
##         )
##         if arxiv_pattern.search(ref_text) and arxiv_pattern.search(ref_text).start() < year_pos: continue
##         valid_years.append((year_str, year_pos))
##     for match in re.finditer(r'ï¼Œ\s*(\d{4}[a-c]?)\s*ï¼Œ\s*ã€‚', ref_text):
##         year_str = match.group(1)
##         year_pos = match.start(1)
##         year_core = year_str[:4]
##         if not is_valid_year(year_core): continue
##         pre_context = ref_text[max(0, year_pos - 5):year_pos]
##         if re.search(r'\d', pre_context): continue
##         valid_years.append((year_str, year_pos))
##     return valid_years
##
## def is_reference_head(para):
##     return bool(find_apa(para) or re.match(r"^\[\d+\]", para) or find_apalike(para))
##
## def find_apa_matches(ref_text):
##     APA_PATTERN = r'[ï¼ˆ(](\d{4}[a-c]?|n\.d\.)\s*(?:,\s*[A-Za-z]+\s*\d{1,2})?\s*[ï¼‰)]?[ã€‚\.]?'
##     matches = []
##     for m in re.finditer(APA_PATTERN, ref_text, re.IGNORECASE):
##         year_str = m.group(1)[:4]
##         year_pos = m.start(1)
##         pre_context = ref_text[max(0, year_pos - 5):year_pos]
##         if re.search(r'\d', pre_context): continue
##         if year_str.isdigit() and is_valid_year(year_str):
##             matches.append(m)
##         elif m.group(1).lower() == "n.d.":
##             matches.append(m)
##     return matches
##
## def find_apalike_matches(ref_text):
##     matches = []
##     pattern1 = r'[,ï¼Œ.ã€‚]\s*(\d{4}[a-c]?)[.ã€‚ï¼Œ]'
##     for m in re.finditer(pattern1, ref_text):
##         year_str = m.group(1)
##         year_pos = m.start(1)
##         year_core = year_str[:4]
##         if not is_valid_year(year_core): continue
##         pre_context = ref_text[max(0, year_pos - 5):year_pos]
##         after_context = ref_text[m.end(1):m.end(1) + 5]
##         if re.search(r'\d', pre_context): continue
##         if re.match(r'\.(\d{1,2}|[a-z0-9]{2,})', after_context, re.IGNORECASE): continue
##         arxiv_pattern = re.compile(
##             r'arxiv:\d{4}\.\d{5}[^a-zA-Z0-9]{0,3}\s*[,ï¼Œ]?\s*' + re.escape(year_str), re.IGNORECASE
##         )
##         if arxiv_pattern.search(ref_text) and arxiv_pattern.search(ref_text).start() < year_pos: continue
##         matches.append(m)
##     pattern2 = r'ï¼Œ\s*(\d{4}[a-c]?)\s*ï¼Œ\s*ã€‚'
##     for m in re.finditer(pattern2, ref_text):
##         year_str = m.group(1)
##         year_pos = m.start(1)
##         year_core = year_str[:4]
##         pre_context = ref_text[max(0, year_pos - 5):year_pos]
##         if re.search(r'\d', pre_context): continue
##         if is_valid_year(year_core):
##             matches.append(m)
##     return matches
##
## def split_multiple_apa_in_paragraph(paragraph):
##     apa_matches = find_apa_matches(paragraph)
##     apalike_matches = find_apalike_matches(paragraph)
##     all_matches = apa_matches + apalike_matches
##     all_matches.sort(key=lambda m: m.start())
##     if len(all_matches) < 2:
##         return [paragraph]
##     split_indices = []
##     for match in all_matches[1:]:
##         cut_index = max(0, match.start() - 5)
##         split_indices.append(cut_index)
##     segments = []
##     start = 0
##     for idx in split_indices:
##         segments.append(paragraph[start:idx].strip())
##         start = idx
##     segments.append(paragraph[start:].strip())
##     return [s for s in segments if s]
##
## def merge_references_by_heads(paragraphs):
##     merged = []
##     for para in paragraphs:
##         apa_count = 1 if find_apa(para) else 0
##         apalike_count = len(find_apalike(para))
##         if apa_count >= 2 or apalike_count >= 2:
##             sub_refs = split_multiple_apa_in_paragraph(para)
##             merged.extend([s.strip() for s in sub_refs if s.strip()])
##         else:
##             if is_reference_head(para):
##                 merged.append(para.strip())
##             else:
##                 if merged:
##                     merged[-1] += " " + para.strip()
##                 else:
##                     merged.append(para.strip())
##     return merged
##
## def detect_reference_format(ref_text):
##     """åµæ¸¬åƒè€ƒæ–‡ç»æ ¼å¼"""
##     if re.match(r'^\s*[ã€\[]\s*\d+\s*[ã€‘\]]\s*', ref_text):
##         return 'IEEE'
##     
##     if find_apa(ref_text):
##         return 'APA'
##     
##     if find_apalike(ref_text):
##         return 'APA_LIKE'
##     
##     return 'Unknown'


# ==================== [ä¿®æ­£ç‰ˆï¼šIEEE æ–‡ç»è³‡è¨Šæ“·å–] - å·²è¨»è§£ä¿ç•™ ====================

## def extract_ieee_reference_info_fixed(ref_text):
##     """ä¿®æ­£ç‰ˆ IEEE æ ¼å¼æ“·å–ï¼ˆä¿®å¾©å¤šä½œè€…å•é¡Œï¼‰"""
##     
##     number_match = re.match(r'^\s*[\[ã€]\s*(\d+)\s*[\]ã€‘]\s*', ref_text)
##     if not number_match:
##         return None
##     
##     ref_number = number_match.group(1)
##     rest_text = ref_text[number_match.end():].strip()
##     
##     authors = "Unknown"
##     title = None
##     year = "Unknown"
##     
##     # æ­¥é©Ÿ 1ï¼šå„ªå…ˆæ‰¾å¼•è™Ÿä¸­çš„æ¨™é¡Œ
##     quote_patterns = [
##         (r'"', r'"'),
##         (r'â€œ', r'â€'),
##         (r'ã€Œ', r'ã€'),
##         (r'\'', r'\''),
##         (r'â€œ', r'â€œ'),
##         (r'â€', r'â€'),
##     ]
##     
##     title_found = False
##     
##     for open_q, close_q in quote_patterns:
##         pattern = re.escape(open_q) + r'(.+?)' + re.escape(close_q)
##         match = re.search(pattern, rest_text)
##         
##         if match:
##             title = match.group(1).strip()
##             title = re.sub(r'[,ï¼Œ.ã€‚;ï¼›:ï¼š]*$', '', title).strip()
##             
##             # å¼•è™Ÿå‰çš„æ‰€æœ‰å…§å®¹éƒ½æ˜¯ä½œè€…ï¼ˆåŒ…å«å¤šä½œè€…ï¼‰
##             before_title = rest_text[:match.start()].strip()
##             before_title = before_title.rstrip(',ï¼Œ. ')
##             
##             # ç§»é™¤å¯èƒ½çš„ "and" çµå°¾
##             before_title = re.sub(r'\s+and\s*$', '', before_title, flags=re.IGNORECASE)
##             # ç§»é™¤ et al. çµå°¾
##             before_title = re.sub(r',?\s*et\s+al\.?$', '', before_title, flags=re.IGNORECASE)
##             
##             if before_title:
##                 # æ¸…ç†é–‹é ­çš„ç·¨è™Ÿæ®˜ç•™
##                 before_title = re.sub(r'^\[\d+\]\s*', '', before_title)
##                 
##                 # å®Œæ•´ä¿ç•™æ‰€æœ‰ä½œè€…ï¼ˆç”¨é€—è™Ÿåˆ†éš”çš„å¤šä½œè€…ï¼‰
##                 if re.search(r'[a-zA-Z\u4e00-\u9fff]', before_title) and len(before_title) > 1:
##                     authors = before_title  # ä¿ç•™å®Œæ•´å¤šä½œè€…å­—ä¸²
##             
##             title_found = True
##             break
##     
##     # å¦‚æœæ²’æœ‰æ‰¾åˆ°å¼•è™Ÿæ¨™é¡Œï¼Œç”¨å‚™é¸æ–¹æ¡ˆ
##     if not title_found:
##         # å˜—è©¦ç”¨ "and" åˆ¤æ–·ä½œè€…å€æ®µçµå°¾
##         and_match = re.search(r'\band\b', rest_text, re.IGNORECASE)
##         
##         if and_match:
##             after_and = rest_text[and_match.end():].strip()
##             next_comma = after_and.find(',')
##             
##             if next_comma > 0:
##                 # å¾é–‹é ­åˆ° "and" å¾Œç¬¬ä¸€å€‹é€—è™Ÿç‚ºä½œè€…
##                 authors_section = rest_text[:and_match.end() + next_comma].strip()
##                 authors_section = authors_section.rstrip(',ï¼Œ. ')
##                 
##                 # å®Œæ•´ä¿ç•™ä½œè€…å€æ®µ
##                 if authors_section and re.search(r'[a-zA-Z]', authors_section):
##                     authors = authors_section
##                 
##                 # é€—è™Ÿå¾Œçš„å…§å®¹ç‚ºæ¨™é¡Œå€™é¸
##                 remaining = rest_text[and_match.end() + next_comma:].strip()
##                 remaining = remaining.lstrip(',ï¼Œ. ')
##                 
##                 title_match = re.match(r'^([^,ï¼Œ.ã€‚]+)', remaining)
##                 if title_match:
##                     potential_title = title_match.group(1).strip()
##                     if len(potential_title) > 10:
##                         title = potential_title
##         else:
##             # æ²’æœ‰ "and"ï¼Œå˜—è©¦ç”¨ç¬¬ä¸€å€‹é€—è™Ÿåˆ†éš”
##             parts = rest_text.split(',', 2)
##             
##             if len(parts) >= 2:
##                 potential_author = parts[0].strip()
##                 if potential_author and re.search(r'[a-zA-Z]', potential_author):
##                     authors = potential_author
##                 
##                 potential_title = parts[1].strip()
##                 if len(potential_title) > 10:
##                     title = potential_title
##     
##     # æå–å¹´ä»½
##     year_matches = re.findall(r'\b(19\d{2}|20\d{2})\b', rest_text)
##     if year_matches:
##         year = year_matches[-1]
##     
##     return {
##         'ref_number': ref_number,
##         'authors': authors,
##         'title': title if title else "Unknown",
##         'year': year
##     }
##
## def extract_ieee_reference_full(ref_text):
##     """
##     å®Œæ•´è§£æ IEEE æ ¼å¼åƒè€ƒæ–‡ç»
##     è¿”å›ï¼šæ ¼å¼ã€æ–‡ç»é¡å‹ã€æ‰€æœ‰æ¬„ä½
##     """
##     
##     # åŸºæœ¬æ¬„ä½åˆå§‹åŒ–
##     result = {
##         'format': 'IEEE',
##         'ref_number': None,
##         'source_type': 'Unknown',
##         'authors': None,
##         'title': None,
##         'journal_name': None,
##         'conference_name': None,
##         'book_title': None,
##         'volume': None,
##         'issue': None,
##         'pages': None,
##         'year': None,
##         'month': None,
##         'publisher': None,
##         'location': None,
##         'edition': None,
##         'url': None,
##         'access_date': None,
##         'doi': None,
##         'report_number': None,
##         'patent_number': None,
##         'original': ref_text
##     }
##     
##     # æå–ç·¨è™Ÿ
##     number_match = re.match(r'^\s*[\[ã€]\s*(\d+)\s*[\]ã€‘]\s*', ref_text)
##     if not number_match:
##         return result
##     
##     result['ref_number'] = number_match.group(1)
##     rest_text = ref_text[number_match.end():].strip()
##     
##     # === 1. æå–ä½œè€…å’Œæ¨™é¡Œï¼ˆä½¿ç”¨ä½ çš„å¼•è™Ÿåµæ¸¬é‚è¼¯ï¼‰ ===
##     authors = "Unknown"
##     title = None
##     
##     # å„ªå…ˆæ‰¾å¼•è™Ÿä¸­çš„æ¨™é¡Œï¼ˆä½œè€…èˆ‡æ¨™é¡Œåˆ†ç•Œé»ï¼‰
##     quote_patterns = [
##         (r'"', r'"'),
##         (r'"', r'"'),
##         (r'ã€Œ', r'ã€'),
##         (r'â€œ', r'â€'),
##         (r'â€œ', r'â€œ') ,
##         (r'\'', r'\''),
##         (r'â€', r'â€')
##     ]
##     
##     title_found = False
##     after_title = rest_text
##     
##     for open_q, close_q in quote_patterns:
##         pattern = re.escape(open_q) + r'(.+?)' + re.escape(close_q)
##         match = re.search(pattern, rest_text)
##         
##         if match:
##             title = match.group(1).strip()
##             title = re.sub(r'[,ï¼Œ.ã€‚;ï¼›:ï¼š]*$', '', title).strip()
##             result['title'] = title
##             
##             # å¼•è™Ÿå‰çš„æ‰€æœ‰å…§å®¹éƒ½æ˜¯ä½œè€…ï¼ˆåŒ…å«å¤šä½œè€…ï¼‰
##             before_title = rest_text[:match.start()].strip()
##             before_title = before_title.rstrip(',ï¼Œ. ')
##             
##             # ç§»é™¤å¯èƒ½çš„ "and" çµå°¾
##             before_title = re.sub(r'\s+and\s*$', '', before_title, flags=re.IGNORECASE)
##             # ç§»é™¤ et al. çµå°¾
##             before_title = re.sub(r',?\s*et\s+al\.?$', '', before_title, flags=re.IGNORECASE)
##             
##             if before_title:
##                 # æ¸…ç†é–‹é ­çš„ç·¨è™Ÿæ®˜ç•™
##                 before_title = re.sub(r'^\[\d+\]\s*', '', before_title)
##                 
##                 # å®Œæ•´ä¿ç•™æ‰€æœ‰ä½œè€…ï¼ˆç”¨é€—è™Ÿåˆ†éš”çš„å¤šä½œè€…ï¼‰
##                 if re.search(r'[a-zA-Z\u4e00-\u9fff]', before_title) and len(before_title) > 1:
##                     authors = before_title
##             
##             result['authors'] = authors
##             after_title = rest_text[match.end():].strip()
##             title_found = True
##             break
##     
##     # å¦‚æœæ²’æœ‰æ‰¾åˆ°å¼•è™Ÿæ¨™é¡Œï¼Œç”¨å‚™é¸æ–¹æ¡ˆ
##     if not title_found:
##         # å˜—è©¦ç”¨ "and" åˆ¤æ–·ä½œè€…å€æ®µçµå°¾
##         and_match = re.search(r'\band\b', rest_text, re.IGNORECASE)
##         
##         if and_match:
##             after_and = rest_text[and_match.end():].strip()
##             next_comma = after_and.find(',')
##             
##             if next_comma > 0:
##                 # å¾é–‹é ­åˆ° "and" å¾Œç¬¬ä¸€å€‹é€—è™Ÿç‚ºä½œè€…
##                 authors_section = rest_text[:and_match.end() + next_comma].strip()
##                 authors_section = authors_section.rstrip(',ï¼Œ. ')
##                 
##                 # å®Œæ•´ä¿ç•™ä½œè€…å€æ®µ
##                 if authors_section and re.search(r'[a-zA-Z]', authors_section):
##                     authors = authors_section
##                     result['authors'] = authors
##                 
##                 # é€—è™Ÿå¾Œçš„å…§å®¹ç‚ºæ¨™é¡Œå€™é¸
##                 remaining = rest_text[and_match.end() + next_comma:].strip()
##                 remaining = remaining.lstrip(',ï¼Œ. ')
##                 
##                 title_match = re.match(r'^([^,ï¼Œ.ã€‚]+)', remaining)
##                 if title_match:
##                     potential_title = title_match.group(1).strip()
##                     if len(potential_title) > 10:
##                         title = potential_title
##                         result['title'] = title
##                 
##                 after_title = remaining
##         else:
##             # æ²’æœ‰ "and"ï¼Œå˜—è©¦ç”¨ç¬¬ä¸€å€‹é€—è™Ÿåˆ†éš”
##             parts = rest_text.split(',', 2)
##             
##             if len(parts) >= 2:
##                 potential_author = parts[0].strip()
##                 if potential_author and re.search(r'[a-zA-Z]', potential_author):
##                     authors = potential_author
##                     result['authors'] = authors
##                 
##                 potential_title = parts[1].strip()
##                 if len(potential_title) > 10:
##                     title = potential_title
##                     result['title'] = title
##                 
##                 if len(parts) >= 3:
##                     after_title = parts[2]
##     
##     # === 2. åˆ¤æ–·æ–‡ç»é¡å‹ ===
##     if re.search(r'\bin\b.*(Proc\.|Proceedings|Conference|Symposium|Workshop)', after_title, re.IGNORECASE):
##         result['source_type'] = 'Conference Paper'
##         conf_match = re.search(r'\bin\s+(.+?)(?:,|\d{4})', after_title, re.IGNORECASE)
##         if conf_match:
##             result['conference_name'] = conf_match.group(1).strip()
##     
##     elif re.search(r'(vol\.|volume|no\.|number)', after_title, re.IGNORECASE):
##         result['source_type'] = 'Journal Article'
##         journal_match = re.search(r'^([^,]+)', after_title)
##         if journal_match:
##             result['journal_name'] = journal_match.group(1).strip()
##     
##     elif re.search(r'\[Online\]|Available:|https?://', after_title, re.IGNORECASE):
##         result['source_type'] = 'Website/Online'
##
##     elif re.search(r'\[Online\]|Available:|https?://|arxiv\.org', after_title, re.IGNORECASE):
##         result['source_type'] = 'Website/Online'
##
##     elif re.search(r'(Ph\.D\.|M\.S\.|thesis|dissertation)', after_title, re.IGNORECASE):
##         result['source_type'] = 'Thesis/Dissertation'
##     
##     elif re.search(r'(Tech\. Rep\.|Technical Report)', after_title, re.IGNORECASE):
##         result['source_type'] = 'Technical Report'
##     
##     elif re.search(r'Patent', after_title, re.IGNORECASE):
##         result['source_type'] = 'Patent'
##     
##     elif re.search(r'(Ed\.|Eds\.|edition)', after_title, re.IGNORECASE):
##         result['source_type'] = 'Book'
##     
##     # === 3. æå–é€šç”¨æ¬„ä½ ===
##
##     # å·è™Ÿ
##     vol_match = re.search(r'vol\.\s*(\d+)', after_title, re.IGNORECASE)
##     if vol_match:
##         result['volume'] = vol_match.group(1)
##
##     # æœŸè™Ÿ
##     issue_match = re.search(r'no\.\s*(\d+)', after_title, re.IGNORECASE)
##     if issue_match:
##         result['issue'] = issue_match.group(1)
##
##     # é ç¢¼ï¼ˆæ”¹é€²ï¼šæ›´ç²¾ç¢ºçš„åŒ¹é…ï¼Œé¿å…æŠ“åˆ°æˆæ¬Šè³‡è¨Šä¸­çš„æ•¸å­—ï¼‰
##     pages_match = re.search(r'pp\.\s*([\d]+\s*[â€“\-â€”]\s*[\d]+)', after_title, re.IGNORECASE)
##     if pages_match:
##         # æ¸…ç†é ç¢¼ä¸­çš„ç©ºæ ¼
##         pages = pages_match.group(1)
##         pages = re.sub(r'\s+', '', pages)  # ç§»é™¤ç©ºæ ¼
##         pages = pages.replace('â€“', '-').replace('â€”', '-')  # çµ±ä¸€é€£å­—ç¬¦
##         result['pages'] = pages
##
##     # å¹´ä»½
##     year_matches = re.findall(r'\b(19\d{2}|20\d{2})\b', after_title)
##     if year_matches:
##         result['year'] = year_matches[0]  # å–ç¬¬ä¸€å€‹å¹´ä»½ï¼ˆé¿å…æŠ“åˆ°ä¸‹è¼‰æ—¥æœŸï¼‰
##
##     # æœˆä»½
##     month_match = re.search(r'\b(Jan\.|Feb\.|Mar\.|Apr\.|May|Jun\.|Jul\.|Aug\.|Sep\.|Oct\.|Nov\.|Dec\.)\b', 
##                         after_title, re.IGNORECASE)
##     if month_match:
##         result['month'] = month_match.group(1)
##
##     # URLï¼ˆæ”¹é€²ï¼šæ”¯æ´ arXivã€GitHub ç­‰å„ç¨® URLï¼Œå«ç©ºæ ¼è™•ç†ï¼‰
##     url = None
##
##     # 1. å„ªå…ˆç›´æ¥æŠ“ Available: / Retrieved from å¾Œçš„ç¶²å€ï¼Œå…¨é•·ä¸”å…è¨±ç©ºç™½
##     url_match = re.search(r'(?:Available:|Retrieved from)\s*(https?://[^\s,]+(?:\s+[^\s,]+)*)', after_title, re.IGNORECASE)
##     if url_match:
##         # åˆä½µæ‰€æœ‰æ›è¡Œèˆ‡ç©ºç™½ä½¿å…¶ç‚ºä¸€è¡Œ
##         url = url_match.group(1).strip().replace(' ', '')
##
##     # 2. å¦‚æœæ²’æŠ“åˆ°ï¼Œå†æŠ“æ‰€æœ‰ http é–‹é ­åˆ°é‡åˆ°ç©ºç™½/é€—è™Ÿ/å¥è™Ÿ/çµå°¾
##     if not url:
##         generic_url_match = re.search(r'(https?://[^\s,.;]+)', after_title)
##         if generic_url_match:
##             url = generic_url_match.group(1).strip()
##
##     # 3. æœ€å¾Œå¯«å…¥ result
##     if url:
##         result['url'] = url
##
##     # å­˜å–æ—¥æœŸï¼ˆæ”¹é€²ï¼šæ›´ç²¾ç¢ºåŒ¹é…ï¼‰
##     access_match = re.search(
##         r'(?:accessed|retrieved|downloaded)\s+(?:on\s+)?([A-Za-z]+\.?\s+\d{1,2},?\s*\d{4})', 
##         after_title, 
##         re.IGNORECASE
##     )
##     if access_match:
##         result['access_date'] = access_match.group(1)
##
##     # DOIï¼ˆæ–°å¢ï¼šæ”¯æ´å¤šç¨® DOI æ ¼å¼ï¼‰
##     doi_patterns = [
##         r'doi:\s*(10\.\d{4,}/[^\s,]+)',  # æ¨™æº–æ ¼å¼ï¼šdoi: 10.xxxx/xxxxx
##         r'https?://(?:dx\.)?doi\.org/(10\.\d{4,}/[^\s,]+)',  # URL æ ¼å¼ï¼šhttps://doi.org/10.xxxx/xxxxx
##         r'DOI:\s*(10\.\d{4,}/[^\s,]+)',  # å¤§å¯« DOI
##     ]
##
##     for pattern in doi_patterns:
##         doi_match = re.search(pattern, after_title, re.IGNORECASE)
##         if doi_match:
##             result['doi'] = doi_match.group(1).rstrip('.,;')
##             break
##
##     # å‡ºç‰ˆç¤¾èˆ‡åœ°é»
##     publisher_match = re.search(
##         r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*),\s+([A-Z]{2,}(?:,\s+[A-Z]{2,})?)\s*:\s*([^,]+)', 
##         after_title
##     )
##     if publisher_match:
##         result['location'] = publisher_match.group(1) + ', ' + publisher_match.group(2)
##         result['publisher'] = publisher_match.group(3)
##
##     # ç‰ˆæœ¬
##     edition_match = re.search(r'(\d+(?:st|nd|rd|th)\s+ed\.)', after_title, re.IGNORECASE)
##     if edition_match:
##         result['edition'] = edition_match.group(1)
##
##     # å ±å‘Šç·¨è™Ÿ
##     report_match = re.search(r'(Tech\.\s+Rep\.|Rep\.)\s+([\w\-]+)', after_title, re.IGNORECASE)
##     if report_match:
##         result['report_number'] = report_match.group(2)
##
##     # å°ˆåˆ©è™Ÿ
##     patent_match = re.search(r'(U\.S\.|US)\s+Patent\s+([\d,]+)', after_title, re.IGNORECASE)
##     if patent_match:
##         result['patent_number'] = patent_match.group(2)
##     
##     return result
##
##
##
## def extract_apa_reference_info_fixed(ref_text):
##     """
##     APA æ ¼å¼æ“·å–ï¼ˆä¿®æ­£å¤šä½œè€…å•é¡Œ + æ”¯æ´å®Œæ•´æ—¥æœŸ + æ”¹é€²æ¨™é¡Œæ“·å–ï¼‰
##     """
##     
##     # æ‰¾å¹´ä»½ï¼ˆå¿…é ˆæœ‰æ‹¬è™Ÿ,æ”¯æ´å®Œæ•´æ—¥æœŸæ ¼å¼ï¼‰
##     year_match = re.search(
##         r'[ï¼ˆ(]\s*(\d{4}[a-z]?|n\.d\.)\s*(?:,\s*([A-Za-z]+\.?\s*\d{0,2}))?\s*[ï¼‰)]',
##         ref_text, 
##         re.IGNORECASE
##     )
##     
##     if not year_match:
##         return {
##             'author': 'Unknown',
##             'year': 'Unknown',
##             'date': None,
##             'title': None
##         }
##     
##     year_text = year_match.group(1)
##     year = year_text[:4] if year_text.lower() != 'n.d.' else 'n.d.'
##     
##     # æå–å®Œæ•´æ—¥æœŸï¼ˆå¦‚æœæœ‰æœˆä»½æ—¥æœŸï¼‰
##     date_str = year_match.group(2) if year_match.group(2) else None
##     
##     # æå–ä½œè€…ï¼ˆå¹´ä»½å‰çš„å…§å®¹ï¼‰
##     before_year = ref_text[:year_match.start()].strip()
##     author = "Unknown"
##     
##     if before_year:
##         # ç§»é™¤æœ«å°¾çš„æ¨™é»å’Œç©ºæ ¼
##         before_year = before_year.rstrip(',ï¼Œ. ')
##         
##         # æª¢æŸ¥é•·åº¦å’Œå…§å®¹
##         if 2 <= len(before_year) <= 300:
##             # æ’é™¤ç„¡æ•ˆçš„ä½œè€…å
##             invalid_patterns = [
##                 r'^\d+$',  # ç´”æ•¸å­—
##                 r'^[ï¼Œ,\.ã€‚]+$',  # ç´”æ¨™é»
##             ]
##             
##             is_valid = True
##             for pattern in invalid_patterns:
##                 if re.match(pattern, before_year, re.IGNORECASE):
##                     is_valid = False
##                     break
##             
##             # ç›´æ¥ä½¿ç”¨æ•´å€‹ before_yearï¼ˆä¿ç•™å¤šä½œè€…ï¼‰
##             if is_valid and re.search(r'[a-zA-Z\u4e00-\u9fff]', before_year):
##                 author = before_year
##     
##     # æå–æ¨™é¡Œï¼ˆå¹´ä»½å¾Œçš„å…§å®¹ï¼‰
##     after_year = ref_text[year_match.end():].strip()
##     title = None
##     
##     if after_year:
##         # ç§»é™¤é–‹é ­çš„æ¨™é»ç¬¦è™Ÿå’Œç©ºæ ¼
##         after_year = re.sub(r'^[\s.,ï¼Œã€‚)\]ã€‘]+', '', after_year)
##         
##         if after_year:
##             # å…ˆè™•ç†ç‰¹æ®Šæƒ…æ³ï¼šæ–œé«”æ¨™è¨˜
##             after_year = re.sub(r'</?i>', '', after_year)
##             
##             # 1. å…ˆæ‰¾æ˜¯å¦æœ‰æ˜ç¢ºçš„æ¨™é¡ŒçµæŸæ¨™è¨˜
##             title_end_markers = [
##                 r'Retrieved from',
##                 r'Available from',
##                 r'Available at',
##                 r'\[Electronic version\]',
##                 r'DOI:',
##                 r'doi:',
##                 r'https?://',
##             ]
##             
##             title_end_pos = len(after_year)
##             for marker in title_end_markers:
##                 match = re.search(marker, after_year, re.IGNORECASE)
##                 if match and match.start() < title_end_pos:
##                     title_end_pos = match.start()
##             
##             # å–æ¨™é¡ŒçµæŸæ¨™è¨˜å‰çš„å…§å®¹
##             title_candidate = after_year[:title_end_pos].strip()
##             
##             # 2. æ¸…ç†æ¨™é¡Œæœ«å°¾
##             # ç§»é™¤æœ«å°¾çš„æœŸåˆŠè³‡è¨Šæ¨™è¨˜
##             title_candidate = re.sub(
##                 r'\s*[\.,]\s*$',
##                 '',
##                 title_candidate
##             )
##             
##             # ç§»é™¤å¯èƒ½çš„æœŸåˆŠåç¨±ï¼ˆæ–œé«”æ¨™è¨˜å¾Œçš„å…§å®¹ï¼‰
##             # ä¾‹å¦‚: "Title. Journal Name" -> "Title"
##             if '.' in title_candidate:
##                 parts = title_candidate.split('.')
##                 # å¦‚æœç¬¬ä¸€éƒ¨åˆ†å¤ é•·,å°±ç”¨ç¬¬ä¸€éƒ¨åˆ†
##                 if len(parts[0].strip()) >= 10:
##                     title_candidate = parts[0].strip()
##             
##             # 3. é©—è­‰æ¨™é¡Œ
##             if len(title_candidate) >= 5:
##                 if not re.match(r'^(Retrieved|Available|DOI|doi|https?|www)', title_candidate, re.IGNORECASE):
##                     if not (title_candidate.isupper() and len(title_candidate) < 20):
##                         title = title_candidate
##     
##     return {
##         'author': author,
##         'year': year,
##         'date': date_str,
##         'title': title
##     }
##
## def extract_apalike_reference_info_fixed(ref_text):
##     """ä¿®æ­£ç‰ˆ APA_LIKE æ ¼å¼æ“·å–"""
##     
##     years = find_apalike(ref_text)
##     
##     if not years:
##         return {
##             'author': 'Unknown',
##             'year': 'Unknown',
##             'title': None,
##             'format': 'APA_LIKE'
##         }
##     
##     year_str, year_pos = years[-1]
##     year = year_str[:4]
##     
##     before_year = ref_text[:year_pos].strip()
##     
##     author = "Unknown"
##     title = None
##     
##     parts = re.split(r'[,ï¼Œ.ã€‚]', before_year)
##     parts = [p.strip() for p in parts if p.strip()]
##     
##     if parts:
##         if parts[0] and re.search(r'[a-zA-Z\u4e00-\u9fff]', parts[0]):
##             author = parts[0]
##         
##         if len(parts) > 1:
##             potential_title = parts[1]
##             if potential_title and len(potential_title) > 5:
##                 title = potential_title
##     
##     return {
##         'author': author,
##         'year': year,
##         'title': title,
##         'format': 'APA_LIKE'
##     }
##
##
## def merge_ieee_references(ref_paragraphs):
##     """å¢å¼·ç‰ˆ IEEE æ®µè½åˆä½µ"""
##     merged = []
##     current_ref = ""
##     
##     for para in ref_paragraphs:
##         para = para.strip()
##         if not para:
##             continue
##         
##         if re.match(r'^\s*[ã€\[]\s*\d+\s*[ã€‘\]]\s*', para):
##             if current_ref:
##                 merged.append(current_ref.strip())
##             current_ref = para
##             continue
##         
##         if len(para) < 20:
##             current_ref += " " + para
##             continue
##         
##         if re.match(r'^[a-z]', para):
##             current_ref += " " + para
##             continue
##         
##         if re.match(r'^[,ï¼Œ.ã€‚;ï¼›:"\'\-]', para):
##             current_ref += " " + para
##             continue
##         
##         if any(keyword in para for keyword in ['http', 'doi:', 'DOI:', '[Online]', 'Available', '.pdf', '.doi']):
##             current_ref += " " + para
##             continue
##         
##         if current_ref and not re.search(r'[.ã€‚!ï¼?ï¼Ÿ]$', current_ref.strip()):
##             current_ref += " " + para
##             continue
##         
##         if re.match(r'^[A-Z][a-z]+(?:\s+and\s+|\s*,\s*)', para):
##             current_ref += " " + para
##             continue
##         
##         if current_ref:
##             current_ref += " " + para
##         else:
##             current_ref = para
##     
##     if current_ref:
##         merged.append(current_ref.strip())
##     
##     return merged
##
##
## def extract_reference_info(ref_paragraphs):
##     """å¾åƒè€ƒæ–‡ç»æ®µè½ä¸­æå–åŸºæœ¬è³‡è¨Šï¼ˆä½¿ç”¨ä¿®æ­£ç‰ˆï¼‰"""
##     if not ref_paragraphs:
##         return []
##     
##     first_ref = normalize_text(ref_paragraphs[0])
##     is_ieee_format = re.match(r'^\s*[ã€\[]\s*\d+\s*[ã€‘\]]\s*', first_ref)
##     
##     if is_ieee_format:
##         ref_paragraphs = merge_ieee_references(ref_paragraphs)
##     
##     ref_list = []
    
##     for ref_text in ref_paragraphs:
##         format_type = detect_reference_format(ref_text)
##         
##         if format_type == 'IEEE':
##             ieee_info = extract_ieee_reference_info_fixed(ref_text)
##             
##             if ieee_info:
##                 ref_list.append({
##                     'author': ieee_info['authors'],
##                     'year': ieee_info['year'],
##                     'date': None,
##                     'ref_number': ieee_info['ref_number'],
##                     'title': ieee_info['title'],
##                     'format': 'IEEE',
##                     'original': ref_text
##                 })
##             else:
##                 ref_list.append({
##                     'author': 'Parse Error',
##                     'year': 'Unknown',
##                     'date': None,
##                     'ref_number': 'Unknown',
##                     'title': None,
##                     'format': 'IEEE',
##                     'original': ref_text
##                 })
##         
##         elif format_type == 'APA':
##             apa_info = extract_apa_reference_info_fixed(ref_text)
##             ref_list.append({
##                 'author': apa_info['author'],
##                 'year': apa_info['year'],
##                 'date': apa_info.get('date'), 
##                 'ref_number': None,
##                 'title': apa_info['title'],
##                 'format': 'APA',
##                 'original': ref_text
##             })
##         
##         elif format_type == 'APA_LIKE':
##             apalike_info = extract_apalike_reference_info_fixed(ref_text)
##             ref_list.append({
##                 'author': apalike_info['author'],
##                 'year': apalike_info['year'],
##                 'date': None,
##                 'ref_number': None,
##                 'title': apalike_info['title'],
##                 'format': 'APA_LIKE',
##                 'original': ref_text
##             })
##         
##         else:
##             ref_list.append({
##                 'author': 'Unknown Format',
##                 'year': 'Unknown',
##                 'date': None,
##                 'ref_number': None,
##                 'title': None,
##                 'format': 'Unknown',
##                 'original': ref_text
##             })
##     
##     return ref_list


# ==================== [NEW] test1201 + 1204 æ–·è¡Œåˆä½µé‚è¼¯ (æ–°ç‰ˆ) ====================

def find_apa_head(ref_text):
    """[NEW] åµæ¸¬ APA æ ¼å¼é–‹é ­ (å¹´ä»½) - å–ä»£èˆŠçš„ find_apa"""
    # è‹±æ–‡ APA: Author (2020).
    # ä¸­æ–‡ APA: ä½œè€… (2020)ã€‚
    match = re.search(r'[ï¼ˆ(]\s*(\d{4}(?:[a-z])?|n\.d\.)\s*(?:,\s*([A-Za-z]+\.?\s*\d{0,2}))?\s*[)ï¼‰]', ref_text)
    if not match: return False
    
    # ç¢ºä¿å¹´ä»½æ‹¬è™Ÿå‡ºç¾åœ¨å‰é¢ (ä¾‹å¦‚å‰ 50 å€‹å­—å…§ï¼Œé¿å…èª¤åˆ¤æ–‡ä¸­çš„å¹´ä»½)
    if match.start() > 80: return False 
    
    return True

# [OLD] èˆŠç‰ˆ is_reference_head_unified (å·²è¢«ä¸‹æ–¹æ–°ç‰ˆå–ä»£)
# def is_reference_head_unified(para):
#     """
#     [NEW] åˆ¤æ–·ä¸€è¡Œæ–‡å­—æ˜¯å¦ç‚ºä¸€æ¢æ–°æ–‡ç»çš„é–‹é ­
#     """
#     para = normalize_text(para)
#     if re.match(r'^\s*[\[ã€]\s*\d+\s*[ã€‘\]]', para):
#         return True
#     if find_apa_head(para):
#         return True
#     year_match = re.search(r'^.*[\.,]\s*(19|20)\d{2}[a-z]?[\.,]', para[:50])
#     if year_match:
#         return True
#     return False

def is_reference_head_unified(para):
    """
    [UPDATED from test1204-6] [APA/æ··åˆæ¨¡å¼] åˆ¤æ–·ä¸€è¡Œæ–‡å­—æ˜¯å¦ç‚ºæ–°æ–‡ç» (å·²åŠ å¼·é˜²èª¤åˆ¤)
    """
    para = normalize_text(para)
    
    # 1. ğŸš« é»‘åå–®ï¼šçµ•å°ä¸æ˜¯æ–°æ–‡ç»çš„æƒ…æ³
    
    # A. ç¶²å€ä¿è­· (é¿å… DOI æ–·è¡Œè¢«ç•¶æˆæ–°æ–‡ç»)
    if re.search(r'(https?://|doi\.org|doi:|www\.)', para, re.IGNORECASE):
        # é™¤éé€™è¡ŒåŒæ™‚æœ‰å¼·çƒˆçš„ç·¨è™Ÿç‰¹å¾µ [1]ï¼Œå¦å‰‡è¦–ç‚ºç¶²å€
        if not re.match(r'^\s*[\[ã€]', para):
            return False
            
    # B. æœˆä»½/æ—¥æœŸä¿è­· (é¿å… "Mar. 2022." è¢«èª¤åˆ¤)
    # æª¢æŸ¥é–‹é ­æ˜¯å¦ç‚ºè‹±æ–‡æœˆä»½
    if re.match(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*\d{4}', para, re.IGNORECASE):
        return False
        
    # C. å·æœŸé ç¢¼ä¿è­· (é¿å… Vol. 2, pp. 123)
    if re.match(r'^(Vol\.|No\.|pp\.|p\.|Page)', para, re.IGNORECASE):
        return False
        
    # D. å°å¯«é–‹é ­ä¿è­· (è‹±æ–‡äººåé€šå¸¸å¤§å¯«)
    if re.match(r'^[a-z]', para):
        return False

    # 2. âœ… ç™½åå–®ï¼šç¬¦åˆé€™äº›ç‰¹å¾µå°±æ˜¯æ–°æ–‡ç»
    
    # A. ç·¨è™Ÿæ ¼å¼ [1]
    if re.match(r'^\s*[\[ã€]\s*\d+\s*[ã€‘\]]', para):
        return True
        
    # B. APA æ¨™æº–æ ¼å¼ (Year)
    if find_apa_head(para):
        return True
        
    # C. é¡ APA (Year in dots, e.g., Author. 2020.)
    # åš´æ ¼é™åˆ¶ï¼šå¹´ä»½å‰å¿…é ˆæœ‰æ¨™é»ï¼Œä¸”å‰é¢è¦æœ‰è¶³å¤ é•·åº¦çš„æ–‡å­—(ä½œè€…å)
    year_match = re.search(r'[\.,]\s*(19|20)\d{2}[a-z]?[\.,]', para[:80])
    if year_match:
        pre_text = para[:year_match.start()].strip()
        if len(pre_text) > 3: # ä½œè€…åé€šå¸¸ > 3å­—
            # å¦‚æœæ˜¯ç´”è‹±æ–‡ï¼Œé€šå¸¸è¦æœ‰é€—è™Ÿ (Last, F.)
            if not has_chinese(para):
                if ',' in pre_text or '.' in pre_text:
                    return True
            else:
                return True # ä¸­æ–‡åå­—è¼ƒçŸ­ä¸”ä¸ä¸€å®šæœ‰é€—è™Ÿ

    return False

def merge_references_unified(paragraphs):
    """[UPDATED from test1204-6] [APA/æ··åˆæ¨¡å¼] åˆä½µæ–·è¡Œ"""
    merged = []
    current_ref = ""
    
    for para in paragraphs:
        para = para.strip()
        if not para: continue
        
        # æ’é™¤ç´”æ•¸å­—é ç¢¼ (é•·åº¦çŸ­ä¸”ç„¡é€£å­—è™Ÿ)
        if para.isdigit() and len(para) < 4: continue
        
        if is_reference_head_unified(para):
            if current_ref:
                merged.append(current_ref)
            current_ref = para
        else:
            if current_ref:
                if has_chinese(current_ref[-1:]) and has_chinese(para[:1]):
                    current_ref += "" + para
                elif current_ref.endswith('-'): 
                    current_ref = current_ref[:-1] + para
                else:
                    current_ref += " " + para
            else:
                current_ref = para
            
    if current_ref: merged.append(current_ref)
    return merged

def merge_references_ieee_strict(paragraphs):
    """
    [NEW from test1204-6] [IEEE å°ˆç”¨æ¨¡å¼] åš´æ ¼åˆä½µ
    åªèª [n] é–‹é ­ï¼Œå…¶ä»–ä¸€å¾‹è¦–ç‚ºä¸Šä¸€è¡Œçš„å»¶çºŒã€‚
    è§£æ±º Mar. 2022 æˆ– æ–·è¡Œ DOI å•é¡Œã€‚
    """
    merged = []
    current_ref = ""
    pattern_index = re.compile(r'^\s*[\[ã€]\s*\d+\s*[ã€‘\]]')
    
    for para in paragraphs:
        para = para.strip()
        if not para: continue
        
        # æ’é™¤ç´”æ•¸å­—é ç¢¼
        if para.isdigit() and len(para) < 5: continue
        
        if pattern_index.match(para):
            if current_ref:
                merged.append(current_ref)
            current_ref = para
        else:
            if current_ref:
                # è™•ç†æ–·å­—
                if current_ref.endswith('-'):
                    current_ref = current_ref[:-1] + para
                # è™•ç†ä¸­è‹±æ–‡é–“è·
                elif has_chinese(current_ref[-1:]) and has_chinese(para[:1]):
                    current_ref += para
                else:
                    current_ref += " " + para
            else:
                # è¬ä¸€ç¬¬ä¸€è¡Œå°±æ²’æŠ“åˆ°ç·¨è™Ÿï¼Œå…ˆç•¶ä½œç¬¬ä¸€æ¢
                current_ref = para
                
    if current_ref: merged.append(current_ref)
    return merged


# ==================== [NEW] test1201 è©³ç´°è§£æå¼•æ“ (å·²å•Ÿç”¨) ====================

# --- è‹±æ–‡è§£æ ---
def parse_apa_authors_en(author_str):
    if not author_str: return []
    clean_str = re.sub(r'\s+(&|and)\s+', ' ', author_str)
    segments = re.split(r'\.,\s*', clean_str)
    authors = []
    for seg in segments:
        seg = seg.strip()
        if not seg: continue
        if not seg.endswith('.'): seg += '.'
        if ',' in seg:
            parts = seg.split(',', 1)
            authors.append({'last': parts[0].strip(), 'first': parts[1].strip()})
        else:
            authors.append({'last': seg, 'first': ''})
    return authors

def extract_apa_en_detailed(ref_text):
    result = {
        'format': 'APA (EN)', 'lang': 'EN',
        'authors': "Unknown", 'parsed_authors': [],
        'year': None, 'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)
    
    year_match = re.search(r'[ï¼ˆ(]\s*(\d{4}[a-z]?|n\.d\.)\s*(?:,\s*[A-Za-z]+\.?\s*\d{0,2})?\s*[)ï¼‰]', ref_text)
    if not year_match: return result
    
    result['year'] = year_match.group(1)
    author_part = ref_text[:year_match.start()].strip()
    result['authors'] = author_part
    result['parsed_authors'] = parse_apa_authors_en(author_part)
    
    content_part = ref_text[year_match.end():].strip()
    if content_part.startswith('.'): content_part = content_part[1:].strip()
    if result['doi']:
        content_part = re.sub(r'(?:doi:|DOI:|https?://doi\.org/)\s*10\.\d{4,}/[^\sã€‚]+', '', content_part).strip()

    meta_match = re.search(r',\s*(\d+)(?:\s*\((\d+)\))?,\s*([\d\â€“\-]+)(?:\.)?$', content_part)
    if meta_match:
        result['volume'] = meta_match.group(1)
        result['issue'] = meta_match.group(2)
        result['pages'] = meta_match.group(3)
        title_source_part = content_part[:meta_match.start()].strip()
    else:
        pp_match = re.search(r',?\s*pp?\.?\s*([\d\â€“\-]+)(?:\.)?$', content_part)
        if pp_match:
            result['pages'] = pp_match.group(1)
            title_source_part = content_part[:pp_match.start()].strip()
        else:
            title_source_part = content_part

    split_index = title_source_part.rfind('. ')
    if split_index != -1:
        result['title'] = title_source_part[:split_index + 1].strip().rstrip('.')
        result['source'] = title_source_part[split_index + 1:].strip()
    else:
        result['title'] = title_source_part
    return result

def parse_ieee_authors(author_str):
    if not author_str: return []
    author_str = re.sub(r'^\[\d+\]\s*', '', author_str)
    clean_str = re.sub(r'\s+,?\s+and\s+', ',', author_str, flags=re.IGNORECASE)
    segments = clean_str.split(',')
    authors = []
    for seg in segments:
        seg = seg.strip()
        if not seg: continue
        parts = seg.split()
        if len(parts) > 1:
            authors.append({'last': parts[-1], 'first': " ".join(parts[:-1])})
        else:
            authors.append({'last': seg, 'first': ''})
    return authors

def extract_ieee_en_detailed(ref_text):
    """[Updated from test1204-6] è‹±æ–‡ IEEE è©³ç´°è§£æ (å¢å¼·å¹´ä»½æŠ“å–)"""
    result = {
        'format': 'IEEE (EN)', 'lang': 'EN',
        'ref_number': None, 'authors': "Unknown", 'parsed_authors': [],
        'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None, 'year': None,
        'doi': None, 'original': ref_text
    }
    num_match = re.match(r'^\s*[\[ã€]\s*(\d+)\s*[\]ã€‘]', ref_text)
    if num_match:
        result['ref_number'] = num_match.group(1)
        rest_text = ref_text[num_match.end():].strip()
    else:
        rest_text = ref_text

    result['doi'] = extract_doi(rest_text)
    
    # [Updated] æ‰¾æœ€å¾Œå‡ºç¾çš„å¹´ä»½ (IEEE å¹´ä»½é€šå¸¸åœ¨å¾Œé¢)
    year_match = re.findall(r'\b(19\d{2}|20\d{2})\b', rest_text)
    if year_match: result['year'] = year_match[-1]
    
    vol_match = re.search(r'vol\.\s*(\d+)', rest_text, re.IGNORECASE)
    if vol_match: result['volume'] = vol_match.group(1)
    
    no_match = re.search(r'no\.\s*(\d+)', rest_text, re.IGNORECASE)
    if no_match: result['issue'] = no_match.group(1)
    
    pp_match = re.search(r'pp\.\s*([\d\â€“\-]+)', rest_text, re.IGNORECASE)
    if pp_match: result['pages'] = pp_match.group(1)

    quote_match = re.search(r'["â€œ](.+?)["â€]', rest_text)
    if quote_match:
        result['title'] = quote_match.group(1).strip().rstrip(',.')
        before_quote = rest_text[:quote_match.start()].strip().rstrip(',. ')
        if before_quote:
            result['authors'] = before_quote
            result['parsed_authors'] = parse_ieee_authors(before_quote)
            
        after_quote = rest_text[quote_match.end():].strip()
        # [Updated] æ›´ç²¾ç¢ºçš„ä¾†æºæ¸…ç†
        end_indicators = [r'vol\.', r'no\.', r'pp\.', r'\b19\d{2}\b', r'\b20\d{2}\b']
        min_pos = len(after_quote)
        for ind in end_indicators:
            m = re.search(ind, after_quote, re.IGNORECASE)
            if m and m.start() < min_pos: min_pos = m.start()
        
        source_candidate = after_quote[:min_pos].strip().strip(',. ')
        result['source'] = re.sub(r'^in\s+', '', source_candidate, flags=re.IGNORECASE)
    else:
        parts = rest_text.split(',', 1)
        if len(parts) > 1:
            result['authors'] = parts[0].strip()
            result['title'] = parts[1].strip()
    return result

# --- ä¸­æ–‡è§£æ ---
def parse_chinese_authors(author_str):
    if not author_str: return []
    clean_str = re.sub(r'\s*(ç­‰|è‘—|ç·¨)$', '', author_str)
    return re.split(r'[ã€ï¼Œ,]', clean_str)

def extract_apa_zh_detailed(ref_text):
    result = {
        'format': 'APA (ZH)', 'lang': 'ZH',
        'authors': [], 'year': None, 'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)
    year_match = re.search(r'[ï¼ˆ(]\s*(\d{2,4})\s*[)ï¼‰]', ref_text)
    if not year_match: return result
    
    result['year'] = year_match.group(1)
    author_part = ref_text[:year_match.start()].strip()
    result['authors'] = parse_chinese_authors(author_part)
    
    rest = ref_text[year_match.end():].strip().lstrip('.ã€‚ ')
    match_book = re.search(r'ã€Š([^ã€‹]+)ã€‹', rest)
    match_article = re.search(r'ã€ˆ([^ã€‰]+)ã€‰', rest)
    
    if match_article:
        result['title'] = match_article.group(1)
        if match_book: result['source'] = match_book.group(1)
    elif match_book:
        pre_book = rest[:match_book.start()].strip()
        if pre_book:
            result['title'] = pre_book.rstrip('ã€‚. ')
            result['source'] = match_book.group(1)
        else:
            result['title'] = match_book.group(1)
    else:
        # [UPDATED] å¢åŠ å¾Œå‚™æ–¹æ¡ˆï¼Œå¦‚æœæ²’æœ‰æ›¸åè™Ÿï¼Œå˜—è©¦ç”¨å¥è™Ÿåˆ†éš”æŠ“å–ä¾†æº
        parts = re.split(r'[ã€‚.]', rest)
        # éæ¿¾ç©ºå­—ä¸²
        parts = [p.strip() for p in parts if p.strip()]
        if len(parts) > 0: result['title'] = parts[0]
        if len(parts) > 1: result['source'] = parts[1] # å˜—è©¦æŠ“å–ä¾†æº
            
    vol_match = re.search(r'(\d+)\s*[å·]', rest)
    if vol_match: result['volume'] = vol_match.group(1)
    return result

def extract_numbered_zh_detailed(ref_text):
    result = {
        'format': 'Numbered (ZH)', 'lang': 'ZH',
        'ref_number': None, 'authors': [], 'year': None, 'title': None, 'source': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)
    num_match = re.match(r'^\s*[\[ã€]\s*(\d+)\s*[\]ã€‘]', ref_text)
    if num_match:
        result['ref_number'] = num_match.group(1)
        rest = ref_text[num_match.end():].strip()
    else:
        rest = ref_text
    year_match = re.search(r'\b(\d{4})\b', rest)
    if year_match: result['year'] = year_match.group(1)
    
    match_book = re.search(r'ã€Š([^ã€‹]+)ã€‹', rest)
    if match_book:
        result['source'] = match_book.group(1)
        pre = rest[:match_book.start()]
        # å˜—è©¦æŠ“ä½œè€…å’Œç¯‡å
        parts = re.split(r'[ï¼Œ,]', pre)
        if len(parts) > 0: result['authors'] = parse_chinese_authors(parts[0])
        if len(parts) > 1: result['title'] = parts[1]
    else:
        # [UPDATED] å¢åŠ å¾Œå‚™æ–¹æ¡ˆï¼Œå˜—è©¦æŠ“å–ä¾†æº (å‡è¨­çµæ§‹: ä½œè€…, ç¯‡å, ä¾†æº)
        parts = re.split(r'[ï¼Œ,ã€‚.]', rest)
        parts = [p.strip() for p in parts if p.strip()]
        if len(parts) > 0: result['authors'] = parse_chinese_authors(parts[0])
        if len(parts) > 1: result['title'] = parts[1]
        if len(parts) > 2: result['source'] = parts[2] # å˜—è©¦æŠ“å–ä¾†æº

    return result

# --- æ ¸å¿ƒæ•´åˆåˆ†æµ ---
def process_single_reference(ref_text):
    """[NEW] æ ¸å¿ƒåˆ†æµé‚è¼¯ï¼Œæ ¹æ“šèªè¨€å’Œç‰¹å¾µé¸æ“‡è§£æå™¨"""
    ref_text = normalize_text(ref_text)
    
    if has_chinese(ref_text):
        if re.match(r'^\s*[\[ã€]', ref_text):
            data = extract_numbered_zh_detailed(ref_text)
        else:
            data = extract_apa_zh_detailed(ref_text)
    else:
        if re.match(r'^\s*[\[ã€]', ref_text):
            data = extract_ieee_en_detailed(ref_text)
        else:
            data = extract_apa_en_detailed(ref_text)
            
    # [é—œéµæ•´åˆ]ï¼šç¢ºä¿å›å‚³çš„å­—å…¸åŒ…å« 1204 æ¯”å°é‚è¼¯æ‰€éœ€çš„ 'author' (å­—ä¸²) æ¬„ä½
    if isinstance(data.get('authors'), list):
        # é€™è£¡ä¸»è¦æ˜¯ç‚ºäº†æ¯”å°é‚è¼¯ï¼Œå‰ç«¯é¡¯ç¤ºæœƒæœ‰å°ˆé–€çš„è™•ç†
        data['author'] = " ".join(data['authors']) 
    elif isinstance(data.get('authors'), str):
        data['author'] = data['authors']
    else:
        data['author'] = "Unknown"
        
    return data

# ==================== [NEW] test1201 æ ¼å¼è½‰æ›åŠŸèƒ½ ====================

def convert_en_apa_to_ieee(data):
    ieee_authors = []
    for auth in data.get('parsed_authors', []):
        ieee_authors.append(f"{auth['first']} {auth['last']}".strip())
    auth_str = ", ".join(ieee_authors)
    if len(ieee_authors) > 2: auth_str = re.sub(r', ([^,]+)$', r', and \1', auth_str)
    
    parts = []
    if auth_str: parts.append(auth_str + ",")
    if data.get('title'): parts.append(f'"{data["title"]},"')
    if data.get('source'): parts.append(f"{data['source']},")
    if data.get('volume'): parts.append(f"vol. {data['volume']},")
    if data.get('issue'): parts.append(f"no. {data['issue']},")
    if data.get('pages'): parts.append(f"pp. {data['pages']},")
    if data.get('year'): parts.append(f"{data['year']}.")
    if data.get('doi'): parts.append(f"doi: {data['doi']}.")
    return " ".join(parts)

def convert_en_ieee_to_apa(data):
    apa_authors = []
    for auth in data.get('parsed_authors', []):
        apa_authors.append(f"{auth['last']}, {auth['first']}".strip())
    auth_str = ", ".join(apa_authors)
    if len(apa_authors) > 1: auth_str = re.sub(r', ([^,]+)$', r', & \1', auth_str)
    
    parts = []
    if auth_str: parts.append(auth_str)
    if data.get('year'): parts.append(f"({data['year']}).")
    if data.get('title'): parts.append(f"{data['title']}.")
    if data.get('source'): parts.append(f"*{data['source']}*,")
    if data.get('doi'): parts.append(f"https://doi.org/{data['doi']}")
    return " ".join(parts)

def convert_zh_apa_to_num(data):
    parts = []
    # [UPDATED] ä¿®æ­£ä½œè€…é€£æ¥ç¬¦è™Ÿï¼Œlist è½‰å­—ä¸²
    if isinstance(data.get('authors'), list):
        auth = "ã€".join(data.get('authors'))
    else:
        auth = data.get('authors', '')
        
    if auth: parts.append(auth)
    if data.get('title'): parts.append(f"ã€Œ{data['title']}ã€")
    # [UPDATED] ç¢ºä¿å‡ºè™•æœ‰è¢«æŠ“åˆ°æ‰é¡¯ç¤º
    if data.get('source'): parts.append(f"ã€Š{data['source']}ã€‹")
    if data.get('year'): parts.append(data['year'])
    return "ï¼Œ".join(parts) + "ã€‚"

def convert_zh_num_to_apa(data):
    # [UPDATED] ä¿®æ­£ä½œè€…é€£æ¥ç¬¦è™Ÿ
    if isinstance(data.get('authors'), list):
        auth = "ã€".join(data.get('authors'))
    else:
        auth = data.get('authors', '')
        
    parts = []
    parts.append(f"{auth}ï¼ˆ{data.get('year', 'ç„¡å¹´ä»½')}ï¼‰")
    if data.get('title'): parts.append(data['title'])
    if data.get('source'): parts.append(f"ã€Š{data['source']}ã€‹")
    return "ã€‚".join(parts) + "ã€‚"

# ==================== 5. JSON æš«å­˜åŠŸèƒ½ ====================

def init_session_state():
    """session_state æ˜¯ Streamlit çš„è¨˜æ†¶é«”æš«å­˜æ©Ÿåˆ¶ï¼Œé é¢é‡æ–°æ•´ç†å¾Œè³‡æ–™ä¸æœƒæ¶ˆå¤±"""

    #å„²å­˜å…§æ–‡ä¸­çš„å¼•ç”¨
    if 'in_text_citations' not in st.session_state: 
        st.session_state.in_text_citations = []
    # å„²å­˜åƒè€ƒæ–‡ç»åˆ—è¡¨
    if 'reference_list' not in st.session_state:
        st.session_state.reference_list = []
    # å„²å­˜å·²é€é API é©—è­‰éçš„æ­£ç¢ºæ–‡ç»
    if 'verified_references' not in st.session_state:
        st.session_state.verified_references = []

def save_to_session(in_text_citations, reference_list):
    """å°‡è³‡æ–™å„²å­˜åˆ° session state"""
    st.session_state.in_text_citations = in_text_citations
    st.session_state.reference_list = reference_list

def export_to_json():
    """åŒ¯å‡ºç‚º JSON æ ¼å¼: å°‡ä¸‰å€‹æ¸…å–®æ‰“åŒ…æˆä¸€å€‹ JSON ç‰©ä»¶"""
    data = {
        "export_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "in_text_citations": st.session_state.in_text_citations,
        "reference_list": st.session_state.reference_list,
        "verified_references": st.session_state.verified_references
    }

    # ensure_ascii=Falseï¼šä¿ç•™ä¸­æ–‡å­—å…ƒ, indent=2ï¼šæ ¼å¼åŒ–è¼¸å‡ºï¼Œæ–¹ä¾¿é–±è®€
    return json.dumps(data, ensure_ascii=False, indent=2)

def import_from_json(json_str):
    """å¾ JSON åŒ¯å…¥è³‡æ–™"""
    try:
        data = json.loads(json_str)
        st.session_state.in_text_citations = data.get("in_text_citations", [])
        st.session_state.reference_list = data.get("reference_list", [])
        st.session_state.verified_references = data.get("verified_references", [])
        return True, "è³‡æ–™åŒ¯å…¥æˆåŠŸï¼"
    except Exception as e:
        return False, f"åŒ¯å…¥å¤±æ•—ï¼š{str(e)}"

def add_verified_reference(ref_data):
    """æ–°å¢å·²é©—è­‰çš„æ–‡ç»è³‡æ–™"""
    if 'verified_references' not in st.session_state:
        st.session_state.verified_references = []
    st.session_state.verified_references.append(ref_data)

# ==================== Streamlit UI ====================
st.set_page_config(page_title="æ–‡ç»æª¢æŸ¥ç³»çµ± V3", layout="wide")
#st.set_page_config(page_title="æ–‡ç»æª¢æŸ¥ç³»çµ± (Merged)", layout="wide")
# åˆå§‹åŒ– session state
init_session_state()
st.title("ğŸ“š å­¸è¡“æ–‡ç»å¼•ç”¨æª¢æŸ¥ç³»çµ±")
#st.title("ğŸŒ å…¨æ–¹ä½å­¸è¡“æ–‡ç»åˆ†æèˆ‡æ¯”å°ç³»çµ± (Merged)")

st.markdown("""
### âœ¨ åŠŸèƒ½ç‰¹è‰²
1. âœ… **åƒè€ƒæ–‡ç»æª¢æŸ¥**ï¼šæª¢æŸ¥æ–‡ç»æ˜¯å¦éƒ½è¢«å¼•ç”¨
2. âœ… **å…§æ–‡å¼•ç”¨æª¢æŸ¥**ï¼šæª¢æŸ¥å…§æ–‡ä¸­çš„å¼•ç”¨æ˜¯å¦éƒ½å°æ‡‰åƒè€ƒæ–‡ç»
3. âœ… **ä¸­è‹±æ–‡è¾¨è­˜ & æ ¼å¼è½‰æ› (New)**ï¼šè‡ªå‹•å€åˆ†ä¸­è‹±æ–‡ã€APA/IEEE äº’è½‰
4. âœ… **æ·±åº¦æ¬„ä½è§£æ (New)**ï¼šç²¾æº–æ‹†è§£ä½œè€…ã€å¹´ä»½ã€ç¯‡åã€DOI
5. âœ… **ç”Ÿæˆæª¢æŸ¥å ±è¡¨**ï¼šè¼¸å‡ºå®Œæ•´å ±å‘Š            
""")

st.markdown("---")

# ==================== JSON è³‡æ–™ç®¡ç†å€ ====================
with st.sidebar:
    st.header("ğŸ’¾ è³‡æ–™ç®¡ç†")
    
    # é¡¯ç¤ºç•¶å‰æš«å­˜ç‹€æ…‹
    st.subheader("ğŸ“Š ç•¶å‰æš«å­˜ç‹€æ…‹")
    st.metric("å…§æ–‡å¼•ç”¨æ•¸é‡", len(st.session_state.in_text_citations))
    st.metric("åƒè€ƒæ–‡ç»æ•¸é‡", len(st.session_state.reference_list))
    st.metric("å·²é©—è­‰æ–‡ç»", len(st.session_state.verified_references))
    
    st.markdown("---")
    
    # åŒ¯å‡ºåŠŸèƒ½
    st.subheader("ğŸ“¤ åŒ¯å‡ºè³‡æ–™")
    if st.button("åŒ¯å‡ºç‚º JSON", use_container_width=True):
        json_data = export_to_json()
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ JSON æª”æ¡ˆ",
            data=json_data,
            file_name=f"citation_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json",
            use_container_width=True
        )
    
    # åŒ¯å…¥åŠŸèƒ½
    st.subheader("ğŸ“¥ åŒ¯å…¥è³‡æ–™")
    uploaded_json = st.file_uploader("ä¸Šå‚³ JSON æª”æ¡ˆ", type=['json'])
    if uploaded_json:
        json_str = uploaded_json.read().decode('utf-8')
        success, message = import_from_json(json_str)
        if success:
            st.session_state.json_imported = True
            st.success(message)
        else:
            st.error(message)
            
    # æ¸…é™¤åŒ¯å…¥æ¨™è¨˜ï¼ˆç•¶æª”æ¡ˆè¢«ç§»é™¤æ™‚ï¼‰
    if not uploaded_json and 'json_imported' in st.session_state:
        del st.session_state.json_imported
    
    # æ¸…ç©ºè³‡æ–™
    st.markdown("---")
    st.subheader("ğŸ—‘ï¸ æ¸…ç©ºè³‡æ–™")
    if st.button("æ¸…ç©ºæ‰€æœ‰æš«å­˜", type="secondary", use_container_width=True):
        st.session_state.in_text_citations = []
        st.session_state.reference_list = []
        st.session_state.verified_references = []
        st.success("å·²æ¸…ç©ºæ‰€æœ‰æš«å­˜è³‡æ–™")
        st.rerun() #é‡æ–°è¼‰å…¥é é¢ï¼Œæ›´æ–°å´é‚Šæ¬„çš„æ•¸é‡é¡¯ç¤º

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ Word æˆ– PDF æª”æ¡ˆ", type=["docx", "pdf"])

# å¦‚æœæœ‰åŒ¯å…¥çš„è³‡æ–™ä½†æ²’æœ‰ä¸Šå‚³æª”æ¡ˆï¼Œé¡¯ç¤ºåŒ¯å…¥çš„è³‡æ–™
if not uploaded_file and (st.session_state.in_text_citations or st.session_state.reference_list):
    st.info("ğŸ“¥ é¡¯ç¤ºå·²åŒ¯å…¥çš„è³‡æ–™")

elif uploaded_file:
    file_ext = uploaded_file.name.split(".")[-1].lower()
    
    st.subheader(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š{uploaded_file.name}")
    
    with st.spinner("æ­£åœ¨è®€å–æª”æ¡ˆ..."):
        if file_ext == "docx":
            all_paragraphs = extract_paragraphs_from_docx(uploaded_file)
        elif file_ext == "pdf":
            all_paragraphs = extract_paragraphs_from_pdf(uploaded_file)
        else:
            st.error("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼")
            st.stop()
    
    st.success(f"âœ… æˆåŠŸè®€å– {len(all_paragraphs)} å€‹æ®µè½")
    
    st.markdown("---")
    
    content_paras, ref_paras, ref_start_idx, ref_keyword = classify_document_sections(all_paragraphs)
    
    
    
    st.subheader("ğŸ” å…§æ–‡å¼•ç”¨åˆ†æ")
    
    if content_paras:
        in_text_citations = extract_in_text_citations(content_paras)

        # å°‡å…§æ–‡å¼•ç”¨è½‰æ›ç‚ºå¯åºåˆ—åŒ–æ ¼å¼ä¸¦å„²å­˜ (ç¢ºä¿å¯ä»¥è½‰ç‚º JSON)
        serializable_citations = []
        for cite in in_text_citations:
            cite_dict = {
                'author': cite.get('author'),
                'co_author': cite.get('co_author'),
                'year': cite.get('year'),
                'ref_number': cite.get('ref_number'),
                'original': cite.get('original'),
                'normalized': cite.get('normalized'),
                'position': cite.get('position'),
                'type': cite.get('type'),
                'format': cite.get('format')
            }
            serializable_citations.append(cite_dict)
        
        # å„²å­˜åˆ° session state
        st.session_state.in_text_citations = serializable_citations
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                border-radius: 12px;
                padding: 24px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 20px; opacity: 0.9; margin-bottom: 8px;">å…§æ–‡å¼•ç”¨ç¸½æ•¸</div>
                <div style="font-size: 36px; font-weight: bold;">{len(in_text_citations)}</div>
            </div>
            """, unsafe_allow_html=True)
        
        apa_count = sum(1 for c in in_text_citations if c['format'] == 'APA')
        with col2:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
                border-radius: 12px;
                padding: 24px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 20px; opacity: 0.9; margin-bottom: 8px;">APA æ ¼å¼å¼•ç”¨</div>
                <div style="font-size: 36px; font-weight: bold;">{apa_count}</div>
            </div>
            """, unsafe_allow_html=True)
        
        ieee_count = sum(1 for c in in_text_citations if c['format'] == 'IEEE')
        with col3:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #0066cc 0%, #0080ff 100%);
                border-radius: 12px;
                padding: 24px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 20px; opacity: 0.9; margin-bottom: 8px;">IEEE æ ¼å¼å¼•ç”¨</div>
                <div style="font-size: 36px; font-weight: bold;">{ieee_count}</div>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        if in_text_citations:
            with st.expander("ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰å…§æ–‡å¼•ç”¨"):
                for i, cite in enumerate(in_text_citations, 1):
                    if cite['format'] == 'APA':
                        co_author_text = f" & {cite['co_author']}" if cite['co_author'] else ""
                        st.markdown(
                            f"{i}. `{cite['original']}` â€” "
                            f"**[{cite['format']}]** "
                            f"ä½œè€…ï¼š**{cite['author']}{co_author_text}** | "
                            f"å¹´ä»½ï¼š**{cite['year']}** | "
                            f"é¡å‹ï¼š{cite['type']}"
                        )
                    else:
                        st.markdown(
                            f"{i}. `{cite['original']}` â€” "
                            f"**[{cite['format']}]** "
                            f"åƒè€ƒç·¨è™Ÿï¼š**{cite['ref_number']}**"
                        )
        else:
            st.info("æœªæ‰¾åˆ°ä»»ä½•å…§æ–‡å¼•ç”¨")
    else:
        st.warning("ç„¡å…§æ–‡æ®µè½å¯ä¾›åˆ†æ")
    
    st.markdown("---")
    
    if ref_paras:
        st.subheader("ğŸ“– åƒè€ƒæ–‡ç»è©³ç´°è§£æèˆ‡è½‰æ› (æ•´åˆç‰ˆ)")
        
        # [UPDATED from test1204-6] è‡ªå‹•åµæ¸¬ IEEE æ¨¡å¼
        is_ieee_mode = False
        sample_count = min(len(ref_paras), 15)
        for i in range(sample_count):
            if re.match(r'^\s*[\[ã€]\s*\d+\s*[ã€‘\]]', ref_paras[i].strip()):
                is_ieee_mode = True
                break
        
        if is_ieee_mode:
            st.info("ğŸ’¡ åµæ¸¬åˆ° IEEE ç·¨è™Ÿæ ¼å¼ ([1], [2]...)ï¼Œå•Ÿç”¨**åš´æ ¼åˆ†å‰²æ¨¡å¼** (é˜²æ­¢æ—¥æœŸ/DOI æ–·è¡Œ)")
            merged_refs = merge_references_ieee_strict(ref_paras)
        else:
            st.info("ğŸ’¡ åµæ¸¬åˆ°ä¸€èˆ¬æ ¼å¼ (APA/ä¸­æ–‡)ï¼Œå•Ÿç”¨**æ™ºæ…§æ··åˆæ¨¡å¼**")
            merged_refs = merge_references_unified(ref_paras)
        
        # ä½¿ç”¨æ–°ç‰ˆçš„è©³ç´°è§£æå¼•æ“
        parsed_refs = [process_single_reference(r) for r in merged_refs]
        st.session_state.reference_list = parsed_refs
        
        st.info(f"æˆåŠŸè§£æå‡º {len(parsed_refs)} ç­†åƒè€ƒæ–‡ç»")
        
        # ==============================================================================
        # [NEW DISPLAY LOGIC] ä»¿ç…§ 1204github.py çš„é¡¯ç¤ºé¢¨æ ¼ (å¡ç‰‡ + åˆ†å€ + Icon)
        # ==============================================================================
        
        # 1. çµ±è¨ˆå¡ç‰‡å€åŸŸ
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                border-radius: 12px;
                padding: 20px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 18px; opacity: 0.9; margin-bottom: 6px;">åƒè€ƒæ–‡ç»ç¸½æ•¸</div>
                <div style="font-size: 28px; font-weight: bold;">{len(parsed_refs)}</div>
            </div>
            """, unsafe_allow_html=True)
        
        apa_refs_count = sum(1 for r in parsed_refs if 'APA' in r.get('format', ''))
        with col2:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
                border-radius: 12px;
                padding: 20px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 18px; opacity: 0.9; margin-bottom: 6px;">APA æ ¼å¼</div>
                <div style="font-size: 28px; font-weight: bold;">{apa_refs_count}</div>
            </div>
            """, unsafe_allow_html=True)
        
        ieee_refs_count = sum(1 for r in parsed_refs if 'IEEE' in r.get('format', ''))
        with col3:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #0066cc 0%, #0080ff 100%);
                border-radius: 12px;
                padding: 20px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 18px; opacity: 0.9; margin-bottom: 6px;">IEEE æ ¼å¼</div>
                <div style="font-size: 28px; font-weight: bold;">{ieee_refs_count}</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col4:
             st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #ff7675 0%, #ff9a3d 100%);
                border-radius: 12px;
                padding: 20px;
                text-align: center;
                color: white;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 18px; opacity: 0.9; margin-bottom: 6px;">å…¶ä»–/æ··åˆ</div>
                <div style="font-size: 28px; font-weight: bold;">0</div>
            </div>
            """, unsafe_allow_html=True)
             
        with col5:
             st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #95de64 0%, #b3e5fc 100%);
                border-radius: 12px;
                padding: 20px;
                text-align: center;
                color: #333;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            ">
                <div style="font-size: 18px; opacity: 0.9; margin-bottom: 6px;">æœªçŸ¥æ ¼å¼</div>
                <div style="font-size: 28px; font-weight: bold;">0</div>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # 2. IEEE ç¨ç«‹å±•ç¤ºå€ (ä»¿ 1204 é¢¨æ ¼)
        st.markdown("### ğŸ“– IEEE åƒè€ƒæ–‡ç»è©³ç´°è§£æ")
        ieee_list = [ref for ref in parsed_refs if 'IEEE' in ref.get('format', '')]
        
        if ieee_list:
            st.info(f"å…±æ‰¾åˆ° {len(ieee_list)} ç­† IEEE æ ¼å¼åƒè€ƒæ–‡ç»")
            
            for i, ref in enumerate(ieee_list, 1):
                # ç°¡å–®æ¨æ¸¬ source_type ä»¥é¸æ“‡ icon
                src = ref.get('source', '') or ''
                title_text = ref.get('title', 'æœªæä¾›æ¨™é¡Œ')
                ref_num = ref.get('ref_number', str(i))
                
                # Icon æ¨æ¸¬é‚è¼¯
                icon = 'ğŸ“„' 
                if any(x in src.lower() for x in ['proc.', 'proceedings', 'conference', 'symposium', 'workshop']):
                    icon = 'ğŸ“„' 
                elif any(x in src.lower() for x in ['journal', 'trans.', 'transactions', 'letters', 'magazine']):
                    icon = 'ğŸ“š' 
                elif any(x in src.lower() for x in ['thesis', 'dissertation']):
                    icon = 'ğŸ“' 
                elif 'http' in src.lower() or 'www' in src.lower():
                    icon = 'ğŸŒ' 
                
                with st.expander(f"{icon} [{ref_num}] {title_text}", expanded=False):
                    
                    c_info, c_action = st.columns([3, 1])
                    
                    with c_info:
                        # [FIX] ä½œè€…é¡¯ç¤ºè™•ç†ï¼šå¦‚æœæ˜¯ listï¼Œæ ¹æ“šèªè¨€åˆä½µæˆå­—ä¸²
                        if ref.get('authors'):
                            authors_data = ref['authors']
                            if isinstance(authors_data, list):
                                # ä¸­æ–‡ç”¨é “è™Ÿï¼Œè‹±æ–‡ç”¨é€—è™Ÿ
                                if ref.get('lang') == 'ZH':
                                    author_display = "ã€".join(authors_data)
                                else:
                                    # é€™è£¡å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´è‹±æ–‡çš„å¤šä½œè€…é¡¯ç¤ºæ–¹å¼ (å¦‚ ", " æˆ– " & ")
                                    author_display = ", ".join(authors_data)
                            else:
                                author_display = authors_data
                                
                            st.markdown(f"**ğŸ‘¥ ä½œè€…**")
                            st.markdown(f"ã€€â””â”€ {author_display}")
                        
                        if ref.get('source'):
                            st.markdown(f"**ğŸ“– ä¾†æº**")
                            st.markdown(f"ã€€â””â”€ {ref['source']}")
                            
                        vol_issue = []
                        if ref.get('volume'): vol_issue.append(f"Vol. {ref['volume']}")
                        if ref.get('issue'): vol_issue.append(f"No. {ref['issue']}")
                        if vol_issue:
                            st.markdown(f"**ğŸ“Š å·æœŸ**")
                            st.markdown(f"ã€€â””â”€ {', '.join(vol_issue)}")
                        
                        if ref.get('pages'):
                            st.markdown(f"**ğŸ“„ é ç¢¼**")
                            st.markdown(f"ã€€â””â”€ pp. {ref['pages']}")
                        
                        if ref.get('year'):
                            st.markdown(f"**ğŸ“… å¹´ä»½**ï¼š{ref['year']}")
                            
                        if ref.get('doi'):
                            st.markdown(f"**ğŸ” DOI**")
                            st.markdown(f"ã€€â””â”€ [{ref['doi']}](https://doi.org/{ref['doi']})")
                            
                        st.markdown("**ğŸ“ åŸæ–‡**")
                        st.code(ref['original'], language=None)
                    
                    with c_action:
                        st.markdown("**ğŸ› ï¸ æ ¼å¼è½‰æ›**")
                        if st.button("è½‰ APA", key=f"ieee_btn_apa_{i}"):
                            st.code(convert_en_ieee_to_apa(ref), language='text')

        else:
            st.info("æœªæ‰¾åˆ° IEEE æ ¼å¼åƒè€ƒæ–‡ç»")
            
        st.markdown("---")
        
        # 3. APA èˆ‡å…¶ä»–æ ¼å¼å±•ç¤ºå€
        st.markdown("### ğŸ“š APA èˆ‡å…¶ä»–æ ¼å¼åƒè€ƒæ–‡ç»")
        apa_list = [ref for ref in parsed_refs if 'APA' in ref.get('format', '') or 'Numbered' in ref.get('format', '')]
        
        with st.expander("ğŸ“‹ æŸ¥çœ‹ APA / ä¸­æ–‡æ ¼å¼å®Œæ•´è³‡è¨Š"):
            for i, ref in enumerate(apa_list, 1):
                fmt = ref.get('format')
                title_display = ref.get('title') or "ç„¡æ¨™é¡Œ"
                
                st.markdown(f"### {i}. [{fmt}]")
                
                c_info, c_action = st.columns([3, 1])
                
                with c_info:
                    # [FIX] ä½œè€…é¡¯ç¤ºè™•ç†ï¼šå¦‚æœæ˜¯ listï¼Œæ ¹æ“šèªè¨€åˆä½µæˆå­—ä¸²
                    authors_data = ref.get('authors')
                    if isinstance(authors_data, list):
                        if ref.get('lang') == 'ZH':
                            author_display = "ã€".join(authors_data)
                        else:
                            author_display = ", ".join(authors_data)
                    else:
                        author_display = authors_data or "Unknown"

                    st.markdown(f"**ğŸ“ ä½œè€…**ï¼š{author_display}")
                    st.markdown(f"**ğŸ“„ æ¨™é¡Œ**ï¼š{title_display}")
                    st.markdown(f"**ğŸ“… å¹´ä»½**ï¼š{ref.get('year')}")
                    
                    if ref.get('source'):
                        st.markdown(f"**ğŸ“– ä¾†æº**ï¼š{ref.get('source')}")
                    
                    st.text_area(
                        label="åŸæ–‡",
                        value=ref['original'],
                        height=80,
                        key=f"apa_orig_{i}",
                        disabled=True
                    )
                
                with c_action:
                    st.markdown("**ğŸ› ï¸ æ ¼å¼è½‰æ›**")
                    if ref.get('lang') == 'EN':
                         if st.button("è½‰ IEEE", key=f"apa_btn_ieee_{i}"):
                             st.code(convert_en_apa_to_ieee(ref), language='text')
                    elif ref.get('lang') == 'ZH':
                         if 'APA' in fmt:
                             if st.button("è½‰ç·¨è™Ÿ", key=f"zh_btn_num_{i}"):
                                 st.code(convert_zh_apa_to_num(ref), language='text')
                         elif 'Numbered' in fmt:
                             if st.button("è½‰ APA", key=f"zh_btn_apa_{i}"):
                                 st.code(convert_zh_num_to_apa(ref), language='text')
                
                st.markdown("---")

        ## ==================== [èˆŠçš„é¡¯ç¤ºé‚è¼¯ - å·²è¨»è§£ä¿ç•™] ====================
        ## é€™æ˜¯åŸæœ¬ test_extraction.py çš„é¡¯ç¤ºæ–¹å¼ (ç°¡å–®åˆ—è¡¨)ï¼Œç¾å·²è¢«ä¸Šæ–¹ 1204 é¢¨æ ¼å–ä»£
        ##
        ## with st.expander("ğŸ” é»æ“Šå±•é–‹è©³ç´°æ¸…å–®èˆ‡æ ¼å¼è½‰æ›å·¥å…·"):
        ##     for i, ref in enumerate(parsed_refs, 1):
        ##         lang_tag = "ğŸ‡¹ğŸ‡¼ ä¸­æ–‡" if ref.get('lang') == 'ZH' else "ğŸ‡ºğŸ‡¸ è‹±æ–‡"
        ##         fmt = ref.get('format', 'Unknown')
        ##         title = ref.get('title') or "ç„¡æ¨™é¡Œ"
        ##         
        ##         st.markdown(f"**{i}. [{lang_tag}] {title}**")
        ##         
        ##         c_info, c_action = st.columns([3, 1])
        ##         with c_info:
        ##             st.caption(f"æ ¼å¼: {fmt} | å¹´ä»½: {ref.get('year')} | ä½œè€…: {ref.get('author')}")
        ##             #st.text(ref['original'])
        ##             st.text_area("åŸæ–‡", ref['original'], height=70, disabled=True, key=f"orig_text_{i}")
        ##         
        ##         with c_action:
        ##             # [Test1201] æ ¼å¼è½‰æ›æŒ‰éˆ•å€
        ##             if ref.get('lang') == 'EN':
        ##                 if 'APA' in fmt:
        ##                     if st.button("è½‰ IEEE", key=f"btn_ieee_{i}"):
        ##                         st.code(convert_en_apa_to_ieee(ref))
        ##                 elif 'IEEE' in fmt:
        ##                     if st.button("è½‰ APA", key=f"btn_apa_{i}"):
        ##                         st.code(convert_en_ieee_to_apa(ref))
        ##             elif ref.get('lang') == 'ZH':
        ##                 if 'APA' in fmt:
        ##                     if st.button("è½‰ç·¨è™Ÿ", key=f"btn_num_{i}"):
        ##                         st.code(convert_zh_apa_to_num(ref))
        ##                 elif 'Numbered' in fmt:
        ##                     if st.button("è½‰ APA", key=f"btn_zhapa_{i}"):
        ##                         st.code(convert_zh_num_to_apa(ref))
        ##         st.divider()
        ## ==================================================================== 
           
    else:
        st.warning("ç„¡åƒè€ƒæ–‡ç»æ®µè½å¯ä¾›åˆ†æ")

st.markdown("---")

st.header("ğŸš€ äº¤å‰æ¯”å°åˆ†æ")
st.info("ğŸ‘† è«‹ç¢ºèªä¸Šæ–¹è§£æçµæœç„¡èª¤å¾Œï¼Œé»æ“Šä¸‹æ–¹æŒ‰éˆ•é–‹å§‹æª¢æŸ¥ã€‚")

if st.button("é–‹å§‹äº¤å‰æ¯”å°", type="primary", use_container_width=True):
    if not st.session_state.in_text_citations or not st.session_state.reference_list:
        st.error("âŒ è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•æ¯”å°ã€‚è«‹ç¢ºèªæ˜¯å¦å·²æˆåŠŸè§£æå…§æ–‡å¼•ç”¨èˆ‡åƒè€ƒæ–‡ç»ã€‚")
    else:
        with st.spinner("æ­£åœ¨é€²è¡Œé›™å‘äº¤å‰æ¯”å°..."):
                # å‘¼å«æˆ‘å€‘å‰›å¯«å¥½çš„ check_references å‡½å¼
            missing, unused = check_references(
                st.session_state.in_text_citations,
                st.session_state.reference_list
                )
                
                # å°‡çµæœå­˜å…¥ session state ä»¥ä¾¿é¡¯ç¤º
            st.session_state.missing_refs = missing
            st.session_state.unused_refs = unused
                
            st.success("âœ… æ¯”å°å®Œæˆï¼")

    # ==========================================
    # ç¬¬ä¸‰éšæ®µï¼šé¡¯ç¤ºæ¯”å°çµæœå ±å‘Š
    # ==========================================
    
    # æª¢æŸ¥ session state ä¸­æ˜¯å¦æœ‰æ¯”å°çµæœï¼Œæœ‰çš„è©±æ‰é¡¯ç¤º
    if 'missing_refs' in st.session_state and 'unused_refs' in st.session_state:
        st.subheader("ğŸ“Š æ¯”å°çµæœå ±å‘Š")
        
        # ä½¿ç”¨ Tabs åˆ†é é¡¯ç¤ºå…©é¡éŒ¯èª¤
        tab1, tab2 = st.tabs([
            f"âŒ éºæ¼çš„åƒè€ƒæ–‡ç» ({len(st.session_state.missing_refs)})", 
            f"âš ï¸ æœªä½¿ç”¨çš„åƒè€ƒæ–‡ç» ({len(st.session_state.unused_refs)})"
        ])
        
        with tab1:
            st.caption("ğŸ’¡ èªªæ˜ï¼šé€™äº›å¼•ç”¨å‡ºç¾åœ¨å…§æ–‡ä¸­ï¼Œä½†åœ¨åƒè€ƒæ–‡ç»åˆ—è¡¨è£¡æ‰¾ä¸åˆ°å°æ‡‰é …ç›®ã€‚")
    
            if not st.session_state.missing_refs:
                st.success("å¤ªæ£’äº†ï¼æ‰€æœ‰å…§æ–‡å¼•ç”¨éƒ½åœ¨åƒè€ƒæ–‡ç»åˆ—è¡¨ä¸­æ‰¾åˆ°äº†ã€‚")
            else:
                for i, item in enumerate(st.session_state.missing_refs, 1):
            # æª¢æŸ¥æ˜¯å¦ç‚ºã€Œç–‘ä¼¼å¹´ä»½éŒ¯èª¤ã€
                    if item.get('error_type') == 'year_mismatch':
                        st.warning(
                    f"{i}. **{item['original']}** (æ ¼å¼: {item['format']})\n\n"
                    f"âš ï¸ **ç–‘ä¼¼å¹´ä»½å¼•ç”¨éŒ¯èª¤**ï¼šç³»çµ±åœ¨åƒè€ƒæ–‡ç»ä¸­æ‰¾åˆ°äº†åŒåä½œè€…ï¼Œ"
                    f"ä½†å¹´ä»½ä¼¼ä¹æ˜¯ **{item.get('year_hint', 'ä¸åŒå¹´ä»½')}**ï¼Œè€Œéå…§æ–‡å¯«çš„ **{item.get('year')}**ã€‚",
                    icon="ğŸ“…"
                )
            # å¦‚æœä¸æ˜¯å¹´ä»½éŒ¯èª¤ï¼Œå°±æ˜¯çœŸçš„æ‰¾ä¸åˆ° (Missing)
                    else:
                        st.error(f"{i}. **{item['original']}** (æ ¼å¼: {item['format']})", icon="ğŸš¨")


        with tab2:
            st.caption("ğŸ’¡ èªªæ˜ï¼šé€™äº›æ–‡ç»åˆ—åœ¨åƒè€ƒæ–‡ç»åˆ—è¡¨ä¸­ï¼Œä½†åœ¨å…§æ–‡ä¸­å¾æœªè¢«å¼•ç”¨éã€‚")
            if not st.session_state.unused_refs:
                st.success("å¤ªæ£’äº†ï¼æ‰€æœ‰åƒè€ƒæ–‡ç»éƒ½åœ¨å…§æ–‡ä¸­è¢«æœ‰æ•ˆå¼•ç”¨ã€‚")
            else:
                for i, item in enumerate(st.session_state.unused_refs, 1):
                    # ä½¿ç”¨é»ƒè‰²è­¦å‘Šï¼Œä¸¦é¡¯ç¤ºåŸå§‹æ–‡å­—
                    st.warning(f"{i}. **{item['original']}**", icon="ğŸ—‘ï¸")

# ==================== æŸ¥çœ‹æš«å­˜è³‡æ–™ ====================
if st.session_state.in_text_citations or st.session_state.reference_list:
    with st.expander("ğŸ” æŸ¥çœ‹å®Œæ•´æš«å­˜è³‡æ–™ï¼ˆJSON æ ¼å¼ï¼‰"):
        st.json({
            "in_text_citations": st.session_state.in_text_citations,
            "reference_list": st.session_state.reference_list,
            "verified_references": st.session_state.verified_references
        })