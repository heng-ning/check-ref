def format_pages_display(pages):
    """æ ¼å¼åŒ–é ç¢¼é¡¯ç¤ºï¼šå¦‚æœåŒ…å«å­—æ¯å°±ä¸åŠ  pp."""
    if not pages:
        return None
    if re.search(r'[A-Za-z]', pages):
        return pages  # S27â€“S31
    else:
        return f"pp. {pages}"  # pp. 123-456

# ===== è‹±æ–‡ APA =====
def parse_apa_authors_en(author_str):
    if not author_str: return []
    
    # å…ˆè™•ç† & æˆ– andï¼ˆAPA æœ€å¾Œä¸€å€‹ä½œè€…å‰çš„é€£æ¥è©ï¼‰
    # å°‡ & æˆ– and æ›¿æ›æˆé€—è™Ÿï¼Œçµ±ä¸€è™•ç†
    clean_str = re.sub(r'\s*,?\s*(&|and)\s+', ', ', author_str, flags=re.IGNORECASE)
    
    # ç”¨ã€Œ., ã€ï¼ˆé»è™Ÿ+é€—è™Ÿ+ç©ºæ ¼ï¼‰ä¾†åˆ†å‰²ä½œè€…
    # é€™æ¨£å¯ä»¥æ­£ç¢ºè™•ç† "Last, F. M., Next, A."
    segments = re.split(r'\.\s*,\s*', clean_str)
    
    authors = []
    for seg in segments:
        seg = seg.strip()
        if not seg: continue
        
        # ç§»é™¤çµå°¾çš„é»è™Ÿï¼ˆå¦‚æœæœ‰ï¼‰
        seg = seg.rstrip('.')
        
        if ',' in seg:
            # æ ¼å¼ï¼šLast, F. M.
            parts = seg.split(',', 1)
            last = parts[0].strip()
            first = parts[1].strip()
            # ç¢ºä¿ first name æœ‰é»è™Ÿçµå°¾
            if first and not first.endswith('.'):
                first += '.'
            authors.append({'last': last, 'first': first})
        else:
            # åªæœ‰å§“æ°
            authors.append({'last': seg, 'first': ''})
    
    return authors

def extract_apa_en_detailed(ref_text):
    result = {
        'format': 'APA (EN)', 'lang': 'EN',
        'authors': "Unknown", 'parsed_authors': [],
        'year': None, 'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None,
        'article_number': None,
        'publisher': None,
        'editors': None,
        'book_title': None,
        'source_type': None,
        'document_type': None,
        'url': None,
        'doi': None, 'original': ref_text
    }

    # å…ˆæå– DOI å’Œ URL (æå‰è™•ç†ï¼Œé¿å…å¹²æ“¾æ¨™é¡Œè§£æ)
    result['doi'] = extract_doi(ref_text)

    # æå– URL (æ”¯æ´å„ç¨®æ ¼å¼ï¼ŒåŒ…å«ç©ºæ ¼æ–·è¡Œå’Œé€£å­—è™Ÿæ–·è¡Œ)
    # æ‰¾åˆ° https:// é–‹é ­ï¼Œç„¶å¾Œå‘å¾ŒæŠ“å–ç›´åˆ°é‡åˆ°æ˜ç¢ºçš„çµæŸæ¨™è¨˜
    url_start = re.search(r'https?://', ref_text)
    if url_start:
        # å¾ https:// é–‹å§‹å‘å¾Œæƒæ
        start_pos = url_start.start()
        url_text = ref_text[start_pos:]
        
        # æ‰¾åˆ° URL çµæŸçš„ä½ç½®ï¼ˆé‡åˆ°å¥è™Ÿ+ç©ºæ ¼ã€é€—è™Ÿã€æˆ–æ–‡æœ«ï¼‰
        # é‡åˆ°ã€Œå¥è™Ÿ+æ›è¡Œ+å¤§å¯«å­—æ¯ã€ä¹Ÿè¦–ç‚ºçµæŸï¼ˆè™•ç† DOI æ–·è¡Œå•é¡Œï¼‰
        end_match = re.search(r'(?:\.\s*\n\s*[A-Z]|\.\s+[A-Z]|,\s|$)', url_text)
        if end_match:
            raw_url = url_text[:end_match.start()].strip()
        else:
            raw_url = url_text.strip()
        
        # æ¸…ç† URLï¼š
        # 1. å…ˆè™•ç†ã€Œé€£å­—è™Ÿ+ç©ºç™½ã€-> ä¿ç•™é€£å­—è™Ÿ
        clean_url = re.sub(r'-\s+', '-', raw_url)
        # 2. ç§»é™¤æ‰€æœ‰å‰©é¤˜ç©ºç™½
        clean_url = re.sub(r'\s+', '', clean_url)
        # 3. ç§»é™¤çµå°¾çš„å¥è™Ÿï¼ˆå¦‚æœæœ‰ï¼‰
        clean_url = clean_url.rstrip('.')
        
        result['url'] = clean_url

        # å¦‚æœ URL æ˜¯ DOI é€£çµï¼Œæ¸…ç©º URL æ¬„ä½
        if re.match(r'^https?://doi\.org/', clean_url, re.IGNORECASE):
            result['url'] = None

        # ä¿ç•™ url_match ä¾›å¾ŒçºŒä½¿ç”¨
        url_match = type('obj', (object,), {'group': lambda self, n: raw_url if n == 0 else None})()
    else:
        url_match = None
    
    year_match = re.search(r'[ï¼ˆ(]\s*(\d{4}[a-z]?|n\.d\.)\s*(?:,\s*[A-Za-z]+\.?\s*\d{0,2})?\s*[)ï¼‰]', ref_text)
    if not year_match: return result
    
    year_group = year_match.group(1)
    result['year'] = year_group if year_group.lower() != 'n.d.' else 'n.d.'

    # æå–å®Œæ•´æ—¥æœŸ (Month Day) - å…ˆæª¢æŸ¥ group æ˜¯å¦å­˜åœ¨
    try:
        date_match = year_match.group(2)
        if date_match:
            result['month'] = date_match
    except IndexError:
        pass  # æ²’æœ‰æœˆä»½è³‡è¨Šï¼Œè·³é
    
    author_part = ref_text[:year_match.start()].strip()
    result['authors'] = author_part
    result['parsed_authors'] = parse_apa_authors_en(author_part)
    
    content_part = ref_text[year_match.end():].strip()
    if content_part.startswith('.'): content_part = content_part[1:].strip()

    # ç§»é™¤ DOI å’Œ URLï¼Œé¿å…å®ƒå€‘è¢«èª¤åˆ¤ç‚ºæ¨™é¡Œæˆ–ä¾†æº
    if result['doi']:
        content_part = re.sub(r'(?:doi:|DOI:|https?://doi\.org/)\s*10\.\d{4,}/[^\sã€‚]+', '', content_part).strip()

    if result['url']:
        # ç§»é™¤åŸå§‹ URLï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„ç©ºæ ¼è®Šé«”ï¼‰
        if url_match:
            # å°‡åŸå§‹ URL ä¸­çš„ç©ºæ ¼è®Šæˆå½ˆæ€§åŒ¹é…æ¨¡å¼
            original_url_text = url_match.group(0)
            # å°‡ URL æ‹†æˆç‰‡æ®µï¼Œç”¨ \s* é€£æ¥ï¼ˆå…è¨±ä»»æ„ç©ºæ ¼ï¼‰
            url_parts = original_url_text.split()
            flexible_pattern = r'\s*'.join(re.escape(part) for part in url_parts)
            content_part = re.sub(flexible_pattern, '', content_part, flags=re.IGNORECASE)
        
        # ä¹Ÿç§»é™¤æ¸…ç†å¾Œçš„ URLï¼ˆä»¥é˜²è¬ä¸€ï¼‰
        content_part = content_part.replace(result['url'], '')
        
        # æ¸…ç†æ®˜ç•™çš„å¤šé¤˜ç©ºæ ¼å’Œæ¨™é»
        content_part = re.sub(r'\s+', ' ', content_part).strip()
        content_part = content_part.rstrip('. ')

    # åˆ¤æ–·æ˜¯å¦ç‚ºæ›¸ç±ç« ç¯€æˆ–ä¸€èˆ¬æ›¸ç±
    # å„ªå…ˆæª¢æŸ¥æ˜¯å¦ç‚ºæ›¸ç±ç« ç¯€æ ¼å¼ï¼ˆIn ... (Eds.)ï¼‰
    is_book_chapter = bool(re.search(r'\bIn\s+.+?\s*\(Eds?\.\)', content_part, re.IGNORECASE))
    # æˆ–æ˜¯ä½œè€…ç‚ºç·¨è€…ï¼Œæˆ–æ¨™é¡ŒåŒ…å«æ›¸ç±é—œéµå­—
    is_book = is_book_chapter or bool(
        re.search(r'\(eds?\.\)', author_part, re.IGNORECASE) or 
        re.search(r'\b(manual|handbook|guide|textbook|encyclopedia|dictionary)\b', content_part, re.IGNORECASE)
    )

    # æå–å¾Œè¨­è³‡æ–™ (å·æœŸé ç¢¼/æ–‡ç« ç·¨è™Ÿ)
    # æ ¼å¼ 1: Journal, Vol(Issue), pages. ä¾‹å¦‚ï¼šJournal, 14(2), 123-456.
    # æ ¼å¼ 2: Journal, Vol(Issue), article_number. ä¾‹å¦‚ï¼šJournal, 13(11), 6474.
    # æ ¼å¼ 3: Journal, Vol. ä¾‹å¦‚ï¼šJournal, 160.
    meta_match = re.search(
        r',\s*(\d+)(?:\s*\((\d+)\))?(?:,\s*([A-Za-z]?\d+(?:[\â€“\-][A-Za-z]?\d+)?))?(?:\.|\s|$)', 
        content_part
    )

    if meta_match:
        result['volume'] = meta_match.group(1)
        result['issue'] = meta_match.group(2) if meta_match.group(2) else None
        pages_or_article = meta_match.group(3)
        
        # åˆ¤æ–·æ˜¯é ç¢¼é‚„æ˜¯æ–‡ç« ç·¨è™Ÿ
        if pages_or_article and pages_or_article.strip():
            # å¦‚æœåŒ…å«é€£å­—è™Ÿï¼ˆ- æˆ– â€“ï¼‰ï¼Œä¸€å®šæ˜¯é ç¢¼
            if '-' in pages_or_article or 'â€“' in pages_or_article:
                result['pages'] = pages_or_article
            else:
                # ç´”æ•¸å­—æˆ–å¸¶å­—æ¯å‰ç¶´çš„æ•¸å­—ï¼Œåˆ¤æ–·æ˜¯æ–‡ç« ç·¨è™Ÿé‚„æ˜¯é ç¢¼
                # é‚è¼¯ï¼š4 ä½æ•¸ä»¥ä¸Šé€šå¸¸æ˜¯æ–‡ç« ç·¨è™Ÿï¼ˆå¦‚ 6474ï¼‰ï¼Œ3 ä½æ•¸ä»¥ä¸‹å¯èƒ½æ˜¯é ç¢¼
                if pages_or_article.isdigit():
                    if len(pages_or_article) >= 4:  # 4 ä½æ•¸ä»¥ä¸Š â†’ æ–‡ç« ç·¨è™Ÿ
                        result['article_number'] = pages_or_article
                    else:  # 3 ä½æ•¸ä»¥ä¸‹ â†’ å¯èƒ½æ˜¯å–®é é ç¢¼
                        result['pages'] = pages_or_article
                else:
                    # å¸¶å­—æ¯çš„ï¼ˆå¦‚ S27ï¼‰â†’ å¯èƒ½æ˜¯é ç¢¼æˆ–æ–‡ç« ç·¨è™Ÿ
                    # ç°¡å–®åˆ¤æ–·ï¼šå¸¶å­—æ¯çš„çŸ­æ•¸å­—è¦–ç‚ºé ç¢¼
                    if len(pages_or_article) <= 4:
                        result['pages'] = pages_or_article
                    else:
                        result['article_number'] = pages_or_article
        
        title_source_part = content_part[:meta_match.start()].strip()
    else:
        # æ ¼å¼ 2: å‚³çµ±é ç¢¼æ ¼å¼ pp. 123-456 æˆ– pp. S27â€“S31
        pp_match = re.search(r',?\s*pp?\.?\s*([A-Za-z]?\d+[\â€“\-][A-Za-z]?\d+)(?:\.)?$', content_part)
        if pp_match:
            result['pages'] = pp_match.group(1)
            title_source_part = content_part[:pp_match.start()].strip()
        else:
            title_source_part = content_part

    # æ”¹é€²æ¨™é¡Œèˆ‡ä¾†æºåˆ†å‰²é‚è¼¯
    if is_book:
        # === å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºæ›¸ç±ç« ç¯€æ ¼å¼ ===
        # æ ¼å¼ï¼šç« ç¯€æ¨™é¡Œ. In ç·¨è€… (Eds.), æ›¸å (pp. xxx). å‡ºç‰ˆç¤¾.
        # æ”¹é€²æ­£å‰‡è¡¨é”å¼ï¼Œæ›´ç²¾ç¢ºåŒ¹é…
        chapter_match = re.search(
            r'^(.+?)\.\s+In\s+(.+?)\s*\(Eds?\.\),\s*(.+?)\s*\(pp\.\s*([\d\s\â€“\-â€”]+)\)', 
            title_source_part, 
            re.IGNORECASE
        )

        if chapter_match:
            # é€™æ˜¯æ›¸ç±ç« ç¯€
            result['title'] = chapter_match.group(1).strip()  # ç« ç¯€æ¨™é¡Œ
            result['editors'] = "In " + chapter_match.group(2).strip() + " (Eds.)"  # ç·¨è€…
            result['book_title'] = chapter_match.group(3).strip()  # æ›¸å
            
            # æ¸…ç†é ç¢¼ä¸­çš„å¤šé¤˜ç©ºæ ¼
            raw_pages = chapter_match.group(4).strip()
            clean_pages = re.sub(r'\s+', '', raw_pages)  # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
            result['pages'] = clean_pages  # ä¾‹å¦‚ "254â€“257"
            
            # å‡ºç‰ˆç¤¾åœ¨æ‹¬è™Ÿå¾Œé¢
            after_chapter = title_source_part[chapter_match.end():].strip()
            # ç§»é™¤é–‹é ­çš„å¥é»å’Œç©ºæ ¼
            after_chapter = after_chapter.lstrip('. ').strip()
            if after_chapter:
                # ç§»é™¤çµå°¾çš„å¥é»
                result['publisher'] = after_chapter.rstrip('.')
            
            result['source_type'] = 'Book Chapter'
        else:
            # ä¸€èˆ¬æ›¸ç±æ ¼å¼ï¼šæ¨™é¡Œ. å‡ºç‰ˆç¤¾.
            split_match = re.search(r'\.\s+([A-Z])', title_source_part)
            
            if split_match:
                split_pos = split_match.start()
                result['title'] = title_source_part[:split_pos].strip()
                
                # å‡ºç‰ˆç¤¾éƒ¨åˆ†
                publisher_part = title_source_part[split_pos + 1:].strip()
                next_dot = publisher_part.find('.')
                if next_dot != -1:
                    result['publisher'] = publisher_part[:next_dot].strip()
                else:
                    result['publisher'] = publisher_part.rstrip('.')
            else:
                result['title'] = title_source_part.rstrip('.')
    else:
        # æœŸåˆŠæ ¼å¼ï¼šæ¨™é¡Œ. æœŸåˆŠå
        # å…ˆè­˜åˆ¥ä¸¦ç§»é™¤æ–‡ç»é¡å‹æ¨™è¨» (å¦‚ [Project Report], Technical Report ç­‰)
        document_type_pattern = r'\.\s*(\[?(?:Project|Technical|Research|Working|Conference|Discussion)\s+(?:Report|Paper|Brief)\]?)\.'
        doc_type_match = re.search(document_type_pattern, title_source_part, re.IGNORECASE)
        
        if doc_type_match:
            # æå–æ–‡ç»é¡å‹
            result['document_type'] = doc_type_match.group(1).strip('[]')
            
            # å¾ title_source_part ä¸­ç§»é™¤æ–‡ç»é¡å‹
            title_source_part = (
                title_source_part[:doc_type_match.start()] + 
                '. ' + 
                title_source_part[doc_type_match.end():]
            ).strip()
        
        # åŸæœ¬çš„æ¨™é¡Œèˆ‡ä¾†æºåˆ†å‰²é‚è¼¯
        split_index = title_source_part.rfind('. ')
        if split_index != -1:
            result['title'] = title_source_part[:split_index].strip()
            result['source'] = title_source_part[split_index + 1:].strip().rstrip('.')
        else:
            if not title_source_part.startswith('http'):
                result['title'] = title_source_part.rstrip('.')

    # æ¸…ç†æ‰€æœ‰æ–‡å­—æ¬„ä½ä¸­çš„æ–·è¡Œé€£å­—è™Ÿ
    text_fields = ['title', 'source', 'publisher', 'editors', 'book_title', 'journal_name', 'conference_name']
    for field in text_fields:
        if result.get(field) and isinstance(result[field], str):
            # ç§»é™¤å–®å­—ä¸­çš„æ–·è¡Œé€£å­—è™Ÿï¼ˆå¦‚ "perform- ance" -> "performance"ï¼‰
            # æ¨¡å¼1: é€£å­—è™Ÿ+ç©ºæ ¼+å°å¯«å­—æ¯
            result[field] = re.sub(r'-\s+([a-z])', r'\1', result[field])
            # æ¨¡å¼2: å–®ç´”çš„é€£å­—è™Ÿ+ç©ºæ ¼ï¼ˆå‚™ç”¨ï¼‰
            result[field] = re.sub(r'-\s+', '', result[field])

    return result

# ===== ä¸­æ–‡ APA =====
def parse_chinese_authors(author_str):
    if not author_str: return []
    clean_str = re.sub(r'\s*(ç­‰|è‘—|ç·¨)$', '', author_str)
    return re.split(r'[ã€ï¼Œ,]', clean_str)

def extract_apa_zh_detailed(ref_text):
    result = {
        'format': 'APA (ZH)', 'lang': 'ZH',
        'authors': [], 'year': None, 'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)
    year_match = re.search(r'[ï¼ˆ(]\s*(\d{2,4})\s*[)ï¼‰]', ref_text)
    if not year_match: return result
    
    result['year'] = year_match.group(1)
    author_part = ref_text[:year_match.start()].strip()
    result['authors'] = parse_chinese_authors(author_part)
    
    rest = ref_text[year_match.end():].strip().lstrip('.ã€‚ ')
    match_book = re.search(r'ã€Š([^ã€‹]+)ã€‹', rest)
    match_article = re.search(r'ã€ˆ([^ã€‰]+)ã€‰', rest)
    
    if match_article:
        result['title'] = match_article.group(1)
        if match_book: result['source'] = match_book.group(1)
    elif match_book:
        pre_book = rest[:match_book.start()].strip()
        if pre_book:
            result['title'] = pre_book.rstrip('ã€‚. ')
            result['source'] = match_book.group(1)
        else:
            result['title'] = match_book.group(1)
    else:
        # [UPDATED] å¢åŠ å¾Œå‚™æ–¹æ¡ˆï¼Œå¦‚æœæ²’æœ‰æ›¸åè™Ÿï¼Œå˜—è©¦ç”¨å¥è™Ÿåˆ†éš”æŠ“å–ä¾†æº
        parts = re.split(r'[ã€‚.]', rest)
        # éæ¿¾ç©ºå­—ä¸²
        parts = [p.strip() for p in parts if p.strip()]
        if len(parts) > 0: result['title'] = parts[0]
        if len(parts) > 1: result['source'] = parts[1] # å˜—è©¦æŠ“å–ä¾†æº
            
    vol_match = re.search(r'(\d+)\s*[å·]', rest)
    if vol_match: result['volume'] = vol_match.group(1)
    return result

def extract_numbered_zh_detailed(ref_text):
    result = {
        'format': 'Numbered (ZH)', 'lang': 'ZH',
        'ref_number': None, 'authors': [], 'year': None, 'title': None, 'source': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)
    num_match = re.match(r'^\s*[\[ã€]\s*(\d+)\s*[\]ã€‘]', ref_text)
    if num_match:
        result['ref_number'] = num_match.group(1)
        rest = ref_text[num_match.end():].strip()
    else:
        rest = ref_text
    year_match = re.search(r'\b(\d{4})\b', rest)
    if year_match: result['year'] = year_match.group(1)
    
    match_book = re.search(r'ã€Š([^ã€‹]+)ã€‹', rest)
    if match_book:
        result['source'] = match_book.group(1)
        pre = rest[:match_book.start()]
        # å˜—è©¦æŠ“ä½œè€…å’Œç¯‡å
        parts = re.split(r'[ï¼Œ,]', pre)
        if len(parts) > 0: result['authors'] = parse_chinese_authors(parts[0])
        if len(parts) > 1: result['title'] = parts[1]
    else:
        # [UPDATED] å¢åŠ å¾Œå‚™æ–¹æ¡ˆï¼Œå˜—è©¦æŠ“å–ä¾†æº (å‡è¨­çµæ§‹: ä½œè€…, ç¯‡å, ä¾†æº)
        parts = re.split(r'[ï¼Œ,ã€‚.]', rest)
        parts = [p.strip() for p in parts if p.strip()]
        if len(parts) > 0: result['authors'] = parse_chinese_authors(parts[0])
        if len(parts) > 1: result['title'] = parts[1]
        if len(parts) > 2: result['source'] = parts[2] # å˜—è©¦æŠ“å–ä¾†æº

    return result

# ===== APA æ–·è¡Œåˆä½µï¼ˆæ··åˆæ¨¡å¼ï¼‰=====
def find_apa_head(ref_text):
    """[NEW] åµæ¸¬ APA æ ¼å¼é–‹é ­ (å¹´ä»½) - å–ä»£èˆŠçš„ find_apa"""
    # è‹±æ–‡ APA: Author (2020).
    # ä¸­æ–‡ APA: ä½œè€… (2020)ã€‚
    match = re.search(r'[ï¼ˆ(]\s*(\d{4}(?:[a-z])?|n\.d\.)\s*(?:,\s*([A-Za-z]+\.?\s*\d{0,2}))?\s*[)ï¼‰]', ref_text)
    if not match: return False
    
    # ç¢ºä¿å¹´ä»½æ‹¬è™Ÿå‡ºç¾åœ¨å‰é¢ (ä¾‹å¦‚å‰ 50 å€‹å­—å…§ï¼Œé¿å…èª¤åˆ¤æ–‡ä¸­çš„å¹´ä»½)
    if match.start() > 80: return False 
    
    return True

def is_reference_head_unified(para):
    """
    [UPDATED] [APA/æ··åˆæ¨¡å¼] åˆ¤æ–·ä¸€è¡Œæ–‡å­—æ˜¯å¦ç‚ºæ–°æ–‡ç»
    """
    para = normalize_text(para)

    # DOI ç‰¹å¾µï¼šæ•¸å­—é–‹é ­ + æ–œç·š + å­—æ¯æ•¸å­—æ··åˆ
    if re.match(r'^\d{4,}/[a-z0-9\.\-/]+', para, re.IGNORECASE):
        return False
    if re.match(r'^[a-z0-9]+\-[a-z]{2}$', para, re.IGNORECASE):
        return False
    
    # 0. âœ… å¼·ç‰¹å¾µç™½åå–®ï¼šæ˜ç¢ºçš„æ–°æ–‡ç»é–‹é ­ï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
    
    # A. ç·¨è™Ÿæ ¼å¼ [1]
    if re.match(r'^\s*[\[ã€]\s*\d+\s*[ã€‘\]]', para):
        return True
    
    # B. æ¨™æº– APA ä½œè€…æ ¼å¼ï¼šLast, F. é–‹é ­
    # åªè¦é–‹é ­æ˜¯ "å§“, åç¸®å¯«"ï¼Œä¸”ä¸æ˜¯å°å¯«æˆ–æ•¸å­—é–‹é ­ï¼Œå°±å¾ˆå¯èƒ½æ˜¯æ–°æ–‡ç»
    # ä¸ç®¡å¹´ä»½åœ¨å“ªï¼ˆå¯èƒ½è¢«æ–·è¡Œåˆ°ä¸‹ä¸€æ®µï¼‰
    author_start = re.match(r'^([A-Z][A-Za-z\-\']+),\s+([A-Z]\.(?:\s*[A-Z]\.)*)', para)
    
    # C. çµ„ç¹”ä½œè€…æ ¼å¼ï¼šé–‹é ­å¤§å¯«å–®å­— + (ç¸®å¯«) + å¹´ä»½
    # ä¾‹å¦‚ï¼šWorld Health Organization (WHO). (2020)
    org_author_match = re.match(
        r'^[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+)*\s*\([A-Z]+\)\.\s*\((\d{4})', 
        para
    )
    if org_author_match:
        year = org_author_match.group(1)
        if is_valid_year(year):
            return True

    # D. ä¸€èˆ¬çµ„ç¹”ä½œè€…ï¼ˆæ²’æœ‰ç¸®å¯«ï¼‰ï¼šé–‹é ­å¤šå€‹å¤§å¯«å–®å­— + å¹´ä»½
    # ä¾‹å¦‚ï¼šNational Research Council. (2019)
    org_simple_match = re.match(
        r'^[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){2,}\.\s*\((\d{4})', 
        para
    )
    if org_simple_match:
        year = org_simple_match.group(1)
        if is_valid_year(year):
            return True

    if author_start:
        # é€²ä¸€æ­¥é©—è­‰ï¼šæ’é™¤æ˜é¡¯ä¸æ˜¯ä½œè€…çš„æƒ…æ³
        # 1. å¾Œé¢ä¸èƒ½ç›´æ¥æ¥å°å¯«å­—æ¯ï¼ˆè¡¨ç¤ºæ˜¯å¥å­ä¸­é–“ï¼‰
        after_author = para[author_start.end():].strip()
        if after_author and after_author[0].islower():
            pass  # å¯èƒ½æ˜¯å¥å­ï¼Œä¸è™•ç†
        else:
            # 2. æª¢æŸ¥æ˜¯å¦æœ‰åˆç†çš„å¾ŒçºŒå…§å®¹ï¼ˆé€—è™Ÿã€&ã€orã€å¹´ä»½æ‹¬è™Ÿï¼‰
            if re.match(r'^[,&\(]', after_author) or not after_author:
                return True
            # 3. å¦‚æœå¾Œé¢é‚„æœ‰å…¶ä»–ä½œè€…åï¼ˆèªªæ˜æ˜¯ä½œè€…åˆ—è¡¨é–‹é ­ï¼‰
            if re.search(r'[,&]\s+[A-Z][a-z]+,\s+[A-Z]\.', after_author[:50]):
                return True
            # 4. å¦‚æœæ˜¯ DOI/URL çµå°¾å¾Œçš„æ–°ä½œè€…
            # æª¢æŸ¥ï¼šä½œè€…æ ¼å¼å®Œæ•´ + å¾Œé¢æœ‰å¹´ä»½ â†’ é€™æ˜¯æ–°æ–‡ç»
            if re.search(r'\(\d{4}\)', para):
                return True
    
    # 1. ğŸš« é»‘åå–®ï¼šçµ•å°ä¸æ˜¯æ–°æ–‡ç»çš„æƒ…æ³
    
    # A. ç¶²å€ä¿è­·
    if re.search(r'(https?://|doi\.org|doi:|www\.)', para, re.IGNORECASE):
        url_only = re.sub(r'https?://[^\s]+', '', para).strip()
        if len(url_only) < 10:
            return False
        if not (re.match(r'^\s*[\[ã€]', para) or author_start):
            return False
            
    # B. æœˆä»½/æ—¥æœŸä¿è­·
    if re.match(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*\d{4}', para, re.IGNORECASE):
        return False
        
    # C. å·æœŸé ç¢¼ä¿è­·
    if re.match(r'^(Vol\.|No\.|pp\.|p\.|Page)', para, re.IGNORECASE):
        return False
        
    # D. å°å¯«é–‹é ­ä¿è­·
    if re.match(r'^[a-z]', para):
        return False
    
    # E. ä½œè€…åˆ—è¡¨å»¶çºŒä¿è­·ï¼ˆåªæœ‰ & æˆ–é€—è™Ÿ+åå­—ï¼Œæ²’æœ‰å§“æ°é–‹é ­ï¼‰
    # ä¾‹å¦‚ï¼š", & Varatharajan, S." é€™ç¨®ä¸ç®—æ–°æ–‡ç»é–‹é ­
    if re.match(r'^[,&]\s', para):
        return False

    # å¦‚æœé–‹é ­æ˜¯ç¸®å¯«ï¼ˆå¦‚ "A., Malhotra"ï¼‰ï¼Œä½†å¾Œé¢æ²’æœ‰å¹´ä»½æ‹¬è™Ÿ (20XX)
    # é€™æ˜¯ä½œè€…åˆ—è¡¨å»¶çºŒï¼Œä¸æ˜¯æ–°æ–‡ç»é–‹é ­
    # ä¾‹å¦‚ï¼š"A., Malhotra, R. K., & Martin, J. L." (æ²’æœ‰å¹´ä»½)
    if re.match(r'^[A-Z]\.(?:\s*[A-Z]\.)*\s*,', para):
        # æª¢æŸ¥é€™ä¸€æ®µæ˜¯å¦æœ‰å¹´ä»½æ‹¬è™Ÿ (19XX) æˆ– (20XX)
        # å¦‚æœæ²’æœ‰å¹´ä»½ï¼Œé€™è‚¯å®šæ˜¯ä½œè€…åˆ—è¡¨å»¶çºŒ
        if not re.search(r'[ï¼ˆ(]\s*(?:19|20)\d{2}', para):
            return False

    # 2. âœ… å…¶ä»–ç™½åå–®ç‰¹å¾µ
    
    # C. APA æ¨™æº–æ ¼å¼ (Year) - å¹´ä»½åœ¨æ‹¬è™Ÿå…§
    if find_apa_head(para):
        return True
        
    # D. é¡ APA (Year in dots)
    year_match = re.search(r'[\.,]\s*(19|20)\d{2}[a-z]?[\.,]', para[:80])
    if year_match:
        pre_text = para[:year_match.start()].strip()
        if len(pre_text) > 3:
            if not has_chinese(para):
                if ',' in pre_text or '.' in pre_text:
                    return True
            else:
                return True

    return False

def merge_references_unified(paragraphs):
    """[UPDATED from test1204-6] [APA/æ··åˆæ¨¡å¼] åˆä½µæ–·è¡Œ"""
    merged = []
    current_ref = ""
    
    for i, para in enumerate(paragraphs):
        para = para.strip()
        if not para: continue
        
        # æ’é™¤ç´”æ•¸å­—é ç¢¼ (é•·åº¦çŸ­ä¸”ç„¡é€£å­—è™Ÿ)
        if para.isdigit() and len(para) < 4: continue

        # æ’é™¤é é¦–/é å°¾æ–‡å­—
        # ç‰¹å¾µï¼šå…¨å¤§å¯«ã€é•·åº¦çŸ­ã€æ²’æœ‰å¹´ä»½æ‹¬è™Ÿã€æ²’æœ‰ç·¨è™Ÿ
        if para.isupper() and len(para) < 50:
            # 1. åŒ…å« ET AL çš„ä½œè€…é é¦–
            if 'ET AL' in para:
                continue
            # 2. ç¸®å¯«é–‹é ­çš„é é¦–ï¼ˆå¦‚ "S. JAYDARIFARD ET AL."ï¼‰
            if re.match(r'^[A-Z]\.\s+[A-Z]+', para):
                continue
            # 3. æœŸåˆŠåç¨±æˆ–ç« ç¯€æ¨™é¡Œçš„é é¦–ï¼ˆå¦‚ "TRANSPORT REVIEWS"ï¼‰
            # æ’é™¤æ¢ä»¶ï¼šå…¨å¤§å¯« + æ²’æœ‰æ•¸å­— + æ²’æœ‰æ‹¬è™Ÿ + æ²’æœ‰æ¨™é»ï¼ˆé™¤äº†ç©ºæ ¼ï¼‰
            if not re.search(r'[\d\(\)\[\]\.,:;]', para):
                continue  # è·³éé€™è¡Œ
        
        is_new_ref = is_reference_head_unified(para)

        # ç‰¹æ®Šåˆ¤æ–·ï¼šå¦‚æœç•¶å‰æ–‡ç»ä»¥ & æˆ– , çµå°¾ï¼ˆè¡¨ç¤ºä½œè€…åˆ—è¡¨æœªå®Œæˆï¼‰
        # ä¸”é€™è¡Œé–‹é ­æ˜¯ä½œè€…å+å¹´ä»½ï¼Œé€™è¡Œæ‡‰è©²æ˜¯ä½œè€…åˆ—è¡¨çš„æœ€å¾Œä¸€ä½ï¼Œä¸æ˜¯æ–°æ–‡ç»
        if is_new_ref and current_ref:
            # æª¢æŸ¥ä¸Šä¸€è¡Œçµå°¾
            current_ref_stripped = current_ref.rstrip()
            if current_ref_stripped.endswith('&') or current_ref_stripped.endswith(','):
                # æª¢æŸ¥é€™è¡Œæ˜¯å¦ç‚ºï¼šä½œè€…å + å¹´ä»½ï¼ˆä½œè€…åˆ—è¡¨æœ€å¾Œä¸€ä½çš„æ¨¡å¼ï¼‰
                # ä¾‹å¦‚ï¼šVaratharajan, S. (2019). ...
                if re.match(r'^[A-Z][A-Za-z\-\']+,\s+[A-Z]\.\s*[ï¼ˆ(]', para):
                    # é€™æ˜¯ä½œè€…åˆ—è¡¨çš„æœ€å¾Œä¸€ä½ï¼Œæ‡‰è©²åˆä½µ
                    is_new_ref = False

        # å¦‚æœç•¶å‰ç´¯ç©çš„æ–‡ç»æ²’æœ‰å¹´ä»½ï¼Œä¸”æ–°æ®µè½æœ‰å¹´ä»½
        # é‚£æ–°æ®µè½æ‡‰è©²æ˜¯ç•¶å‰æ–‡ç»çš„å»¶çºŒï¼Œä¸æ˜¯æ–°æ–‡ç»
        if is_new_ref and current_ref:
            # æª¢æŸ¥ current_ref æ˜¯å¦æœ‰å¹´ä»½
            has_year_in_current = bool(re.search(r'[ï¼ˆ(]\s*(?:19|20)\d{2}', current_ref))
            # æª¢æŸ¥ para æ˜¯å¦æœ‰å¹´ä»½
            has_year_in_para = bool(re.search(r'[ï¼ˆ(]\s*(?:19|20)\d{2}', para))
            
            # å¦‚æœç•¶å‰æ–‡ç»æ²’å¹´ä»½ï¼Œä½†æ–°æ®µè½æœ‰å¹´ä»½ â†’ æ–°æ®µè½æ˜¯å»¶çºŒ
            if not has_year_in_current and has_year_in_para:
                is_new_ref = False
        
        # å¦‚æœç•¶å‰ç´¯ç©çš„æ–‡ç»ä»¥ DOI æˆ–å®Œæ•´ URL çµå°¾ä¸”æ–°æ®µè½æ˜¯æ˜ç¢ºçš„ä½œè€…é–‹é ­ï¼Œå¼·åˆ¶åˆ‡åˆ†
        if current_ref and not is_new_ref:
            current_stripped = current_ref.rstrip()
            # æª¢æŸ¥æ˜¯å¦ä»¥ DOI æˆ– URL çµå°¾
            ends_with_doi_url = bool(
                re.search(r'(https?://[^\s]+|doi\.org/[^\s]+|10\.\d{4}/[^\s]+)[.\s]*$', current_stripped)
            )
            
            # æª¢æŸ¥æ–°æ®µè½æ˜¯å¦ç‚ºæ˜ç¢ºçš„ä½œè€…é–‹é ­
            clear_author_start = bool(
                re.match(r'^([A-Z][A-Za-z\-\']+),\s+([A-Z]\.(?:\s*[A-Z]\.)*)', para) and
                re.search(r'\(\d{4}\)', para)
            )
            
            # å¦‚æœå…©å€‹æ¢ä»¶éƒ½æ»¿è¶³ï¼Œå¼·åˆ¶åˆ‡åˆ†
            if ends_with_doi_url and clear_author_start:
                is_new_ref = True

        if is_new_ref:
            if current_ref:
                merged.append(current_ref)
            current_ref = para
        else:
            if current_ref:
                if has_chinese(current_ref[-1:]) and has_chinese(para[:1]):
                    current_ref += "" + para
                elif current_ref.endswith('-'):
                    # åˆ¤æ–·æ˜¯å¦ç‚ºå–®å­—æ–·è¡Œ
                    if para and para[0].islower():
                        current_ref = current_ref[:-1] + para
                    else:
                        current_ref = current_ref + " " + para
                # è™•ç†é ç¢¼æ–·è¡Œï¼šé€£å­—è™Ÿ+ç©ºæ ¼+æ•¸å­—
                elif re.search(r'[\â€“\-â€”]\s*$', current_ref) and para and para[0].isdigit():
                    current_ref = current_ref.rstrip() + para
                # è™•ç† DOI æ–·è¡Œ
                elif re.search(r'doi\.org/[^\s]+\.$', current_ref, re.IGNORECASE) and para and para[0].isdigit():
                    current_ref = current_ref + para  # DOI ç›´æ¥é€£æ¥
                # è™•ç†ä¸€èˆ¬ URL çµå°¾æ˜¯å¥é»çš„æ–·è¡Œ
                elif re.search(r'https?://[^\s]+\.$', current_ref) and para and not para[0].isupper():
                    current_ref = current_ref + para
                else:
                    current_ref += " " + para
            else:
                current_ref = para
            
    if current_ref: 
        merged.append(current_ref)
    
    return merged

# ===== APA æ ¼å¼è½‰æ› =====
def convert_en_apa_to_ieee(data):
    ieee_authors = []
    for auth in data.get('parsed_authors', []):
        ieee_authors.append(f"{auth['first']} {auth['last']}".strip())
    auth_str = ", ".join(ieee_authors)
    if len(ieee_authors) > 2: auth_str = re.sub(r', ([^,]+)$', r', and \1', auth_str)
    
    parts = []
    if auth_str: parts.append(auth_str + ",")
    if data.get('title'): parts.append(f'"{data["title"]},"')
    
    # åˆ†åˆ¥è™•ç†æœŸåˆŠå’Œæ›¸ç±
    if data.get('source'):  # æœŸåˆŠ
        parts.append(f"{data['source']},")
    elif data.get('publisher'):  # æ›¸ç±
        parts.append(f"{data['publisher']},")
    
    if data.get('volume'): parts.append(f"vol. {data['volume']},")
    if data.get('issue'): parts.append(f"no. {data['issue']},")
    if data.get('pages'): parts.append(f"pp. {data['pages']},")
    
    # åŠ å…¥æœˆä»½
    if data.get('month'): parts.append(f"{data['month']}")
    if data.get('year'): parts.append(f"{data['year']}.")
    
    # åŠ å…¥ DOI æˆ– URL
    if data.get('doi'): parts.append(f"doi: {data['doi']}.")
    elif data.get('url'): parts.append(f"[Online]. Available: {data['url']}")
    
    return " ".join(parts)

def convert_zh_apa_to_num(data):
    parts = []
    # [UPDATED] ä¿®æ­£ä½œè€…é€£æ¥ç¬¦è™Ÿï¼Œlist è½‰å­—ä¸²
    if isinstance(data.get('authors'), list):
        auth = "ã€".join(data.get('authors'))
    else:
        auth = data.get('authors', '')
        
    if auth: parts.append(auth)
    if data.get('title'): parts.append(f"ã€Œ{data['title']}ã€")
    # [UPDATED] ç¢ºä¿å‡ºè™•æœ‰è¢«æŠ“åˆ°æ‰é¡¯ç¤º
    if data.get('source'): parts.append(f"ã€Š{data['source']}ã€‹")
    if data.get('year'): parts.append(data['year'])
    return "ï¼Œ".join(parts) + "ã€‚"

def convert_zh_num_to_apa(data):
    # [UPDATED] ä¿®æ­£ä½œè€…é€£æ¥ç¬¦è™Ÿ
    if isinstance(data.get('authors'), list):
        auth = "ã€".join(data.get('authors'))
    else:
        auth = data.get('authors', '')
        
    parts = []
    parts.append(f"{auth}ï¼ˆ{data.get('year', 'ç„¡å¹´ä»½')}ï¼‰")
    if data.get('title'): parts.append(data['title'])
    if data.get('source'): parts.append(f"ã€Š{data['source']}ã€‹")
    return "ã€‚".join(parts) + "ã€‚"

# ===== æ ¸å¿ƒæ•´åˆ =====
def process_single_reference(ref_text):
    """
    [Updated] æ ¸å¿ƒåˆ†æµé‚è¼¯
    ç­–ç•¥ï¼šå„ªå…ˆåˆ¤æ–·æ˜¯å¦ç‚ºç·¨è™Ÿæ ¼å¼ ([1]...), è‹¥æ˜¯å‰‡ä¸€å¾‹äº¤çµ¦ IEEE æ¨¡çµ„ (ä¸­è‹±æ–‡é€šç”¨)ã€‚
    """
    ref_text = normalize_text(ref_text)
    
    # 1. å„ªå…ˆåˆ¤æ–·ï¼šæ˜¯å¦ç‚ºç·¨è™Ÿæ ¼å¼ ([1], [2], ã€1ã€‘...)
    # åªè¦æ˜¯ç·¨è™Ÿé–‹é ­ï¼Œä¸ç®¡ä¸­è‹±æ–‡ï¼Œå…¨éƒ¨è¦–ç‚º IEEE æ ¼å¼
    if re.match(r'^\s*[\[ã€]', ref_text):
        # å‘¼å«æ‚¨å‰›å‰›æ›´æ–°éã€ä¸­è‹±æ–‡é€šåƒçš„ IEEE è§£æå™¨
        data = extract_ieee_reference_full(ref_text)
        # é€™å€‹è§£æå™¨å›å‚³çš„ data['format'] é è¨­å°±æ˜¯ 'IEEE'
        
    else:
        # 2. éç·¨è™Ÿæ ¼å¼ (APA é¡)ï¼Œå†æ ¹æ“šèªè¨€åˆ†æµ
        if has_chinese(ref_text):
            data = extract_apa_zh_detailed(ref_text)
        else:
            data = extract_apa_en_detailed(ref_text)
            
    # [é—œéµæ•´åˆ]ï¼šç¢ºä¿å›å‚³çš„å­—å…¸åŒ…å«æ¯”å°é‚è¼¯æ‰€éœ€çš„ 'author' æ¬„ä½
    if isinstance(data.get('authors'), list):
        data['author'] = " ".join(data['authors']) 
    elif isinstance(data.get('authors'), str):
        data['author'] = data['authors']
    else:
        data['author'] = "Unknown"
        
    return data


# å¼•å…¥ï¼š
import re
from common_utils import (
    normalize_text,
    has_chinese,
    extract_doi,
    is_valid_year
)
from ieee_module import extract_ieee_reference_full