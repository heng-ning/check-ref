def format_pages_display(pages):
    """æ ¼å¼åŒ–é ç¢¼é¡¯ç¤ºï¼šå¦‚æœåŒ…å«å­—æ¯å°±ä¸åŠ  pp."""
    if not pages:
        return None
    if re.search(r'[A-Za-z]', pages):
        return pages  # S27â€“S31
    else:
        return f"pp. {pages}"  # pp. 123-456

# ===== è‹±æ–‡ APA =====
def parse_apa_authors_en(author_str):
    if not author_str: return []
    
    # å…ˆè™•ç† & æˆ– andï¼ˆAPA æœ€å¾Œä¸€å€‹ä½œè€…å‰çš„é€£æ¥è©ï¼‰
    # å°‡ & æˆ– and æ›¿æ›æˆé€—è™Ÿï¼Œçµ±ä¸€è™•ç†
    clean_str = re.sub(r'\s*,?\s*(&|and)\s+', ', ', author_str, flags=re.IGNORECASE)
    
    # ç”¨ã€Œ., ã€ï¼ˆé»è™Ÿ+é€—è™Ÿ+ç©ºæ ¼ï¼‰ä¾†åˆ†å‰²ä½œè€…
    # é€™æ¨£å¯ä»¥æ­£ç¢ºè™•ç† "Last, F. M., Next, A."
    segments = re.split(r'\.\s*,\s*', clean_str)
    
    authors = []
    for seg in segments:
        seg = seg.strip()
        if not seg: continue
        
        # ç§»é™¤çµå°¾çš„é»è™Ÿï¼ˆå¦‚æœæœ‰ï¼‰
        seg = seg.rstrip('.')
        
        if ',' in seg:
            # æ ¼å¼ï¼šLast, F. M.
            parts = seg.split(',', 1)
            last = parts[0].strip()
            first = parts[1].strip()
            # ç¢ºä¿ first name æœ‰é»è™Ÿçµå°¾
            if first and not first.endswith('.'):
                first += '.'
            authors.append({'last': last, 'first': first})
        else:
            # åªæœ‰å§“æ°
            authors.append({'last': seg, 'first': ''})
    
    return authors

def extract_apa_en_detailed(ref_text):
    result = {
        'format': 'APA (EN)', 'lang': 'EN',
        'authors': "Unknown", 'parsed_authors': [],
        'year': None, 'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None,
        'article_number': None,
        'publisher': None,
        'editors': None,
        'book_title': None,
        'edition': None,
        'source_type': None,
        'document_type': None,
        'url': None,
        'doi': None, 'original': ref_text
    }

    # å…ˆæå– DOI å’Œ URL (æå‰è™•ç†ï¼Œé¿å…å¹²æ“¾æ¨™é¡Œè§£æ)
    result['doi'] = extract_doi(ref_text)

    # æå– URL (æ”¯æ´å„ç¨®æ ¼å¼ï¼ŒåŒ…å«ç©ºæ ¼æ–·è¡Œå’Œé€£å­—è™Ÿæ–·è¡Œ)
    # æ‰¾åˆ° https:// é–‹é ­ï¼Œç„¶å¾Œå‘å¾ŒæŠ“å–ç›´åˆ°é‡åˆ°æ˜ç¢ºçš„çµæŸæ¨™è¨˜
    url_start = re.search(r'https?://', ref_text)
    if url_start:
        # å¾ https:// é–‹å§‹å‘å¾Œæƒæ
        start_pos = url_start.start()
        url_text = ref_text[start_pos:]
        
        # æ‰¾åˆ° URL çµæŸçš„ä½ç½®ï¼ˆé‡åˆ°å¥è™Ÿ+ç©ºæ ¼ã€é€—è™Ÿã€æˆ–æ–‡æœ«ï¼‰
        # é‡åˆ°ã€Œå¥è™Ÿ+æ›è¡Œ+å¤§å¯«å­—æ¯ã€ä¹Ÿè¦–ç‚ºçµæŸï¼ˆè™•ç† DOI æ–·è¡Œå•é¡Œï¼‰
        end_match = re.search(r'(?:\.\s*\n\s*[A-Z]|\.\s+[A-Z]|,\s|$)', url_text)
        if end_match:
            raw_url = url_text[:end_match.start()].strip()
        else:
            raw_url = url_text.strip()
        
        # æ¸…ç† URLï¼š
        # 1. å…ˆè™•ç†ã€Œé€£å­—è™Ÿ+ç©ºç™½ã€-> ä¿ç•™é€£å­—è™Ÿ
        clean_url = re.sub(r'-\s+', '-', raw_url)
        # 2. ç§»é™¤æ‰€æœ‰å‰©é¤˜ç©ºç™½
        clean_url = re.sub(r'\s+', '', clean_url)
        # 3. ç§»é™¤çµå°¾çš„å¥è™Ÿï¼ˆå¦‚æœæœ‰ï¼‰
        clean_url = clean_url.rstrip('.')
        
        result['url'] = clean_url

        # å¦‚æœ URL æ˜¯ DOI é€£çµï¼Œæ¸…ç©º URL æ¬„ä½
        if re.match(r'^https?://doi\.org/', clean_url, re.IGNORECASE):
            result['url'] = None

        # ä¿ç•™ url_match ä¾›å¾ŒçºŒä½¿ç”¨
        url_match = type('obj', (object,), {'group': lambda self, n: raw_url if n == 0 else None})()
    else:
        url_match = None
    
    year_match = re.search(r'[ï¼ˆ(]\s*(\d{4}[a-z]?|n\.d\.)\s*(?:,\s*[A-Za-z]+\.?\s*\d{0,2})?\s*[)ï¼‰]', ref_text)
    if not year_match: return result
    
    year_group = year_match.group(1)
    result['year'] = year_group if year_group.lower() != 'n.d.' else 'n.d.'

    # æå–å®Œæ•´æ—¥æœŸ (Month Day) - å…ˆæª¢æŸ¥ group æ˜¯å¦å­˜åœ¨
    try:
        date_match = year_match.group(2)
        if date_match:
            result['month'] = date_match
    except IndexError:
        pass  # æ²’æœ‰æœˆä»½è³‡è¨Šï¼Œè·³é
    
    author_part = ref_text[:year_match.start()].strip()
    result['authors'] = author_part
    result['parsed_authors'] = parse_apa_authors_en(author_part)
    
    content_part = ref_text[year_match.end():].strip()
    if content_part.startswith('.'): content_part = content_part[1:].strip()

    # ç§»é™¤ DOI å’Œ URLï¼Œé¿å…å®ƒå€‘è¢«èª¤åˆ¤ç‚ºæ¨™é¡Œæˆ–ä¾†æº
    if result['doi']:
        content_part = re.sub(r'(?:doi:|DOI:|https?://doi\.org/)\s*10\.\d{4,}/[^\sã€‚]+', '', content_part).strip()

    if result['url']:
        # ç§»é™¤åŸå§‹ URLï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„ç©ºæ ¼è®Šé«”ï¼‰
        if url_match:
            # å°‡åŸå§‹ URL ä¸­çš„ç©ºæ ¼è®Šæˆå½ˆæ€§åŒ¹é…æ¨¡å¼
            original_url_text = url_match.group(0)
            # å°‡ URL æ‹†æˆç‰‡æ®µï¼Œç”¨ \s* é€£æ¥ï¼ˆå…è¨±ä»»æ„ç©ºæ ¼ï¼‰
            url_parts = original_url_text.split()
            flexible_pattern = r'\s*'.join(re.escape(part) for part in url_parts)
            content_part = re.sub(flexible_pattern, '', content_part, flags=re.IGNORECASE)
        
        # ä¹Ÿç§»é™¤æ¸…ç†å¾Œçš„ URLï¼ˆä»¥é˜²è¬ä¸€ï¼‰
        content_part = content_part.replace(result['url'], '')
        
        # æ¸…ç†æ®˜ç•™çš„å¤šé¤˜ç©ºæ ¼å’Œæ¨™é»
        content_part = re.sub(r'\s+', ' ', content_part).strip()
        content_part = content_part.rstrip('. ')

    # åˆ¤æ–·æ˜¯å¦ç‚ºæ›¸ç±ç« ç¯€æˆ–ä¸€èˆ¬æ›¸ç±
    # å„ªå…ˆæª¢æŸ¥æ˜¯å¦ç‚ºæ›¸ç±ç« ç¯€æ ¼å¼ï¼ˆIn ... (Eds.)ï¼‰
    is_book_chapter = bool(re.search(r'\bIn\s+.+?\s*\(Eds?\.\)', content_part, re.IGNORECASE))

    # åˆ¤æ–·æ˜¯å¦ç‚ºæ›¸ç±
    is_book = is_book_chapter or bool(
        re.search(r'\(eds?\.\)', author_part, re.IGNORECASE) or 
        re.search(r'\b(manual|handbook|guide|textbook|encyclopedia|dictionary)\b', content_part, re.IGNORECASE)
    )

    if not is_book:
        # æª¢æŸ¥æ˜¯å¦æœ‰å·æœŸé ç¢¼ï¼ˆå¼·çƒˆæš—ç¤ºæœŸåˆŠï¼‰
        has_volume_issue = bool(re.search(r',\s*\d+\s*[\(ï¼ˆ]', content_part))
        has_volume_pages = bool(re.search(r',\s*\d+\s*,\s*[A-Z]?\d+', content_part))
        
        if not (has_volume_issue or has_volume_pages):
            # æ²’æœ‰å·æœŸé ç¢¼ï¼Œæª¢æŸ¥å‡ºç‰ˆç¤¾ç‰¹å¾µ
            
            # 1. çŸ¥åå‡ºç‰ˆç¤¾åç¨±
            well_known_publishers = r'\b(Wiley|Springer|Elsevier|Sage|Routledge|Pearson|McGraw|Oxford|Cambridge|Freeman|Jossey|Bass|Guilford|Palgrave|Macmillan|Penguin|Random|Simon|Schuster|HarperCollins|Norton|Houghton|Mifflin|Addison|Wesley)\b'
            if re.search(well_known_publishers, content_part, re.IGNORECASE):
                is_book = True
            
            # 2. å‡ºç‰ˆç¤¾é—œéµå­—
            elif re.search(r'\b(Press|Publisher|Publishing|Books|University|College|Institute|Foundation|Association|Inc\.|Ltd\.|LLC|Co\.|Group)\b', content_part, re.IGNORECASE):
                is_book = True
            
            # 3. çµæ§‹æ¨¡å¼ï¼šæ¨™é¡Œ. å‡ºç‰ˆç¤¾. ï¼ˆä¸”ä¸å«æœŸåˆŠé—œéµå­—ï¼‰
            else:
                sentence_splits = list(re.finditer(r'\.\s+[A-Z]', content_part))
                has_comma_with_numbers = bool(re.search(r',\s*\d+', content_part))
                
                if len(sentence_splits) == 1 and not has_comma_with_numbers:
                    last_part = content_part[sentence_splits[0].end()-1:].strip()
                    if not re.search(r'\b(Journal|Review|Magazine|Quarterly|Bulletin|Proceedings|Transactions|Annals)\b', last_part, re.IGNORECASE):
                        if re.match(r'^(?:[A-Z]\.\s+)*[A-Z][A-Za-z\-&]+(?:\s+[A-Z][A-Za-z\-&]+)*\.?\s*$', last_part):
                            is_book = True

    # æå–å¾Œè¨­è³‡æ–™ (å·æœŸé ç¢¼/æ–‡ç« ç·¨è™Ÿ)
    # æ ¼å¼ 1: Journal, Vol(Issue), pages. ä¾‹å¦‚ï¼šJournal, 14(2), 123-456.
    # æ ¼å¼ 2: Journal, Vol(Issue), article_number. ä¾‹å¦‚ï¼šJournal, 13(11), 6474.
    # æ ¼å¼ 3: Journal, Vol. ä¾‹å¦‚ï¼šJournal, 160.
    meta_match = re.search(
        r',\s*(\d+)(?:\s*\(([^)]+)\))?(?:,\s*([A-Za-z]?\d+(?:[\â€“\-][A-Za-z]?\d+)?))?(?:\.|\s|$)', 
        content_part
    )

    if meta_match:
        result['volume'] = meta_match.group(1)
        result['issue'] = meta_match.group(2) if meta_match.group(2) else None
        pages_or_article = meta_match.group(3)
        
        # åˆ¤æ–·æ˜¯é ç¢¼é‚„æ˜¯æ–‡ç« ç·¨è™Ÿ
        if pages_or_article and pages_or_article.strip():
            # å¦‚æœåŒ…å«é€£å­—è™Ÿï¼ˆ- æˆ– â€“ï¼‰ï¼Œä¸€å®šæ˜¯é ç¢¼
            if '-' in pages_or_article or 'â€“' in pages_or_article:
                result['pages'] = pages_or_article
            else:
                # ç´”æ•¸å­—æˆ–å¸¶å­—æ¯å‰ç¶´çš„æ•¸å­—ï¼Œåˆ¤æ–·æ˜¯æ–‡ç« ç·¨è™Ÿé‚„æ˜¯é ç¢¼
                # é‚è¼¯ï¼š4 ä½æ•¸ä»¥ä¸Šé€šå¸¸æ˜¯æ–‡ç« ç·¨è™Ÿï¼ˆå¦‚ 6474ï¼‰ï¼Œ3 ä½æ•¸ä»¥ä¸‹å¯èƒ½æ˜¯é ç¢¼
                if pages_or_article.isdigit():
                    if len(pages_or_article) >= 4:  # 4 ä½æ•¸ä»¥ä¸Š â†’ æ–‡ç« ç·¨è™Ÿ
                        result['article_number'] = pages_or_article
                    else:  # 3 ä½æ•¸ä»¥ä¸‹ â†’ å¯èƒ½æ˜¯å–®é é ç¢¼
                        result['pages'] = pages_or_article
                else:
                    # å¸¶å­—æ¯çš„ï¼ˆå¦‚ S27ï¼‰â†’ å¯èƒ½æ˜¯é ç¢¼æˆ–æ–‡ç« ç·¨è™Ÿ
                    # ç°¡å–®åˆ¤æ–·ï¼šå¸¶å­—æ¯çš„çŸ­æ•¸å­—è¦–ç‚ºé ç¢¼
                    if len(pages_or_article) <= 4:
                        result['pages'] = pages_or_article
                    else:
                        result['article_number'] = pages_or_article
        
        title_source_part = content_part[:meta_match.start()].strip()
    else:
        # æ ¼å¼ 2: å‚³çµ±é ç¢¼æ ¼å¼ pp. 123-456 æˆ– pp. S27â€“S31
        pp_match = re.search(r',?\s*pp?\.?\s*([A-Za-z]?\d+[\â€“\-][A-Za-z]?\d+)(?:\.)?$', content_part)
        if pp_match:
            result['pages'] = pp_match.group(1)
            title_source_part = content_part[:pp_match.start()].strip()
        else:
            title_source_part = content_part

    # æ”¹é€²æ¨™é¡Œèˆ‡ä¾†æºåˆ†å‰²é‚è¼¯
    if is_book:
        # === å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºæ›¸ç±ç« ç¯€æ ¼å¼ ===
        # æ ¼å¼ï¼šç« ç¯€æ¨™é¡Œ. In ç·¨è€… (Eds.), æ›¸å (pp. xxx). å‡ºç‰ˆç¤¾.
        chapter_match = re.search(
            r'^(.+?)\.\s+In\s+(.+?)\s*\(Eds?\.\),\s*(.+?)\s*\((?:(\d+(?:st|nd|rd|th)\s+ed\.),?\s*)?pp\.\s*([\d\s\â€“\-â€”]+)\)', 
            title_source_part, 
            re.IGNORECASE
        )
        if chapter_match:
            # é€™æ˜¯æ›¸ç±ç« ç¯€
            result['title'] = chapter_match.group(1).strip()  # ç« ç¯€æ¨™é¡Œ
            result['editors'] = "In " + chapter_match.group(2).strip() + " (Eds.)"  # ç·¨è€…
            result['book_title'] = chapter_match.group(3).strip()  # æ›¸å
            
            # ç‰ˆæ¬¡ï¼ˆå¯èƒ½ç‚º Noneï¼‰
            if chapter_match.group(4):
                result['edition'] = chapter_match.group(4).strip()
            
            # æ¸…ç†é ç¢¼ä¸­çš„å¤šé¤˜ç©ºæ ¼
            raw_pages = chapter_match.group(5).strip()
            clean_pages = re.sub(r'\s+', '', raw_pages)  # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
            result['pages'] = clean_pages  # ä¾‹å¦‚ "254â€“257"
            
            # å‡ºç‰ˆç¤¾åœ¨æ‹¬è™Ÿå¾Œé¢
            after_chapter = title_source_part[chapter_match.end():].strip()
            # ç§»é™¤é–‹é ­çš„å¥é»å’Œç©ºæ ¼
            after_chapter = after_chapter.lstrip('. ').strip()
            if after_chapter:
                # ç§»é™¤çµå°¾çš„å¥é»
                result['publisher'] = after_chapter.rstrip('.')
            
            result['source_type'] = 'Book Chapter'
        else:
            # ä¸€èˆ¬æ›¸ç±æ ¼å¼ï¼šæ¨™é¡Œ. å‡ºç‰ˆç¤¾.
            split_match = re.search(r'\.\s+([A-Z])', title_source_part)

            if split_match:
                split_pos = split_match.start()
                result['title'] = title_source_part[:split_pos].strip()
                
                # å‡ºç‰ˆç¤¾éƒ¨åˆ†ï¼šå¾åŒ¹é…çš„å¤§å¯«å­—æ¯é–‹å§‹åˆ°çµå°¾
                publisher_part = title_source_part[split_match.end() - 1:].strip()
                result['publisher'] = publisher_part.rstrip('.')
                
                # æª¢æŸ¥æ¨™é¡Œä¸­æ˜¯å¦åŒ…å«ç‰ˆæ¬¡è³‡è¨Š
                edition_in_title = re.search(r'\((\d+(?:st|nd|rd|th)\s+ed\.)\)\s*$', result['title'])
                if edition_in_title:
                    result['edition'] = edition_in_title.group(1)
                    # å¾æ¨™é¡Œä¸­ç§»é™¤ç‰ˆæ¬¡éƒ¨åˆ†
                    result['title'] = result['title'][:edition_in_title.start()].strip()
            else:
                result['title'] = title_source_part.rstrip('.')
                
                # æª¢æŸ¥æ¨™é¡Œä¸­æ˜¯å¦åŒ…å«ç‰ˆæ¬¡è³‡è¨Šï¼ˆç„¡å‡ºç‰ˆç¤¾çš„æƒ…æ³ï¼‰
                edition_in_title = re.search(r'\((\d+(?:st|nd|rd|th)\s+ed\.)\)\s*$', result['title'])
                if edition_in_title:
                    result['edition'] = edition_in_title.group(1)
                    result['title'] = result['title'][:edition_in_title.start()].strip()
    else:
        # æœŸåˆŠæ ¼å¼ï¼šæ¨™é¡Œ. æœŸåˆŠå
        # å…ˆè­˜åˆ¥ä¸¦ç§»é™¤æ–‡ç»é¡å‹æ¨™è¨» (å¦‚ [Project Report], Technical Report ç­‰)
        document_type_pattern = r'\.\s*(\[?(?:Project|Technical|Research|Working|Conference|Discussion)\s+(?:Report|Paper|Brief)\]?)\.'
        doc_type_match = re.search(document_type_pattern, title_source_part, re.IGNORECASE)
        
        if doc_type_match:
            # æå–æ–‡ç»é¡å‹
            result['document_type'] = doc_type_match.group(1).strip('[]')
            
            # å¾ title_source_part ä¸­ç§»é™¤æ–‡ç»é¡å‹
            title_source_part = (
                title_source_part[:doc_type_match.start()] + 
                '. ' + 
                title_source_part[doc_type_match.end():]
            ).strip()
        
        # åŸæœ¬çš„æ¨™é¡Œèˆ‡ä¾†æºåˆ†å‰²é‚è¼¯
        split_index = title_source_part.rfind('. ')
        if split_index != -1:
            result['title'] = title_source_part[:split_index].strip()
            result['source'] = title_source_part[split_index + 1:].strip().rstrip('.')
        else:
            if not title_source_part.startswith('http'):
                result['title'] = title_source_part.rstrip('.')

    # æ¸…ç†æ‰€æœ‰æ–‡å­—æ¬„ä½ä¸­çš„æ–·è¡Œé€£å­—è™Ÿ
    text_fields = ['title', 'source', 'publisher', 'editors', 'book_title', 'journal_name', 'conference_name']
    for field in text_fields:
        if result.get(field) and isinstance(result[field], str):
            # ç§»é™¤å–®å­—ä¸­çš„æ–·è¡Œé€£å­—è™Ÿï¼ˆå¦‚ "perform- ance" -> "performance"ï¼‰
            # æ¨¡å¼1: é€£å­—è™Ÿ+ç©ºæ ¼+å°å¯«å­—æ¯
            result[field] = re.sub(r'-\s+([a-z])', r'\1', result[field])
            # æ¨¡å¼2: å–®ç´”çš„é€£å­—è™Ÿ+ç©ºæ ¼ï¼ˆå‚™ç”¨ï¼‰
            result[field] = re.sub(r'-\s+', '', result[field])
        if result['parsed_authors']:
        # è½‰æˆ ["Hwang G. H.", "Chen P. H.", ...]
            result['authors'] = [
            f"{a['last']} {a['first']}".strip()
            for a in result['parsed_authors']
        ]
    return result

# ===== ä¸­æ–‡ APA =====
def parse_chinese_authors(author_str):
    if not author_str: return []
    clean_str = re.sub(r'\s*(ç­‰|è‘—|ç·¨)$', '', author_str)
    return re.split(r'[ã€ï¼Œ,]', clean_str)

def extract_apa_zh_detailed(ref_text):
    result = {
        'format': 'APA (ZH)', 'lang': 'ZH',
        'authors': [], 'year': None, 'title': None, 'source': None,
        'volume': None, 'issue': None, 'pages': None,
        'url': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)

    # å…ˆç§»é™¤è¡Œé¦–çš„æ•¸å­—ç·¨è™Ÿï¼Œå¦‚ "5. " æˆ– "12. "
    ref_text = re.sub(r'^\s*\d+\.\s*', '', ref_text)

    # æå– URL
    url_match = re.search(r'https?://[^\sã€‚]+', ref_text)
    if url_match:
        raw_url = url_match.group(0)
        result['url'] = raw_url.rstrip('ã€‚.')
        # å¾ ref_text ä¸­ç§»é™¤ URLï¼Œé¿å…æ±¡æŸ“å¾ŒçºŒè§£æ
        ref_text = ref_text[:url_match.start()].strip().rstrip('ã€‚. ')

    year_match = re.search(r'[ï¼ˆ(]\s*(\d{2,4})\s*[)ï¼‰]', ref_text)
    if not year_match: 
        # ç„¡ä½œè€…çš„ç‰¹æ®Šæ ¼å¼è™•ç† (å¦‚æ”¿åºœæ–‡ä»¶)
        special_match = re.search(r'(.+?)[ï¼ˆ(](\d{4})\s*å¹´.+?[)ï¼‰]', ref_text)
        if special_match:
            result['title'] = special_match.group(1).strip()
            result['year'] = special_match.group(2)
            result['authors'] = []
            
            # æå– URL
            url_match = re.search(r'https?://[^\s]+', ref_text)
            if url_match:
                result['url'] = url_match.group(0).rstrip('ã€‚.')
            
            return result
        return result
    
    result['year'] = year_match.group(1)
    author_part = ref_text[:year_match.start()].strip()

    # ç§»é™¤ä½œè€…åç¨±ä¸­é–“çš„ä¸­æ–‡ç©ºç™½ï¼Œä¾‹å¦‚ã€Œæ•™è‚²éƒ¨ é«”è‚²ç½²ã€â†’ã€Œæ•™è‚²éƒ¨é«”è‚²ç½²ã€
    author_part = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', author_part)

    result['authors'] = parse_chinese_authors(author_part)
    
    rest = ref_text[year_match.end():].strip().lstrip('.ã€‚ ')

    # å…ˆå˜—è©¦æå–å·æœŸé ç¢¼ï¼Œä¸¦å¾ rest ä¸­ç§»é™¤
    meta_match = re.search(
        r'[,ï¼Œ]\s*(\d+)\s*[å·]?\s*(?:[ï¼ˆ(]\s*(\d+)\s*[)ï¼‰æœŸ]?)?\s*[,ï¼Œã€‚]\s*(\d+)\s*[â€“\-~]\s*(\d+)',
        rest
    )
    if meta_match:
        result['volume'] = meta_match.group(1)
        if meta_match.group(2):
            result['issue'] = meta_match.group(2)
        result['pages'] = f"{meta_match.group(3)}â€“{meta_match.group(4)}"
        # ç§»é™¤å·æœŸé ç¢¼éƒ¨åˆ†ï¼Œä¸¦æ¸…ç†çµå°¾çš„æ¨™é»
        rest = rest[:meta_match.start()].strip()
        rest = rest.rstrip(',ï¼Œã€‚. ')
    else:
        # åªæœ‰å·è™Ÿï¼ˆåŸæœ¬çš„é‚è¼¯ä¿æŒä¸è®Šï¼‰
        vol_match = re.search(r'[,ï¼Œ]\s*(\d+)\s*[å·]', rest)
        if vol_match:
            result['volume'] = vol_match.group(1)
            rest = rest[:vol_match.start()].strip().rstrip(',ï¼Œã€‚. ')

    match_book = re.search(r'ã€Š([^ã€‹]+)ã€‹', rest)
    match_article = re.search(r'ã€ˆ([^ã€‰]+)ã€‰', rest)
    
    # æå–æ¨™é¡Œå’Œä¾†æº
    if match_article:
        result['title'] = match_article.group(1)
        if match_book: result['source'] = match_book.group(1)
    elif match_book:
        pre_book = rest[:match_book.start()].strip()
        if pre_book:
            result['title'] = pre_book.rstrip('ã€‚. ')
            result['source'] = match_book.group(1)
        else:
            result['title'] = match_book.group(1)
    else:
        # å„ªå…ˆç”¨ã€Œä¸­æ–‡å¥è™Ÿã€åˆ†éš”
        # é¿å…èª¤åˆ¤å°æ•¸é»ï¼ˆå¦‚ 2.0.ï¼‰
        match = re.search(r'ã€‚', rest)
        
        if match:
            result['title'] = rest[:match.start()].strip()
            result['source'] = rest[match.end():].strip().lstrip('ã€‚.,ï¼Œ. ').rstrip(',ï¼Œã€‚. ')
        else:
            # å¦‚æœæ²’æœ‰ä¸­æ–‡å¥è™Ÿï¼Œæ‰¾ã€Œå‰å¾Œéƒ½ä¸æ˜¯æ•¸å­—çš„è‹±æ–‡å¥è™Ÿã€
            match_en = re.search(r'(?<!\d)\.(?!\d)', rest)
            if match_en:
                result['title'] = rest[:match_en.start()].strip()
                result['source'] = rest[match_en.end():].strip().lstrip('ã€‚.,ï¼Œ. ').rstrip(',ï¼Œã€‚. ')
            else:
                result['title'] = rest.strip()

    return result


def extract_numbered_zh_detailed(ref_text):
    result = {
        'format': 'Numbered (ZH)', 'lang': 'ZH',
        'ref_number': None, 'authors': [], 'year': None, 'title': None, 'source': None,
        'doi': None, 'original': ref_text
    }
    result['doi'] = extract_doi(ref_text)
    num_match = re.match(r'^\s*[\[ã€]\s*(\d+)\s*[\]ã€‘]', ref_text)
    if num_match:
        result['ref_number'] = num_match.group(1)
        rest = ref_text[num_match.end():].strip()
    else:
        rest = ref_text
    year_match = re.search(r'\b(\d{4})\b', rest)
    if year_match: result['year'] = year_match.group(1)
    
    match_book = re.search(r'ã€Š([^ã€‹]+)ã€‹', rest)
    if match_book:
        result['source'] = match_book.group(1)
        pre = rest[:match_book.start()]
        # å˜—è©¦æŠ“ä½œè€…å’Œç¯‡å
        parts = re.split(r'[ï¼Œ,]', pre)
        if len(parts) > 0: result['authors'] = parse_chinese_authors(parts[0])
        if len(parts) > 1: result['title'] = parts[1]
    else:
        # [UPDATED] å¢åŠ å¾Œå‚™æ–¹æ¡ˆï¼Œå˜—è©¦æŠ“å–ä¾†æº (å‡è¨­çµæ§‹: ä½œè€…, ç¯‡å, ä¾†æº)
        parts = re.split(r'[ï¼Œ,ã€‚.]', rest)
        parts = [p.strip() for p in parts if p.strip()]
        if len(parts) > 0: result['authors'] = parse_chinese_authors(parts[0])
        if len(parts) > 1: result['title'] = parts[1]
        if len(parts) > 2: result['source'] = parts[2] # å˜—è©¦æŠ“å–ä¾†æº

    return result

# ===== APA æ–·è¡Œåˆä½µï¼ˆæ··åˆæ¨¡å¼ï¼‰=====
def find_apa_head(ref_text):
    """[NEW] åµæ¸¬ APA æ ¼å¼é–‹é ­ (å¹´ä»½) - å–ä»£èˆŠçš„ find_apa"""
    # è‹±æ–‡ APA: Author (2020).
    # ä¸­æ–‡ APA: ä½œè€… (2020)ã€‚
    match = re.search(r'[ï¼ˆ(]\s*(\d{4}(?:[a-z])?|n\.d\.)\s*(?:,\s*([A-Za-z]+\.?\s*\d{0,2}))?\s*[)ï¼‰]', ref_text)
    if not match: return False
    
    # ç¢ºä¿å¹´ä»½æ‹¬è™Ÿå‡ºç¾åœ¨å‰é¢ (ä¾‹å¦‚å‰ 50 å€‹å­—å…§ï¼Œé¿å…èª¤åˆ¤æ–‡ä¸­çš„å¹´ä»½)
    if match.start() > 80: return False 
    
    return True

def is_reference_head_unified(para):
    """
    [UPDATED] [APA/æ··åˆæ¨¡å¼] åˆ¤æ–·ä¸€è¡Œæ–‡å­—æ˜¯å¦ç‚ºæ–°æ–‡ç»
    """
    para = normalize_text(para)

    # DOI ç‰¹å¾µï¼šæ•¸å­—é–‹é ­ + æ–œç·š + å­—æ¯æ•¸å­—æ··åˆ
    if re.match(r'^\d{4,}/[a-z0-9\.\-/]+', para, re.IGNORECASE):
        return False
    if re.match(r'^[a-z0-9]+\-[a-z]{2}$', para, re.IGNORECASE):
        return False
    
    # 0. âœ… å¼·ç‰¹å¾µç™½åå–®ï¼šæ˜ç¢ºçš„æ–°æ–‡ç»é–‹é ­ï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
    
    # A. ç·¨è™Ÿæ ¼å¼ [1]
    if re.match(r'^\s*[\[ã€]\s*\d+\s*[ã€‘\]]', para):
        return True
    
    # B. æ¨™æº– APA ä½œè€…æ ¼å¼ï¼šLast, F. é–‹é ­
    # åªè¦é–‹é ­æ˜¯ "å§“, åç¸®å¯«"ï¼Œä¸”ä¸æ˜¯å°å¯«æˆ–æ•¸å­—é–‹é ­ï¼Œå°±å¾ˆå¯èƒ½æ˜¯æ–°æ–‡ç»
    # ä¸ç®¡å¹´ä»½åœ¨å“ªï¼ˆå¯èƒ½è¢«æ–·è¡Œåˆ°ä¸‹ä¸€æ®µï¼‰
    author_start = re.match(r'^([A-Z][A-Za-z\-\']+),\s+([A-Z]\.(?:\s*[A-Z]\.)*)', para)
    
    # C. çµ„ç¹”ä½œè€…æ ¼å¼ï¼šé–‹é ­å¤§å¯«å–®å­— + (ç¸®å¯«) + å¹´ä»½
    # ä¾‹å¦‚ï¼šWorld Health Organization (WHO). (2020)
    org_author_match = re.match(
        r'^[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+)*\s*\([A-Z]+\)\.\s*\((\d{4})', 
        para
    )
    if org_author_match:
        year = org_author_match.group(1)
        if is_valid_year(year):
            return True

    # D. ä¸€èˆ¬çµ„ç¹”ä½œè€…ï¼ˆæ²’æœ‰ç¸®å¯«ï¼‰ï¼šé–‹é ­å¤šå€‹å¤§å¯«å–®å­— + å¹´ä»½
    # ä¾‹å¦‚ï¼šNational Research Council. (2019)
    org_simple_match = re.match(
        r'^[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){2,}\.\s*\((\d{4})', 
        para
    )
    if org_simple_match:
        year = org_simple_match.group(1)
        if is_valid_year(year):
            return True

    if author_start:
        # é€²ä¸€æ­¥é©—è­‰ï¼šæ’é™¤æ˜é¡¯ä¸æ˜¯ä½œè€…çš„æƒ…æ³
        # 1. å¾Œé¢ä¸èƒ½ç›´æ¥æ¥å°å¯«å­—æ¯ï¼ˆè¡¨ç¤ºæ˜¯å¥å­ä¸­é–“ï¼‰
        after_author = para[author_start.end():].strip()
        if after_author and after_author[0].islower():
            pass  # å¯èƒ½æ˜¯å¥å­ï¼Œä¸è™•ç†
        else:
            # 2. æª¢æŸ¥æ˜¯å¦æœ‰åˆç†çš„å¾ŒçºŒå…§å®¹ï¼ˆé€—è™Ÿã€&ã€orã€å¹´ä»½æ‹¬è™Ÿï¼‰
            if re.match(r'^[,&\(]', after_author) or not after_author:
                return True
            # 3. å¦‚æœå¾Œé¢é‚„æœ‰å…¶ä»–ä½œè€…åï¼ˆèªªæ˜æ˜¯ä½œè€…åˆ—è¡¨é–‹é ­ï¼‰
            if re.search(r'[,&]\s+[A-Z][a-z]+,\s+[A-Z]\.', after_author[:50]):
                return True
            # 4. å¦‚æœæ˜¯ DOI/URL çµå°¾å¾Œçš„æ–°ä½œè€…
            # æª¢æŸ¥ï¼šä½œè€…æ ¼å¼å®Œæ•´ + å¾Œé¢æœ‰å¹´ä»½ â†’ é€™æ˜¯æ–°æ–‡ç»
            if re.search(r'\(\d{4}\)', para):
                return True
    
    # 1. ğŸš« é»‘åå–®ï¼šçµ•å°ä¸æ˜¯æ–°æ–‡ç»çš„æƒ…æ³
    
    # A. ç¶²å€ä¿è­·
    if re.search(r'(https?://|doi\.org|doi:|www\.)', para, re.IGNORECASE):
        url_only = re.sub(r'https?://[^\s]+', '', para).strip()
        if len(url_only) < 10:
            return False
        if not (re.match(r'^\s*[\[ã€]', para) or author_start):
            return False
            
    # B. æœˆä»½/æ—¥æœŸä¿è­·
    if re.match(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*\d{4}', para, re.IGNORECASE):
        return False
        
    # C. å·æœŸé ç¢¼ä¿è­·
    if re.match(r'^(Vol\.|No\.|pp\.|p\.|Page)', para, re.IGNORECASE):
        return False
        
    # D. å°å¯«é–‹é ­ä¿è­·
    if re.match(r'^[a-z]', para):
        return False
    
    # E. ä½œè€…åˆ—è¡¨å»¶çºŒä¿è­·ï¼ˆåªæœ‰ & æˆ–é€—è™Ÿ+åå­—ï¼Œæ²’æœ‰å§“æ°é–‹é ­ï¼‰
    # ä¾‹å¦‚ï¼š", & Varatharajan, S." é€™ç¨®ä¸ç®—æ–°æ–‡ç»é–‹é ­
    if re.match(r'^[,&]\s', para):
        return False

    # å¦‚æœé–‹é ­æ˜¯ç¸®å¯«ï¼ˆå¦‚ "A., Malhotra"ï¼‰ï¼Œä½†å¾Œé¢æ²’æœ‰å¹´ä»½æ‹¬è™Ÿ (20XX)
    # é€™æ˜¯ä½œè€…åˆ—è¡¨å»¶çºŒï¼Œä¸æ˜¯æ–°æ–‡ç»é–‹é ­
    # ä¾‹å¦‚ï¼š"A., Malhotra, R. K., & Martin, J. L." (æ²’æœ‰å¹´ä»½)
    if re.match(r'^[A-Z]\.(?:\s*[A-Z]\.)*\s*,', para):
        # æª¢æŸ¥é€™ä¸€æ®µæ˜¯å¦æœ‰å¹´ä»½æ‹¬è™Ÿ (19XX) æˆ– (20XX)
        # å¦‚æœæ²’æœ‰å¹´ä»½ï¼Œé€™è‚¯å®šæ˜¯ä½œè€…åˆ—è¡¨å»¶çºŒ
        if not re.search(r'[ï¼ˆ(]\s*(?:19|20)\d{2}', para):
            return False

    # 2. âœ… å…¶ä»–ç™½åå–®ç‰¹å¾µ
    
    # C. APA æ¨™æº–æ ¼å¼ (Year) - å¹´ä»½åœ¨æ‹¬è™Ÿå…§
    if find_apa_head(para):
        return True
        
    # D. é¡ APA (Year in dots)
    year_match = re.search(r'[\.,]\s*(19|20)\d{2}[a-z]?[\.,]', para[:80])
    if year_match:
        pre_text = para[:year_match.start()].strip()
        if len(pre_text) > 3:
            if not has_chinese(para):
                if ',' in pre_text or '.' in pre_text:
                    return True
            else:
                return True

    return False

def merge_references_unified(paragraphs):
    """[UPDATED from test1204-6] [APA/æ··åˆæ¨¡å¼] åˆä½µæ–·è¡Œ"""
    merged = []
    current_ref = ""
    
    for i, para in enumerate(paragraphs):
        para = para.strip()
        if not para: continue
        
        # æ’é™¤ç´”æ•¸å­—é ç¢¼ (é•·åº¦çŸ­ä¸”ç„¡é€£å­—è™Ÿ)
        if para.isdigit() and len(para) < 4: continue

        # æ’é™¤é é¦–/é å°¾æ–‡å­—
        # ç‰¹å¾µï¼šå…¨å¤§å¯«ã€é•·åº¦çŸ­ã€æ²’æœ‰å¹´ä»½æ‹¬è™Ÿã€æ²’æœ‰ç·¨è™Ÿ
        if para.isupper() and len(para) < 50:
            # 1. åŒ…å« ET AL çš„ä½œè€…é é¦–
            if 'ET AL' in para:
                continue
            # 2. ç¸®å¯«é–‹é ­çš„é é¦–ï¼ˆå¦‚ "S. JAYDARIFARD ET AL."ï¼‰
            if re.match(r'^[A-Z]\.\s+[A-Z]+', para):
                continue
            # 3. æœŸåˆŠåç¨±æˆ–ç« ç¯€æ¨™é¡Œçš„é é¦–ï¼ˆå¦‚ "TRANSPORT REVIEWS"ï¼‰
            # æ’é™¤æ¢ä»¶ï¼šå…¨å¤§å¯« + æ²’æœ‰æ•¸å­— + æ²’æœ‰æ‹¬è™Ÿ + æ²’æœ‰æ¨™é»ï¼ˆé™¤äº†ç©ºæ ¼ï¼‰
            if not re.search(r'[\d\(\)\[\]\.,:;]', para):
                continue  # è·³éé€™è¡Œ
        
        is_new_ref = is_reference_head_unified(para)

        # ç‰¹æ®Šåˆ¤æ–·ï¼šå¦‚æœç•¶å‰æ–‡ç»ä»¥ & æˆ– , çµå°¾ï¼ˆè¡¨ç¤ºä½œè€…åˆ—è¡¨æœªå®Œæˆï¼‰
        # ä¸”é€™è¡Œé–‹é ­æ˜¯ä½œè€…å+å¹´ä»½ï¼Œé€™è¡Œæ‡‰è©²æ˜¯ä½œè€…åˆ—è¡¨çš„æœ€å¾Œä¸€ä½ï¼Œä¸æ˜¯æ–°æ–‡ç»
        if is_new_ref and current_ref:
            # æª¢æŸ¥ä¸Šä¸€è¡Œçµå°¾
            current_ref_stripped = current_ref.rstrip()
            if current_ref_stripped.endswith('&') or current_ref_stripped.endswith(','):
                # æª¢æŸ¥é€™è¡Œæ˜¯å¦ç‚ºï¼šä½œè€…å + å¹´ä»½ï¼ˆä½œè€…åˆ—è¡¨æœ€å¾Œä¸€ä½çš„æ¨¡å¼ï¼‰
                # ä¾‹å¦‚ï¼šVaratharajan, S. (2019). ...
                if re.match(r'^[A-Z][A-Za-z\-\']+,\s+[A-Z]\.\s*[ï¼ˆ(]', para):
                    # é€™æ˜¯ä½œè€…åˆ—è¡¨çš„æœ€å¾Œä¸€ä½ï¼Œæ‡‰è©²åˆä½µ
                    is_new_ref = False

        # å¦‚æœç•¶å‰ç´¯ç©çš„æ–‡ç»æ²’æœ‰å¹´ä»½ï¼Œä¸”æ–°æ®µè½æœ‰å¹´ä»½
        # é‚£æ–°æ®µè½æ‡‰è©²æ˜¯ç•¶å‰æ–‡ç»çš„å»¶çºŒï¼Œä¸æ˜¯æ–°æ–‡ç»
        if is_new_ref and current_ref:
            # æª¢æŸ¥ current_ref æ˜¯å¦æœ‰å¹´ä»½
            has_year_in_current = bool(re.search(r'[ï¼ˆ(]\s*(?:19|20)\d{2}', current_ref))
            # æª¢æŸ¥ para æ˜¯å¦æœ‰å¹´ä»½
            has_year_in_para = bool(re.search(r'[ï¼ˆ(]\s*(?:19|20)\d{2}', para))
            
            # å¦‚æœç•¶å‰æ–‡ç»æ²’å¹´ä»½ï¼Œä½†æ–°æ®µè½æœ‰å¹´ä»½ â†’ æ–°æ®µè½æ˜¯å»¶çºŒ
            if not has_year_in_current and has_year_in_para:
                is_new_ref = False
        
        # å¦‚æœç•¶å‰ç´¯ç©çš„æ–‡ç»ä»¥ DOI æˆ–å®Œæ•´ URL çµå°¾ä¸”æ–°æ®µè½æ˜¯æ˜ç¢ºçš„ä½œè€…é–‹é ­ï¼Œå¼·åˆ¶åˆ‡åˆ†
        if current_ref and not is_new_ref:
            current_stripped = current_ref.rstrip()
            # æª¢æŸ¥æ˜¯å¦ä»¥ DOI æˆ– URL çµå°¾
            ends_with_doi_url = bool(
                re.search(r'(https?://[^\s]+|doi\.org/[^\s]+|10\.\d{4}/[^\s]+)[.\s]*$', current_stripped) or
                re.search(r'[ã€‚ï¼][ï¼‰)]?\s*$', current_stripped) or  # ä¸­æ–‡å¥è™Ÿçµå°¾ï¼ˆå¯èƒ½æœ‰æ‹¬è™Ÿï¼‰
                re.search(r'[\d]+\s*[â€“\-â€”]\s*[\d]+[ã€‚ï¼]\s*$', current_stripped)  # é ç¢¼+å¥è™Ÿçµå°¾
            )
            
            # æª¢æŸ¥æ–°æ®µè½æ˜¯å¦ç‚ºæ˜ç¢ºçš„ä½œè€…é–‹é ­
            clear_author_start = bool(
                re.match(r'^([A-Z][A-Za-z\-\']+),\s+([A-Z]\.(?:\s*[A-Z]\.)*)', para) and
                re.search(r'\(\d{4}\)', para) or
                re.match(r'^[\u4e00-\u9fff]{2,}[ã€ï¼ˆ(]', para)  # ä¸­æ–‡ä½œè€…é–‹é ­
            )
            
            # å¦‚æœå…©å€‹æ¢ä»¶éƒ½æ»¿è¶³ï¼Œå¼·åˆ¶åˆ‡åˆ†
            if ends_with_doi_url and clear_author_start:
                is_new_ref = True

        if is_new_ref:
            if current_ref:
                merged.append(current_ref)
            current_ref = para
        else:
            if current_ref:
                if has_chinese(current_ref[-1:]) and has_chinese(para[:1]):
                    current_ref += "" + para
                elif current_ref.endswith('-'):
                    # åˆ¤æ–·æ˜¯å¦ç‚ºå–®å­—æ–·è¡Œ
                    if para and para[0].islower():
                        current_ref = current_ref[:-1] + para
                    else:
                        current_ref = current_ref + " " + para
                # è™•ç†é ç¢¼æ–·è¡Œï¼šé€£å­—è™Ÿ+ç©ºæ ¼+æ•¸å­—
                elif re.search(r'[\â€“\-â€”]\s*$', current_ref) and para and para[0].isdigit():
                    current_ref = current_ref.rstrip() + para
                # è™•ç† DOI æ–·è¡Œ
                elif re.search(r'doi\.org/[^\s]+\.$', current_ref, re.IGNORECASE) and para and para[0].isdigit():
                    current_ref = current_ref + para  # DOI ç›´æ¥é€£æ¥
                # è™•ç†ä¸€èˆ¬ URL çµå°¾æ˜¯å¥é»çš„æ–·è¡Œ
                elif re.search(r'https?://[^\s]+\.$', current_ref) and para and not para[0].isupper():
                    current_ref = current_ref + para
                else:
                    current_ref += " " + para
            else:
                current_ref = para
            
    if current_ref: 
        merged.append(current_ref)
    
    return merged

# ===== APA æ ¼å¼è½‰æ› =====
def convert_en_apa_to_ieee(data):
    ieee_authors = []
    for auth in data.get('parsed_authors', []):
        ieee_authors.append(f"{auth['first']} {auth['last']}".strip())
    auth_str = ", ".join(ieee_authors)
    if len(ieee_authors) > 2: auth_str = re.sub(r', ([^,]+)$', r', and \1', auth_str)
    
    parts = []
    if auth_str: parts.append(auth_str + ",")
    if data.get('title'): parts.append(f'"{data["title"]},"')
    
    # è™•ç†æ›¸ç±ç« ç¯€
    if data.get('source_type') == 'Book Chapter':
        # æ ¼å¼ï¼šä½œè€…, "ç« ç¯€æ¨™é¡Œ," ç·¨è€…, æ›¸å, ç‰ˆæ¬¡, å‡ºç‰ˆç¤¾, å¹´ä»½, pp. é ç¢¼.
        if data.get('editors'):
            parts.append(f"{data['editors']},")
        if data.get('book_title'):
            parts.append(f"{data['book_title']},")
        if data.get('edition'):
            parts.append(f"{data['edition']},")
        if data.get('publisher'):
            parts.append(f"{data['publisher']},")
        if data.get('year'):
            parts.append(f"{data['year']},")
        if data.get('pages'):
            parts.append(f"pp. {data['pages']}.")
    else:
        # åˆ†åˆ¥è™•ç†æœŸåˆŠå’Œæ›¸ç±
        if data.get('source'):  # æœŸåˆŠ
            parts.append(f"{data['source']},")
            
            # å·æœŸè™•ç†
            if data.get('volume'):
                volume_str = f"vol. {data['volume']}"
                
                if data.get('issue'):
                    issue_val = str(data['issue'])
                    # åˆ¤æ–·æœŸè™Ÿæ˜¯ç´”æ•¸å­—é‚„æ˜¯æ–‡å­—
                    if issue_val.isdigit() or re.match(r'^\d+[\-â€“â€”]\d+$', issue_val):
                        # ç´”æ•¸å­—æˆ–æ•¸å­—ç¯„åœï¼šä½¿ç”¨ vol. X, no. Y æ ¼å¼
                        volume_str += f", no. {issue_val}"
                    else:
                        # åŒ…å«æ–‡å­—ï¼ˆå¦‚ Supplementï¼‰ï¼šä½¿ç”¨ vol. X(Y) æ ¼å¼
                        volume_str = f"vol. {data['volume']}({issue_val})"
                
                parts.append(volume_str + ",")
            
            # é ç¢¼è™•ç†
            if data.get('pages'):
                pages_val = data['pages']
                # å¦‚æœé ç¢¼åŒ…å«å­—æ¯ï¼Œä¸åŠ  pp.
                if re.search(r'[A-Za-z]', pages_val):
                    parts.append(f"{pages_val},")
                else:
                    parts.append(f"pp. {pages_val},")
                    
        elif data.get('publisher'):  # ä¸€èˆ¬æ›¸ç±
            if data.get('edition'):
                parts.append(f"{data['edition']},")
            parts.append(f"{data['publisher']},")
        
        # åŠ å…¥æœˆä»½
        if data.get('month'): parts.append(f"{data['month']}")
        if data.get('year'): parts.append(f"{data['year']}.")
    
    # åŠ å…¥ DOI æˆ– URL
    if data.get('doi'): 
        parts.append(f"doi: {data['doi']}.")
    elif data.get('url'): 
        parts.append(f"[Online]. Available: {data['url']}")
    
    return " ".join(parts)

def convert_zh_apa_to_num(data):
    # è¼¸å‡ºæ ¼å¼ï¼šä½œè€…ï¼Œå¹´ä»½ï¼Œã€Œæ¨™é¡Œã€ï¼Œã€ŠæœŸåˆŠåã€‹ï¼Œå·æœŸï¼Œé ç¢¼ã€‚

    parts = []
    # ä½œè€…
    if isinstance(data.get('authors'), list):
        auth = "ã€".join(data.get('authors'))
    else:
        auth = data.get('authors', '')
    
    if auth: parts.append(auth)
    
    # å¹´ä»½
    if data.get('year'): parts.append(data['year'])
    
    # æ¨™é¡Œ
    if data.get('title'): parts.append(f"ã€Œ{data['title']}ã€")
    
    # ä¾†æºï¼ˆæœŸåˆŠæˆ–æ›¸åï¼‰
    if data.get('source'): parts.append(f"ã€Š{data['source']}ã€‹")
    
    # å·æœŸ
    vol_issue = []
    if data.get('volume'): vol_issue.append(f"{data['volume']}å·")
    if data.get('issue'): vol_issue.append(f"{data['issue']}æœŸ")
    if vol_issue: parts.append("".join(vol_issue))
    
    # é ç¢¼
    if data.get('pages'): parts.append(data['pages'])
    
    # URL
    if data.get('url'): parts.append(data['url'])
    
    return "ï¼Œ".join(parts) + "ã€‚"

def convert_zh_num_to_apa(data):
    # è¼¸å‡ºæ ¼å¼ï¼šä½œè€…ï¼ˆå¹´ä»½ï¼‰ã€‚æ¨™é¡Œã€‚ã€ŠæœŸåˆŠåã€‹ï¼Œå·(æœŸ)ï¼Œé ç¢¼ã€‚

    parts = []
    
    # ä½œè€…ï¼ˆå¹´ä»½ï¼‰
    if isinstance(data.get('authors'), list):
        auth = "ã€".join(data.get('authors'))
    else:
        auth = data.get('authors', '')
    
    parts.append(f"{auth}ï¼ˆ{data.get('year', 'ç„¡å¹´ä»½')}ï¼‰")
    
    # æ¨™é¡Œ
    if data.get('title'): parts.append(data['title'])
    
    # ä¾†æºï¼ˆæœŸåˆŠæˆ–æ›¸åï¼‰
    if data.get('source'):
        source_part = f"ã€Š{data['source']}ã€‹"
        
        # å·æœŸ
        if data.get('volume'):
            source_part += f"ï¼Œ{data['volume']}"
            if data.get('issue'):
                source_part += f"({data['issue']})"
        
        # é ç¢¼
        if data.get('pages'):
            source_part += f"ï¼Œ{data['pages']}"
        
        parts.append(source_part)
    
    # URL
    if data.get('url'): parts.append(data['url'])
    
    return "ã€‚".join(parts) + "ã€‚"



# å¼•å…¥ï¼š
import re
from common_utils import (
    normalize_text,
    has_chinese,
    extract_doi,
    is_valid_year,
)